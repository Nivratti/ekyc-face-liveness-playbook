{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"eKYC Face Liveness Playbook","text":""},{"location":"#the-complete-deployment-guide-for-banks-financial-institutions","title":"The Complete Deployment Guide for Banks &amp; Financial Institutions","text":"<p>About This Guide</p> <p>This is a comprehensive, deployment-ready reference for face liveness verification in electronic Know Your Customer (eKYC) systems. It covers everything from foundational concepts to production incident response \u2014 designed for CISOs, engineering leads, compliance officers, product managers, and procurement teams at banks and financial institutions worldwide.</p>"},{"location":"#who-this-guide-is-for","title":"Who This Guide Is For","text":"Role What You'll Get CISO / Risk Officer Complete threat landscape, attack taxonomy, security hardening, and incident response playbooks Engineering Lead Technical architecture, model training approaches, integration patterns, and infrastructure guidance Compliance Officer ISO 30107, iBeta, NIST certifications, RBI/EBA/FinCEN/MAS regulatory mapping Product Manager UX patterns, drop-off optimization, banking use case flows, and deployment roadmaps Procurement Team Vendor evaluation framework, build vs buy analysis, RFP templates, and cost/ROI models"},{"location":"#document-structure","title":"Document Structure","text":"<p>This guide is organized into 10 parts with 50+ detailed pages:</p>"},{"location":"#part-i-foundations","title":"Part I \u2014 Foundations","text":"<p>What face liveness is, why it's non-negotiable for banking, where it fits in the eKYC pipeline, and the terminology you need to know.</p>"},{"location":"#part-ii-liveness-methods","title":"Part II \u2014 Liveness Methods","text":"<p>Deep dive into active, passive, hybrid, and hardware-assisted liveness detection \u2014 with detailed comparison matrices.</p>"},{"location":"#part-iii-attack-landscape","title":"Part III \u2014 Attack Landscape","text":"<p>The most comprehensive attack taxonomy available \u2014 from printed photos to adversarial ML attacks, real-time deepfakes, injection attacks, and social engineering vectors.</p>"},{"location":"#part-iv-technical-architecture","title":"Part IV \u2014 Technical Architecture","text":"<p>Client-side and server-side components, model architectures (backbones, training approaches, loss functions), score fusion strategies, and integration patterns.</p>"},{"location":"#part-v-standards-compliance","title":"Part V \u2014 Standards &amp; Compliance","text":"<p>ISO 30107, iBeta Level 1 &amp; 2, NIST FRVT PAD, FIDO certification, and regulatory requirements across India, EU, US, and APAC.</p>"},{"location":"#part-vi-deployment-operations","title":"Part VI \u2014 Deployment &amp; Operations","text":"<p>Input quality requirements, device considerations, infrastructure scaling, UX optimization, monitoring, model updates, and network considerations.</p>"},{"location":"#part-vii-banking-use-cases","title":"Part VII \u2014 Banking Use Cases","text":"<p>Detailed flows for account opening, transaction authentication, Video KYC (V-CIP), loan disbursement, re-KYC, agent-assisted KYC, and cross-border banking.</p>"},{"location":"#part-viii-quality-testing","title":"Part VIII \u2014 Quality &amp; Testing","text":"<p>Performance metrics, testing methodology, red team exercises, edge case handling, demographic fairness testing, and dataset strategy.</p>"},{"location":"#part-ix-security-privacy","title":"Part IX \u2014 Security &amp; Privacy","text":"<p>Security hardening, anti-fraud intelligence, privacy/data protection, incident response playbooks, and legal/contractual frameworks.</p>"},{"location":"#part-x-business-strategy","title":"Part X \u2014 Business Strategy","text":"<p>Build vs buy, vendor evaluation, cost/ROI analysis, implementation roadmap, real-world case studies, and future trends.</p>"},{"location":"#appendices","title":"Appendices","text":"<p>RFP templates, iBeta PAI species reference, regulatory links, research papers, and production go-live checklists.</p>"},{"location":"#how-to-use-this-guide","title":"How to Use This Guide","text":"<p>For Quick Deployment Decisions</p> <p>Start with Part I for context, then jump to Vendor Evaluation and Implementation Roadmap.</p> <p>For Technical Teams Building In-House</p> <p>Follow Parts II \u2192 III \u2192 IV \u2192 VIII sequentially.</p> <p>For Compliance &amp; Risk Teams</p> <p>Focus on Part V for standards, Part III for threat understanding, and Part IX for legal frameworks.</p> <p>Version &amp; Confidentiality</p> <p>Version: 2.0 \u2014 February 2026 Classification: Confidential This document contains detailed security architecture information. Distribute only to authorized personnel.</p>"},{"location":"01-introduction/ekyc-pipeline/","title":"1.3 The eKYC Pipeline \u2014 Where Liveness Fits","text":""},{"location":"01-introduction/ekyc-pipeline/#the-complete-ekyc-flow","title":"The Complete eKYC Flow","text":"<p>Face liveness verification doesn't operate in isolation \u2014 it's one critical stage within a larger identity verification pipeline. Understanding this pipeline is essential for proper integration, error handling, and security architecture.</p> <pre><code>graph TD\n    A[\"\ud83d\udc64 User Initiates&lt;br&gt;KYC Process\"] --&gt; B[\"\ud83d\udcc4 Stage 1:&lt;br&gt;Document Capture\"]\n    B --&gt; C[\"\ud83d\udd0d Stage 2:&lt;br&gt;Document Authentication\"]\n    C --&gt; D[\"\ud83d\udcdd Stage 3:&lt;br&gt;OCR &amp; Data Extraction\"]\n    D --&gt; E[\"\ud83e\uddec Stage 4:&lt;br&gt;FACE LIVENESS&lt;br&gt;VERIFICATION\"]\n    E --&gt; F[\"\ud83d\udc65 Stage 5:&lt;br&gt;Face Matching\"]\n    F --&gt; G[\"\ud83d\uddc4\ufe0f Stage 6:&lt;br&gt;Database &amp; Watchlist Checks\"]\n    G --&gt; H[\"\u2696\ufe0f Stage 7:&lt;br&gt;Risk Scoring &amp; Decision\"]\n    H --&gt; I{\"Decision\"}\n    I --&gt;|\"\u2705 Approved\"| J[\"Account Opened\"]\n    I --&gt;|\"\u26a0\ufe0f Review\"| K[\"Manual Review Queue\"]\n    I --&gt;|\"\u274c Rejected\"| L[\"Application Rejected\"]\n\n    style E fill:#1B4F72,stroke:#154360,color:#fff\n    style J fill:#27ae60,stroke:#1e8449,color:#fff\n    style L fill:#e74c3c,stroke:#c0392b,color:#fff\n    style K fill:#f39c12,stroke:#d68910,color:#fff</code></pre>"},{"location":"01-introduction/ekyc-pipeline/#stage-by-stage-breakdown","title":"Stage-by-Stage Breakdown","text":""},{"location":"01-introduction/ekyc-pipeline/#stage-1-document-capture","title":"Stage 1: Document Capture","text":"<p>The user photographs their government-issued identity document using their device camera.</p> Aspect Details Input Camera frame(s) of identity document Documents Supported Passport, national ID, driving license, Aadhaar card, voter ID, PAN card, residence permit Key Challenges Glare, blur, partial occlusion, poor lighting, skewed angles, damaged documents Quality Checks Blur detection, completeness check, document boundary detection, readability assessment Output Cropped, de-skewed document image(s) \u2014 front and back if applicable <p>Liveness Relevance</p> <p>Document capture quality directly impacts downstream face matching accuracy. A blurry document photo means a blurry reference face, which increases false rejection rates in the liveness + matching pipeline.</p>"},{"location":"01-introduction/ekyc-pipeline/#stage-2-document-authentication","title":"Stage 2: Document Authentication","text":"<p>The system verifies that the captured document is a genuine, unaltered government-issued document.</p> Aspect Details Input Cropped document image(s) Checks Performed Security feature verification (holograms, UV patterns, microprinting), MRZ/barcode validation, tamper detection (photo substitution, text alteration, digital manipulation), template matching against known document types Document Liveness Determines if the document is a physical original (not a photocopy, screen display, or digitally fabricated document) Key Challenges Sophisticated document forgeries, digital-first documents (e.g., mAadhaar), document wear and aging Output Authentication confidence score, detected security features, tamper alerts <p>Coordinated Attack Vector</p> <p>A sophisticated attacker may present both a forged document and a spoofed face simultaneously. The document authentication and face liveness stages must be considered as complementary defenses \u2014 weakness in either stage compromises the entire pipeline.</p>"},{"location":"01-introduction/ekyc-pipeline/#stage-3-ocr-data-extraction","title":"Stage 3: OCR &amp; Data Extraction","text":"<p>Structured data is extracted from the authenticated document using Optical Character Recognition and intelligent field extraction.</p> Aspect Details Input Authenticated document image(s) Data Extracted Full name, date of birth, document number, address, gender, nationality, expiry date, photo region Technologies Layout-aware models (LayoutLMv3, LiLT, Donut), MRZ parsers, barcode/QR decoders Key Challenges Multilingual text, handwritten fields, poor print quality, non-standard layouts Output Structured JSON with extracted fields + confidence scores, extracted face photo from document <p>Face Photo Extraction</p> <p>The face photo extracted from the document at this stage becomes the reference image for face matching in Stage 5. Its quality (resolution, lighting, recency) directly affects matching accuracy.</p>"},{"location":"01-introduction/ekyc-pipeline/#stage-4-face-liveness-verification","title":"Stage 4: Face Liveness Verification \u2b50","text":"<p>This is the focus of this entire guide. The system confirms the user is a real, live, physically present human being.</p> Aspect Details Input Live camera feed (video frames or single selfie image) Processing Face detection \u2192 quality assessment \u2192 liveness analysis (passive/active/hybrid) \u2192 deepfake detection \u2192 score fusion \u2192 decision Key Signals Texture analysis, depth estimation, temporal consistency, physiological signals (rPPG), challenge-response compliance, device integrity Anti-Spoofing Targets Printed photos, screen replays, video replays, 2D/3D masks, deepfakes, virtual camera injection, adversarial attacks Output Liveness score (0-1), liveness decision (live/spoof/uncertain), attack type classification (if detected), captured face image(s) for matching <pre><code>graph LR\n    subgraph \"Stage 4: Liveness Verification (Detailed)\"\n        A[\"Camera&lt;br&gt;Feed\"] --&gt; B[\"Face&lt;br&gt;Detection\"]\n        B --&gt; C[\"Quality&lt;br&gt;Assessment\"]\n        C --&gt;|\"Pass\"| D[\"Passive&lt;br&gt;Liveness\"]\n        C --&gt;|\"Fail\"| C2[\"Re-capture&lt;br&gt;Guidance\"]\n        D --&gt; E[\"Active&lt;br&gt;Challenge&lt;br&gt;(if required)\"]\n        E --&gt; F[\"Deepfake&lt;br&gt;Detection\"]\n        F --&gt; G[\"Score&lt;br&gt;Fusion\"]\n        G --&gt; H{\"Decision\"}\n        H --&gt;|\"Live\"| I[\"\u2192 Stage 5\"]\n        H --&gt;|\"Spoof\"| J[\"\u274c Reject\"]\n        H --&gt;|\"Uncertain\"| K[\"\u26a0\ufe0f Escalate\"]\n    end\n\n    style I fill:#27ae60,stroke:#1e8449,color:#fff\n    style J fill:#e74c3c,stroke:#c0392b,color:#fff\n    style K fill:#f39c12,stroke:#d68910,color:#fff</code></pre>"},{"location":"01-introduction/ekyc-pipeline/#stage-5-face-matching-recognition","title":"Stage 5: Face Matching / Recognition","text":"<p>The live face (confirmed as genuine by liveness) is compared against the document photo to confirm identity.</p> Aspect Details Input Liveness-verified face image(s) + document face photo Processing Face alignment \u2192 feature extraction \u2192 embedding comparison \u2192 similarity scoring Key Challenges Aging between document issuance and current appearance, glasses/makeup/beard changes, lighting differences, resolution mismatch Threshold Management Similarity threshold balances false acceptance (impostor accepted) vs. false rejection (genuine user rejected) Output Match score (0-1), match decision (match/no-match/review), confidence level <p>Liveness + Matching Coupling</p> <p>Some architectures perform liveness and matching as a joint pipeline, using shared feature extractors. This can improve efficiency but creates a single point of failure. The recommended approach for banking is independent liveness and matching engines with separate model architectures.</p>"},{"location":"01-introduction/ekyc-pipeline/#stage-6-database-watchlist-checks","title":"Stage 6: Database &amp; Watchlist Checks","text":"<p>The verified identity is checked against external databases and regulatory lists.</p> Check Type Sources Purpose Sanctions Lists OFAC SDN, EU Consolidated, UN Security Council, HMT Block sanctioned individuals PEP Screening Dow Jones, Refinitiv World-Check, ComplyAdvantage Identify politically exposed persons Adverse Media News databases, court records Flag negative news associations Duplicate Detection Internal customer database Prevent duplicate accounts (same person, multiple identities) Fraud Databases CIFAS (UK), CIBIL (India), LexisNexis Check known fraud histories Government Databases Aadhaar (India), Digilocker, NSDL PAN Direct identity verification against government records"},{"location":"01-introduction/ekyc-pipeline/#stage-7-risk-scoring-decision","title":"Stage 7: Risk Scoring &amp; Decision","text":"<p>All signals from previous stages are aggregated into a final risk assessment.</p> <pre><code>graph TD\n    A[\"Document Auth Score\"] --&gt; G[\"Risk Scoring Engine\"]\n    B[\"OCR Confidence\"] --&gt; G\n    C[\"Liveness Score\"] --&gt; G\n    D[\"Face Match Score\"] --&gt; G\n    E[\"Watchlist Results\"] --&gt; G\n    F[\"Device/Behavioral Signals\"] --&gt; G\n\n    G --&gt; H{\"Risk Level\"}\n    H --&gt;|\"Low Risk\"| I[\"\u2705 Auto-Approve&lt;br&gt;(STP - Straight Through Processing)\"]\n    H --&gt;|\"Medium Risk\"| J[\"\u26a0\ufe0f Manual Review&lt;br&gt;(Enhanced Due Diligence)\"]\n    H --&gt;|\"High Risk\"| K[\"\u274c Auto-Reject&lt;br&gt;(with reason codes)\"]\n\n    style I fill:#27ae60,stroke:#1e8449,color:#fff\n    style J fill:#f39c12,stroke:#d68910,color:#fff\n    style K fill:#e74c3c,stroke:#c0392b,color:#fff</code></pre> Decision Typical STP Rate Action Auto-Approve 70-85% of applications Account opened immediately, welcome flow initiated Manual Review 10-20% of applications Queued for human reviewer with all evidence, SLA 24-48 hours Auto-Reject 5-10% of applications Application rejected with appropriate messaging, fraud team notified if warranted"},{"location":"01-introduction/ekyc-pipeline/#integration-architecture-patterns","title":"Integration Architecture Patterns","text":""},{"location":"01-introduction/ekyc-pipeline/#pattern-1-sequential-pipeline-most-common","title":"Pattern 1: Sequential Pipeline (Most Common)","text":"<p>Each stage completes before the next begins. Simpler to implement and debug, but slower.</p> <pre><code>Document Capture \u2192 Document Auth \u2192 OCR \u2192 Liveness \u2192 Face Match \u2192 Database \u2192 Decision\n     (5s)            (2s)          (1s)    (5-15s)     (1s)        (2s)       (0.5s)\n                                                                     Total: 16-26s\n</code></pre>"},{"location":"01-introduction/ekyc-pipeline/#pattern-2-parallel-pipeline-optimized","title":"Pattern 2: Parallel Pipeline (Optimized)","text":"<p>Independent stages run simultaneously where possible.</p> <pre><code>Document Capture \u2500\u2192 Document Auth \u2500\u2192 OCR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                 \u251c\u2500\u2192 Face Match \u2192 Database \u2192 Decision\nSelfie/Liveness \u2500\u2192 Liveness Verification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                              Total: 8-18s\n</code></pre>"},{"location":"01-introduction/ekyc-pipeline/#pattern-3-progressive-pipeline-best-ux","title":"Pattern 3: Progressive Pipeline (Best UX)","text":"<p>Results are communicated progressively; user sees progress.</p> <pre><code>Document Capture \u2500\u2192 [Show \"Document verified \u2713\"]\n                    \u2193\n                 Document Auth + OCR (parallel)\n                    \u2193\nSelfie + Liveness \u2500\u2192 [Show \"Identity confirmed \u2713\"]\n                    \u2193\n                 Face Match + Database (parallel)\n                    \u2193\n                 [Show \"Account Ready \u2713\"]\n</code></pre>"},{"location":"01-introduction/ekyc-pipeline/#cross-stage-security-considerations","title":"Cross-Stage Security Considerations","text":"<p>Critical Security Principles</p> <p>1. Session Integrity All stages must operate within a single, time-bounded session with anti-replay tokens. An attacker should not be able to pass liveness at one time and use that result for a different session.</p> <p>2. Data Continuity The face image used for liveness verification MUST be the same image used for face matching. If these are decoupled, an attacker could pass liveness with their own face and submit someone else's photo for matching.</p> <p>3. Server-Side Authority Final decisions must be made server-side. Client-side results (including local liveness scores) should be treated as preliminary signals, never as authoritative.</p> <p>4. Atomic Transactions The pipeline should be transactional \u2014 if any stage fails or is compromised, the entire verification should be invalidated, not just the failed stage.</p> <p>5. Coordinated Attack Defense Design for the scenario where an attacker simultaneously presents a forged document AND a spoofed face. Cross-stage correlation (e.g., does the document photo match the live face?) must be enforced.</p>"},{"location":"01-introduction/ekyc-pipeline/#failure-modes-fallback-flows","title":"Failure Modes &amp; Fallback Flows","text":"Failure Point Impact Recommended Fallback Document capture fails (blur, glare) Pipeline cannot start Real-time quality guidance, retry with tips Document authentication fails Possible forgery or unrecognized document Escalate to manual review, request alternate document OCR extraction fails Cannot extract reference data Manual data entry fallback, partial automation Liveness fails (false reject) Genuine user blocked Retry with guidance (2-3 attempts), then escalate to Video KYC or branch Liveness fails (attack detected) Spoof attempt blocked Terminate session, log full audit trail, alert fraud team Face match fails Identity mismatch Consider aging/appearance changes, offer manual review Database check flags Potential watchlist hit Enhanced due diligence, manual review by compliance"},{"location":"01-introduction/ekyc-pipeline/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ol> <li>Face liveness is Stage 4 in a 7-stage eKYC pipeline</li> <li>It sits after document verification and before face matching \u2014 this ordering is critical</li> <li>Session integrity must bind all stages together to prevent mix-and-match attacks</li> <li>Parallel processing can reduce total time from 25s to under 18s</li> <li>The same face image must be used for both liveness and matching</li> <li>Design for coordinated attacks where both document and face are spoofed simultaneously</li> <li>Clear fallback flows prevent genuine customer abandonment while maintaining security</li> </ol> <p>Next: Core Terminology &amp; Glossary \u2192</p>"},{"location":"01-introduction/glossary/","title":"1.4 Core Terminology &amp; Glossary","text":""},{"location":"01-introduction/glossary/#essential-terms","title":"Essential Terms","text":"<p>This glossary provides definitions for all key terms used throughout this guide. Terms are organized by category for easy reference.</p>"},{"location":"01-introduction/glossary/#biometric-liveness-terms","title":"Biometric &amp; Liveness Terms","text":"Term Abbreviation Definition Presentation Attack PA Any attempt to interfere with a biometric system by presenting an artifact (photo, mask, video, deepfake) to the sensor instead of a genuine live biometric Presentation Attack Detection PAD The automated process of detecting presentation attacks; synonymous with \"liveness detection\" and \"anti-spoofing\" Presentation Attack Instrument PAI The specific artifact used to carry out an attack \u2014 e.g., a printed photo, silicone mask, or deepfake video PAI Species \u2014 A category of attack instrument sharing common characteristics (e.g., \"printed photo on A4 paper\" is one species; \"silicone mask\" is another) Bona Fide Presentation \u2014 A genuine, non-attack presentation by a real, live person cooperating normally with the system Liveness Score \u2014 A numerical confidence value (typically 0.0 to 1.0) indicating the probability that a presentation is bona fide (live). Higher = more likely live Face Anti-Spoofing FAS Alternative term for face presentation attack detection; commonly used in academic literature Active Liveness \u2014 Liveness detection requiring user interaction (head turns, blinks, expressions, speech) in response to system prompts Passive Liveness \u2014 Liveness detection requiring no user interaction; analysis of a single image or short video clip Hybrid Liveness \u2014 Combination of active and passive approaches, typically using passive as primary with active fallback Challenge-Response \u2014 Active liveness paradigm where the system issues a random challenge and verifies the user's response"},{"location":"01-introduction/glossary/#performance-metrics","title":"Performance Metrics","text":"Term Abbreviation Definition Attack Presentation Classification Error Rate APCER The proportion of attack presentations incorrectly classified as bona fide. Lower is better. A 0% APCER means no attacks got through. Measured per PAI species Bona Fide Presentation Classification Error Rate BPCER The proportion of genuine presentations incorrectly classified as attacks (false rejections). Lower is better. A 0% BPCER means no real users were rejected Average Classification Error Rate ACER Simple average of APCER and BPCER: <code>ACER = (APCER + BPCER) / 2</code>. Used as a single summary metric True Detection Rate TDR Proportion of attacks correctly detected: <code>TDR = 1 - APCER</code>. Also called True Positive Rate for attacks False Rejection Rate FRR Same as BPCER in liveness context \u2014 rate at which genuine users are falsely rejected False Acceptance Rate FAR Same as APCER in liveness context \u2014 rate at which attacks are falsely accepted Equal Error Rate EER The operating point where APCER equals BPCER. Lower EER indicates better overall system performance Detection Error Tradeoff DET A curve plotting APCER vs BPCER at different thresholds; used to visualize the security-convenience tradeoff Receiver Operating Characteristic ROC Curve plotting TDR vs APCER (or FAR); shows overall discriminative ability of the system Area Under Curve AUC Area under the ROC curve; ranges from 0.5 (random) to 1.0 (perfect). Higher is better Half Total Error Rate HTER Same as ACER; average of FAR and FRR. Common in academic literature <p>Key Relationship</p> <pre><code>APCER + TDR = 1  (for attacks)\nBPCER + Genuine Acceptance Rate = 1  (for genuine users)\n</code></pre> <p>When you tighten the threshold to reduce APCER (block more attacks), BPCER increases (more genuine users rejected). This is the fundamental security-usability tradeoff.</p>"},{"location":"01-introduction/glossary/#standards-certifications","title":"Standards &amp; Certifications","text":"Term Definition ISO/IEC 30107 International standard series for biometric presentation attack detection. Part 1: Framework. Part 2: Data formats. Part 3: Testing and reporting. Part 4: Mobile profile ISO/IEC 19795 Biometric performance testing framework; referenced by 30107-3 for statistical methods iBeta Independent biometric testing laboratory; most widely recognized for PAD conformance testing. Offers Level 1 (2D attacks) and Level 2 (2D + 3D attacks) certifications NIST FRVT NIST Face Recognition Vendor Test \u2014 ongoing evaluation of face recognition and PAD technology NIST FATE NIST Face Analysis Technology Evaluation \u2014 evaluates face analysis technologies including PAD FIDO Alliance Industry consortium for authentication standards; offers Biometric Component Certification Program NIST SP 800-63B Digital identity guidelines specifying Identity Assurance Levels (IAL) 1-3 with PAD requirements at IAL2+ Common Criteria International framework (ISO/IEC 15408) for IT security evaluation; applicable to biometric systems"},{"location":"01-introduction/glossary/#attack-types","title":"Attack Types","text":"Term Definition Print Attack Presentation of a printed photograph (paper, cardstock, or high-quality photo paper) to the camera Screen Replay Attack Displaying a photo or video on a digital screen (phone, tablet, monitor) and presenting it to the camera Video Replay Attack Subtype of screen replay using pre-recorded video to simulate natural facial motion 2D Mask Attack Printed face worn as a mask, often with eye/mouth cutouts for the attacker to see through and simulate blinking 3D Rigid Mask Hard mask (resin, plaster, 3D-printed plastic) replicating the target's facial geometry 3D Flexible Mask Soft mask (silicone, latex) that conforms to the attacker's face and simulates skin-like properties Deepfake AI-generated or AI-manipulated facial imagery; includes face swaps, face reenactment, lip sync, and full synthesis Face Swap Replacing one person's face with another's in video while maintaining head movements and expressions Face Reenactment Transferring facial expressions from a driving source to a target face identity in real-time Virtual Camera Injection Using software (OBS Virtual Cam, ManyCam) to feed pre-recorded or AI-generated content as a live camera feed Camera API Hooking Low-level interception of camera data using frameworks like Frida or Xposed to inject modified frames Adversarial Attack Specially crafted perturbations (often imperceptible to humans) designed to fool neural network classifiers Morphing Attack Blending two faces into a single face image that matches both identities; used for document fraud Relay Attack Legitimate person's live camera feed is remotely transmitted to another device for KYC at a different location Synthetic Identity A fabricated identity combining real and fictitious information with a GAN-generated or stolen face"},{"location":"01-introduction/glossary/#technical-architecture-terms","title":"Technical / Architecture Terms","text":"Term Definition rPPG Remote Photoplethysmography \u2014 technique to detect blood flow and heart rate from facial video by analyzing subtle color changes in skin FACS Facial Action Coding System \u2014 system for classifying facial expressions based on Action Units (AU), each corresponding to specific muscle movements Action Unit (AU) Individual facial muscle movement in FACS (e.g., AU1 = inner brow raise, AU12 = lip corner pull/smile) Moir\u00e9 Pattern Interference pattern created when two regular patterns (like screen pixel grids) overlap; a strong indicator of screen-based attacks Halftone Pattern Dot pattern used in printing; visible under magnification and in frequency analysis; indicator of print attacks Subsurface Scattering Light transport phenomenon where light penetrates skin, scatters internally, and exits at a different point; unique to live skin Specular Highlight Bright spot of reflected light on a shiny surface; pattern differs between skin, paper, plastic, and screen glass Depth Map Per-pixel distance estimation from the camera; live faces produce characteristic 3D depth profiles Feature Embedding Fixed-length numerical vector representing a face's identity or liveness characteristics in learned feature space Score Fusion Combining multiple scores (passive liveness, active verification, deepfake detection) into a single decision Domain Generalization Training methodology aimed at producing models that perform well on unseen domains (new sensors, environments, attack types) SDK Software Development Kit \u2014 packaged library for integrating liveness detection into mobile or web applications"},{"location":"01-introduction/glossary/#regulatory-compliance-terms","title":"Regulatory &amp; Compliance Terms","text":"Term Definition eKYC Electronic Know Your Customer \u2014 digital identity verification process for customer onboarding V-CIP Video-based Customer Identification Process \u2014 RBI-approved method for remote KYC using live video interaction CDD Customer Due Diligence \u2014 process of verifying customer identity and assessing risk; required by AML regulations EDD Enhanced Due Diligence \u2014 additional scrutiny for higher-risk customers (PEPs, high-value transactions, etc.) AML Anti-Money Laundering \u2014 regulations and processes to prevent money laundering through financial systems KYC Know Your Customer \u2014 the broader process of identifying and verifying customers' identities PEP Politically Exposed Person \u2014 individual holding prominent public position; subject to enhanced scrutiny STP Straight-Through Processing \u2014 fully automated processing without human intervention DPDPA Digital Personal Data Protection Act (India, 2023) \u2014 governs personal data processing including biometrics GDPR General Data Protection Regulation (EU) \u2014 classifies biometric data as special category requiring explicit consent BSA Bank Secrecy Act (US) \u2014 requires financial institutions to assist government agencies in detecting/preventing money laundering PSD2 SCA Payment Services Directive 2, Strong Customer Authentication \u2014 EU requirement for multi-factor authentication in payments"},{"location":"01-introduction/glossary/#acronym-quick-reference","title":"Acronym Quick Reference","text":"Acronym Full Form APCER Attack Presentation Classification Error Rate BPCER Bona Fide Presentation Classification Error Rate ACER Average Classification Error Rate PAD Presentation Attack Detection PAI Presentation Attack Instrument rPPG Remote Photoplethysmography FACS Facial Action Coding System NIR Near Infrared ToF Time of Flight DG Domain Generalization GAN Generative Adversarial Network NeRF Neural Radiance Field ONNX Open Neural Network Exchange TFLite TensorFlow Lite FPS Frames Per Second DXA Device Cross-Attestation MRZ Machine Readable Zone NFC Near Field Communication OTP One-Time Password MFA Multi-Factor Authentication <p>Next: Part II \u2014 Active Liveness Detection \u2192</p>"},{"location":"01-introduction/what-is-face-liveness/","title":"1.1 What Is Face Liveness Verification?","text":""},{"location":"01-introduction/what-is-face-liveness/#definition","title":"Definition","text":"<p>Face liveness verification (also called face liveness detection, presentation attack detection (PAD), or anti-spoofing) is the process of determining whether a biometric facial sample presented to a camera sensor originates from a live, physically present human being \u2014 as opposed to an artificial reproduction such as a printed photograph, a screen replay, a 3D mask, or an AI-generated deepfake.</p> <p>In simple terms, it answers one critical question:</p> <p>The Core Question</p> <p>Is there a real, living person in front of this camera right now?</p>"},{"location":"01-introduction/what-is-face-liveness/#the-problem-it-solves","title":"The Problem It Solves","text":"<p>Without liveness verification, a facial recognition system has no way to distinguish between:</p> <pre><code>graph LR\n    A[\"\ud83d\udcf8 Camera Sensor\"] --&gt; B{\"What does it see?\"}\n    B --&gt; C[\"\u2705 Real Person&lt;br&gt;(Bona Fide)\"]\n    B --&gt; D[\"\u274c Printed Photo\"]\n    B --&gt; E[\"\u274c Screen Replay\"]\n    B --&gt; F[\"\u274c 3D Mask\"]\n    B --&gt; G[\"\u274c Deepfake Video\"]\n\n    style C fill:#27ae60,stroke:#1e8449,color:#fff\n    style D fill:#e74c3c,stroke:#c0392b,color:#fff\n    style E fill:#e74c3c,stroke:#c0392b,color:#fff\n    style F fill:#e74c3c,stroke:#c0392b,color:#fff\n    style G fill:#e74c3c,stroke:#c0392b,color:#fff</code></pre> <p>A face recognition system can confirm that \"this face matches the identity document\" \u2014 but it cannot confirm that \"this face belongs to a person who is physically present.\" That's the gap liveness verification fills.</p>"},{"location":"01-introduction/what-is-face-liveness/#formal-definition-isoiec-30107","title":"Formal Definition (ISO/IEC 30107)","text":"<p>The international standard ISO/IEC 30107-1 defines the formal framework:</p> Term ISO Definition Plain English Presentation Attack Presentation to the biometric data capture subsystem with the goal of interfering with the operation of the biometric system Any attempt to fool the camera with something that isn't a real, live person Presentation Attack Detection (PAD) Automated determination of a presentation attack The technology that detects spoofing attempts Presentation Attack Instrument (PAI) Biometric characteristic or object used in a presentation attack The thing used to attack \u2014 a photo, mask, screen, deepfake, etc. Bona Fide Presentation Interaction of the biometric capture subject with the data capture subsystem in a fashion that does not involve a presentation attack A genuine, live person presenting themselves naturally"},{"location":"01-introduction/what-is-face-liveness/#how-it-works-conceptual-overview","title":"How It Works \u2014 Conceptual Overview","text":"<p>Face liveness systems analyze multiple signal dimensions to distinguish real from fake:</p> <pre><code>graph TD\n    subgraph \"Signal Dimensions Analyzed\"\n        A[\"\ud83d\udd2c TEXTURE&lt;br&gt;Skin micro-patterns&lt;br&gt;Pore structure&lt;br&gt;Specular highlights\"] \n        B[\"\ud83d\udcd0 GEOMETRY&lt;br&gt;3D facial structure&lt;br&gt;Depth consistency&lt;br&gt;Parallax effects\"]\n        C[\"\u23f1\ufe0f TEMPORAL&lt;br&gt;Natural motion&lt;br&gt;Micro-expressions&lt;br&gt;Blink patterns\"]\n        D[\"\ud83c\udf08 SPECTRAL&lt;br&gt;Color response&lt;br&gt;NIR reflectance&lt;br&gt;Frequency domain\"]\n        E[\"\ud83e\udde0 BEHAVIORAL&lt;br&gt;Challenge response&lt;br&gt;Gaze tracking&lt;br&gt;Physiological signals\"]\n    end\n\n    A --&gt; F[\"Score Fusion\"]\n    B --&gt; F\n    C --&gt; F\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G{\"Decision\"}\n    G --&gt;|\"Score \u2265 Threshold\"| H[\"\u2705 LIVE\"]\n    G --&gt;|\"Score &lt; Threshold\"| I[\"\u274c SPOOF\"]\n\n    style H fill:#27ae60,stroke:#1e8449,color:#fff\n    style I fill:#e74c3c,stroke:#c0392b,color:#fff</code></pre>"},{"location":"01-introduction/what-is-face-liveness/#signal-dimension-details","title":"Signal Dimension Details","text":"<p>1. Texture Analysis</p> <p>Live human skin has unique properties at the micro-texture level that are extremely difficult to replicate:</p> <ul> <li>Pore structure: Natural skin pores create a characteristic texture visible even at standard camera resolutions. Printed photos show halftone dot patterns instead; screens show pixel grids.</li> <li>Subsurface scattering: Light penetrates skin and scatters beneath the surface, creating a characteristic soft glow. This is absent in flat reproductions.</li> <li>Specular highlights: The way light reflects off skin (especially oily areas like the forehead, nose, and cheeks) follows predictable patterns related to skin microgeometry. Paper and screens have fundamentally different reflectance models.</li> <li>Moire patterns: When a screen is photographed by another camera, interference between the pixel grids creates visible Moir\u00e9 artifacts.</li> </ul> <p>2. Geometry / Depth</p> <p>A real face is a 3D object; most attacks present a 2D surface:</p> <ul> <li>Monocular depth estimation: Neural networks can estimate depth from a single 2D image. Live faces produce depth maps consistent with human facial anatomy (nose protrudes, eyes are recessed, cheeks curve). Flat attacks produce anomalous, inconsistent depth.</li> <li>Parallax effects: When the device or head moves slightly, the relative position of facial features changes in a way consistent with 3D geometry. Flat images don't exhibit this.</li> <li>Edge geometry: The boundary between the face and background in a live presentation has natural depth-of-field blur and 3D edge characteristics different from the sharp, flat edges of a printed photo or screen.</li> </ul> <p>3. Temporal Analysis</p> <p>Real faces exhibit constant, involuntary micro-movements:</p> <ul> <li>Blink patterns: Humans blink every 2-10 seconds with characteristic lid motion dynamics. Photos don't blink; simple video loops have predictable blink timing.</li> <li>Micro-expressions: Involuntary facial muscle activations lasting 50-500ms occur constantly. These are extremely difficult to synthesize.</li> <li>Blood flow (rPPG): Remote photoplethysmography can detect subtle color changes in facial skin caused by blood flow synchronized with heartbeat. This is a strong liveness signal absent in all non-living presentations.</li> <li>Natural motion: Head stability (micro-sway), breathing-related movement, and other physiological motion create temporal patterns unique to live presentations.</li> </ul> <p>4. Spectral Analysis</p> <p>Different materials respond differently to light:</p> <ul> <li>Frequency domain signatures: Fourier/wavelet analysis reveals frequency patterns characteristic of different media (printer halftone frequencies, screen pixel frequencies, camera sensor noise patterns).</li> <li>Color gamut differences: Screens and printers have limited color gamuts compared to real-world skin tones, especially in challenging lighting conditions.</li> <li>Near-infrared response: If NIR sensors are available, skin has dramatically different NIR reflectance than paper, plastic, or screen glass.</li> </ul> <p>5. Behavioral Analysis</p> <p>Active challenge-response provides high-confidence signals:</p> <ul> <li>Challenge compliance: The user correctly performs a randomized action (head turn, blink, smile) within expected timing parameters.</li> <li>Gaze correlation: Eye movements track a moving target naturally, with characteristic saccadic patterns.</li> <li>Physiological consistency: Multiple signals (motion, expression, gaze) are consistent with a single, live human source.</li> </ul>"},{"location":"01-introduction/what-is-face-liveness/#what-liveness-is-not","title":"What Liveness Is NOT","text":"<p>Common Misconceptions</p> <p>Liveness \u2260 Face Recognition Liveness detection determines if a person is real and present. Face recognition determines who the person is. They are complementary but separate technologies.</p> <p>Liveness \u2260 Face Detection Face detection locates faces in images. It says \"there is a face here\" but nothing about whether it's live or spoofed.</p> <p>Liveness \u2260 Identity Verification Identity verification is the complete process of confirming someone is who they claim to be. Liveness is one component within this larger process.</p> <p>Liveness \u2260 Deepfake Detection While modern liveness systems include deepfake detection capabilities, standalone deepfake detectors and liveness systems have different design objectives. Deepfake detectors identify AI manipulation; liveness systems confirm physical presence.</p>"},{"location":"01-introduction/what-is-face-liveness/#the-spectrum-of-sophistication","title":"The Spectrum of Sophistication","text":"<p>Liveness systems range from basic to extremely sophisticated:</p> Level Approach What It Detects What It Misses Level 0 No liveness Nothing Everything \u2014 system accepts any face image Level 1 Basic blink/motion detection Static photos Video replay, all advanced attacks Level 2 Texture + depth analysis Photos, basic screen replay High-quality video, masks, deepfakes Level 3 Multi-signal passive + active Photos, screens, basic masks, basic deepfakes Sophisticated silicone masks, real-time deepfakes Level 4 Full multi-modal with deepfake detection All Level 3 + deepfakes, injection attacks State-of-the-art adversarial attacks, neural avatars Level 5 Adaptive AI with continuous learning All known attacks with rapid adaptation Truly novel, zero-day attack methods <p>Banking Minimum</p> <p>For banking and financial services, Level 3 is the absolute minimum with a clear roadmap to Level 4. Deploying anything below Level 3 exposes the institution to unacceptable fraud risk and regulatory non-compliance.</p>"},{"location":"01-introduction/what-is-face-liveness/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ol> <li>Face liveness verification confirms physical presence of a live human being</li> <li>It operates across five signal dimensions: texture, geometry, temporal, spectral, and behavioral</li> <li>It is distinct from face recognition, face detection, and identity verification</li> <li>It is formalized under ISO/IEC 30107 as Presentation Attack Detection (PAD)</li> <li>Banking deployments require Level 3+ sophistication at minimum</li> <li>It sits at Stage 4 of the eKYC pipeline \u2014 after document verification, before face matching</li> </ol> <p>Next: Why Face Liveness Matters for Banking \u2192</p>"},{"location":"01-introduction/why-it-matters/","title":"1.2 Why Face Liveness Matters for Banking","text":""},{"location":"01-introduction/why-it-matters/#the-stakes","title":"The Stakes","text":"<p>For banks and financial institutions, face liveness verification isn't a \"nice-to-have\" technology feature \u2014 it's a critical security control that directly impacts fraud exposure, regulatory standing, customer trust, and competitive positioning.</p> <p>The Cost of Getting It Wrong</p> <p>A single compromised liveness system can result in:</p> <ul> <li>Millions in direct fraud losses from synthetic identity accounts and account takeover</li> <li>Regulatory fines ranging from $100K to $50M+ depending on jurisdiction</li> <li>License restrictions or mandatory enhanced supervision</li> <li>Reputational damage that takes years to recover from</li> <li>Customer attrition as trust erodes</li> </ul>"},{"location":"01-introduction/why-it-matters/#the-five-pillars-of-why-it-matters","title":"The Five Pillars of Why It Matters","text":""},{"location":"01-introduction/why-it-matters/#1-fraud-prevention-financial-loss-avoidance","title":"1. Fraud Prevention &amp; Financial Loss Avoidance","text":"<p>Identity fraud is the fastest-growing financial crime category globally. Without effective liveness detection, attackers can:</p> <pre><code>graph TD\n    A[\"Attacker obtains&lt;br&gt;victim's photo&lt;br&gt;(social media, data breach)\"] --&gt; B[\"Presents photo/deepfake&lt;br&gt;to eKYC system\"]\n    B --&gt; C{\"Liveness Check?\"}\n    C --&gt;|\"No Liveness\"| D[\"Account opened&lt;br&gt;in victim's name\"]\n    C --&gt;|\"Weak Liveness\"| E[\"Sophisticated attack&lt;br&gt;bypasses check\"]\n    C --&gt;|\"Strong Liveness\"| F[\"\u274c Attack blocked\"]\n\n    D --&gt; G[\"\ud83d\udcb0 Fraud Executed&lt;br&gt;- Money mule account&lt;br&gt;- Loan fraud&lt;br&gt;- Credit card fraud&lt;br&gt;- Money laundering\"]\n    E --&gt; G\n\n    style F fill:#27ae60,stroke:#1e8449,color:#fff\n    style G fill:#e74c3c,stroke:#c0392b,color:#fff</code></pre> <p>Key fraud statistics:</p> Fraud Type Annual Impact (Global) Liveness Role Synthetic Identity Fraud $6B+ (US alone) Primary defense \u2014 prevents creation of synthetic identity accounts Account Takeover $11B+ globally Step-up authentication verifies the real account holder New Account Fraud $3.4B+ (US banking) Blocks onboarding with stolen/fabricated identities Loan/Credit Fraud $2.7B+ annually Prevents loan applications under false identities Money Laundering $800B-2T laundered annually Prevents creation of mule accounts used for layering <p>ROI Example</p> <p>A mid-size bank processing 500,000 digital onboarding attempts per year with a 0.5% attempted fraud rate (2,500 attacks) and an average fraud loss of $5,000 per successful attack:</p> <ul> <li>Without liveness: ~80% attack success rate = 2,000 \u00d7 $5,000 = $10M annual fraud loss</li> <li>With strong liveness (99.5% detection): ~12 successful attacks = $60K annual fraud loss</li> <li>Net savings: $9.94M/year against a typical liveness system cost of $200K-500K/year</li> </ul>"},{"location":"01-introduction/why-it-matters/#2-regulatory-compliance","title":"2. Regulatory Compliance","text":"<p>Regulators worldwide are tightening requirements around digital identity verification, with specific mandates for anti-spoofing measures:</p> Regulator Requirement Consequence of Non-Compliance RBI (India) V-CIP requires real-time liveness during video KYC License restrictions, mandatory remediation, enhanced supervision EBA (Europe) Remote onboarding guidelines mandate robust anti-spoofing Fines up to 10% of annual turnover, prohibition of remote onboarding FinCEN (US) BSA/AML requires adequate CDD procedures Civil penalties up to $1M/day per violation, criminal prosecution MAS (Singapore) Technology Risk Management mandates anti-spoofing Regulatory actions, restrictions on digital banking license HKMA (Hong Kong) Remote onboarding guidelines require liveness Enhanced supervision, mandatory technology audits BaFin (Germany) Video identification requires qualified examiner + anti-spoofing Prohibition of video identification, mandatory reversion to in-person <p>Regulatory Trend</p> <p>The direction is unmistakable: every major financial regulator is moving toward explicit requirements for biometric anti-spoofing. Institutions without robust liveness verification face increasing compliance risk with every regulatory update.</p>"},{"location":"01-introduction/why-it-matters/#3-customer-trust-experience","title":"3. Customer Trust &amp; Experience","text":"<p>Liveness verification, when implemented well, enhances rather than hinders customer experience:</p> <p>The Digital Onboarding Gap:</p> <pre><code>graph LR\n    subgraph \"Without Liveness (Branch Required)\"\n        A1[\"Customer starts&lt;br&gt;online application\"] --&gt; A2[\"Must visit branch&lt;br&gt;for ID verification\"] --&gt; A3[\"30-60 min wait&lt;br&gt;+ travel time\"] --&gt; A4[\"Account opened&lt;br&gt;(Days later)\"]\n    end\n\n    subgraph \"With Liveness (Fully Digital)\"\n        B1[\"Customer starts&lt;br&gt;mobile application\"] --&gt; B2[\"Selfie + liveness&lt;br&gt;(10-30 seconds)\"] --&gt; B3[\"Instant verification&lt;br&gt;+ face matching\"] --&gt; B4[\"Account opened&lt;br&gt;(Minutes)\"]\n    end</code></pre> <p>Impact on conversion:</p> Metric Branch-Based KYC Digital KYC without Liveness Digital KYC with Liveness Application completion rate 40-60% N/A (non-compliant) 75-92% Time to account opening 3-7 days \u2014 5-15 minutes Customer satisfaction (NPS) +10 to +25 \u2014 +40 to +65 Cost per verification $15-50 \u2014 $0.10-2.00 Geographic reach Branch footprint only \u2014 Nationwide/Global <p>Customer Trust Signal</p> <p>Customers increasingly expect secure digital experiences. A visible, smooth liveness check actually increases trust \u2014 customers see the bank is taking their security seriously. Survey data shows 78% of banking customers prefer biometric verification over OTP/password methods when properly implemented.</p>"},{"location":"01-introduction/why-it-matters/#4-competitive-advantage","title":"4. Competitive Advantage","text":"<p>In a market where digital banking is the primary battleground for customer acquisition:</p> <ul> <li>Neobanks and fintechs are setting customer expectations with frictionless onboarding</li> <li>Traditional banks that can't offer equivalent digital experiences lose market share</li> <li>First-mover advantage in liveness implementation compounds over time through better fraud models trained on more data</li> <li>Multi-geography expansion becomes feasible when onboarding doesn't require physical presence</li> <li>Cost structure shifts dramatically \u2014 digital onboarding at $0.10-2.00 vs. branch-based at $15-50 per verification</li> </ul>"},{"location":"01-introduction/why-it-matters/#5-ecosystem-partnership-requirements","title":"5. Ecosystem &amp; Partnership Requirements","text":"<p>Modern banking operates within an interconnected ecosystem where liveness verification is increasingly a prerequisite:</p> <ul> <li>Open Banking / Account Aggregator: Partners require verified digital identities for data sharing</li> <li>Payment Networks: UPI, SWIFT gpi, and card networks are introducing biometric authentication requirements</li> <li>Insurance Partners: Bancassurance products increasingly require biometric verification for policy issuance</li> <li>Government Platforms: UIDAI (Aadhaar), EU Digital Identity Wallet, and Singapore SingPass mandate anti-spoofing for integration</li> <li>Correspondent Banking: Cross-border partners require evidence of robust KYC including biometric verification</li> </ul>"},{"location":"01-introduction/why-it-matters/#real-world-impact-what-happens-without-liveness","title":"Real-World Impact: What Happens Without Liveness","text":"<p>Case: Bank Account Fraud Ring (India, 2023)</p> <p>A fraud ring opened 2,300+ accounts at multiple banks using stolen Aadhaar photos displayed on smartphone screens. The banks had face matching but no liveness detection. These accounts were used to launder \u20b9180 crore ($21.6M) from online loan fraud schemes. RBI mandated immediate implementation of liveness checks across all digital onboarding channels.</p> <p>Case: Deepfake Account Takeover (Europe, 2024)</p> <p>An attacker used real-time deepfake technology to impersonate a corporate client during a video banking session, authorizing a \u20ac35 million wire transfer. The bank's video KYC system had basic motion detection but no deepfake detection capability. The fraud was discovered 4 hours later when the real client called.</p> <p>Case: Synthetic Identity at Scale (US, 2023)</p> <p>A sophisticated operation created 10,000+ synthetic identities using GAN-generated faces paired with manufactured identity documents. These were used to open accounts, build credit histories over 12-18 months, then execute bust-out fraud totaling $200M+ across multiple financial institutions. Banks with advanced liveness (deepfake + synthetic face detection) blocked 97% of attempts.</p>"},{"location":"01-introduction/why-it-matters/#the-decision-matrix","title":"The Decision Matrix","text":"<p>For leadership teams evaluating investment in face liveness verification:</p> Factor No Liveness Basic Liveness Advanced Liveness Regulatory Risk \ud83d\udd34 Critical \ud83d\udfe1 Moderate \ud83d\udfe2 Low Fraud Exposure \ud83d\udd34 Extreme \ud83d\udfe1 Significant \ud83d\udfe2 Minimal Digital Onboarding \ud83d\udd34 Not possible (compliant) \ud83d\udfe1 Limited channels \ud83d\udfe2 Full omnichannel Customer Experience \ud83d\udd34 Branch-dependent \ud83d\udfe1 Partial digital \ud83d\udfe2 Fully digital Competitive Position \ud83d\udd34 Falling behind \ud83d\udfe1 Table stakes \ud83d\udfe2 Differentiator Ecosystem Access \ud83d\udd34 Restricted \ud83d\udfe1 Basic partnerships \ud83d\udfe2 Full ecosystem Annual Cost $0 + fraud losses $100K-300K $300K-1M Expected ROI Negative 3-5x 10-20x"},{"location":"01-introduction/why-it-matters/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ol> <li>Face liveness is a business-critical security control, not just a technology feature</li> <li>Fraud prevention ROI typically exceeds 10x for mid-size banks</li> <li>Regulatory compliance increasingly mandates anti-spoofing globally</li> <li>Customer experience improves dramatically with digital onboarding</li> <li>Competitive positioning depends on digital KYC capability</li> <li>Real-world cases demonstrate catastrophic consequences of weak or absent liveness</li> </ol> <p>Next: The eKYC Pipeline \u2192</p>"},{"location":"02-fundamentals/active-liveness/","title":"2.1 Active Liveness Detection","text":""},{"location":"02-fundamentals/active-liveness/#overview","title":"Overview","text":"<p>Active liveness detection requires the user to perform specific actions in response to system-generated prompts. The system verifies both that the actions are performed correctly and that they exhibit characteristics consistent with a live human being.</p> <pre><code>sequenceDiagram\n    participant U as User\n    participant A as App (Client)\n    participant S as Server\n\n    A-&gt;&gt;S: Request liveness session\n    S-&gt;&gt;A: Session ID + Random challenge sequence\n    A-&gt;&gt;U: Display challenge: \"Turn head left\"\n    U-&gt;&gt;A: Performs action (camera captures)\n    A-&gt;&gt;A: Local face tracking &amp; quality check\n    A-&gt;&gt;U: Display challenge: \"Smile\"\n    U-&gt;&gt;A: Performs action\n    A-&gt;&gt;A: Local verification\n    A-&gt;&gt;S: Encrypted frames + metadata\n    S-&gt;&gt;S: Active challenge verification\n    S-&gt;&gt;S: Passive liveness analysis (parallel)\n    S-&gt;&gt;S: Deepfake detection (parallel)\n    S-&gt;&gt;A: Liveness result</code></pre>"},{"location":"02-fundamentals/active-liveness/#challenge-types-in-detail","title":"Challenge Types in Detail","text":""},{"location":"02-fundamentals/active-liveness/#1-head-movement-challenges","title":"1. Head Movement Challenges","text":"<p>The user is instructed to turn their head in a specific direction (left, right, up, down) or perform a sequence of movements.</p> <p>How It Works:</p> <pre><code>graph TD\n    A[\"Challenge Issued:&lt;br&gt;'Turn head left'\"] --&gt; B[\"Face Landmark&lt;br&gt;Tracking (478 points)\"]\n    B --&gt; C[\"3D Head Pose&lt;br&gt;Estimation\"]\n    C --&gt; D{\"Yaw angle&lt;br&gt;changed &gt; 15\u00b0?\"}\n    D --&gt;|Yes| E[\"Analyze Motion&lt;br&gt;Naturalness\"]\n    D --&gt;|No| F[\"Timeout / Retry\"]\n    E --&gt; G{\"Natural motion&lt;br&gt;characteristics?\"}\n    G --&gt;|Yes| H[\"\u2705 Challenge Passed\"]\n    G --&gt;|No| I[\"\u274c Suspicious Motion\"]\n\n    style H fill:#27ae60,color:#fff\n    style I fill:#e74c3c,color:#fff</code></pre> <p>What the system analyzes:</p> Signal Live Person Photo/Screen Attack 3D Mask Yaw/Pitch/Roll change Smooth, continuous None or rigid (whole image moves) Can simulate but lacks skin deformation Motion blur Natural blur at edges during rotation Absent or uniform blur Minimal, unnatural Parallax effects Nose tip moves faster than ears, depth-consistent No parallax \u2014 flat image Approximate parallax but incorrect for flexible features Skin deformation Neck skin folds, cheek compression None Absent or artificial Temporal dynamics Acceleration/deceleration curve matches human biomechanics Instantaneous or mechanical Approximate but measurable differences Background consistency Background perspective shifts with head movement Background moves with face (screen) or stays static (photo) Background may be visible around mask edges <p>Implementation parameters:</p> Parameter Recommended Value Rationale Minimum yaw change 15-25\u00b0 Below 15\u00b0 is too easy to fake; above 25\u00b0 causes user discomfort Maximum allowed time 3-5 seconds per direction Prevents coached/assisted attacks while accommodating normal users Minimum angular velocity 5\u00b0/second Ensures motion is intentional, not environmental vibration Frame rate for tracking 15-30 FPS Below 15 FPS loses motion detail; above 30 adds no value for most cameras Landmark model 478-point mesh (MediaPipe) or 68-point (dlib) 478-point provides superior head pose accuracy"},{"location":"02-fundamentals/active-liveness/#2-facial-expression-challenges","title":"2. Facial Expression Challenges","text":"<p>The user is prompted to perform specific facial expressions \u2014 smile, raise eyebrows, open mouth, close eyes.</p> <p>How It Works:</p> <p>The system uses the Facial Action Coding System (FACS) to verify genuine muscle movements. Each expression corresponds to specific Action Units (AUs):</p> Expression Primary Action Units What System Checks Smile AU6 (cheek raise) + AU12 (lip corner pull) Both AUs activate together (Duchenne smile indicators); cheek muscles lift; crow's feet appear around eyes Blink AU45 (blink) Lid closure speed (150-400ms natural), lid opening trajectory, simultaneous bilateral closure Raised Eyebrows AU1 (inner brow raise) + AU2 (outer brow raise) Forehead wrinkles appear; skin texture changes dynamically; natural asymmetry Open Mouth AU25 (lips part) + AU26 (jaw drop) Jaw hinge motion, teeth visibility progression, lip deformation Puffed Cheeks AU34 (puff) Bilateral cheek expansion, chin muscle tension, natural asymmetry <p>Key detection signals:</p> <ul> <li>Muscle activation authenticity: Real expressions involve coordinated muscle groups. A photo bent to simulate a smile doesn't produce the eye crinkle (AU6) or cheek lift.</li> <li>Temporal dynamics: Real expressions have natural onset (200-500ms), peak, and offset phases. Deepfakes often show unnatural timing.</li> <li>Micro-expression leakage: Before and after the requested expression, involuntary micro-expressions occur in genuine presentations.</li> <li>Skin texture change: Wrinkles appear and disappear dynamically with expressions; paper and screens can't replicate this.</li> </ul> <p>Accessibility Concern</p> <p>Expression-based challenges can be difficult or impossible for users with facial paralysis (Bell's palsy), Parkinson's disease, post-stroke conditions, or Moebius syndrome. Always provide alternative challenge types or passive liveness fallback for accessibility compliance.</p>"},{"location":"02-fundamentals/active-liveness/#3-gaze-tracking-challenges","title":"3. Gaze Tracking Challenges","text":"<p>The user follows a moving target on the screen with their eyes.</p> <p>How It Works:</p> <pre><code>graph LR\n    A[\"Random target&lt;br&gt;appears on screen\"] --&gt; B[\"User tracks&lt;br&gt;with eyes\"]\n    B --&gt; C[\"Iris position&lt;br&gt;extracted per frame\"]\n    C --&gt; D[\"Gaze trajectory&lt;br&gt;mapped\"]\n    D --&gt; E{\"Trajectory matches&lt;br&gt;target path?\"}\n    E --&gt;|Match| F[\"Analyze saccadic&lt;br&gt;patterns\"]\n    F --&gt; G{\"Natural eye&lt;br&gt;movement?\"}\n    G --&gt;|Yes| H[\"\u2705 Pass\"]\n    G --&gt;|No| I[\"\u274c Fail\"]\n\n    style H fill:#27ae60,color:#fff\n    style I fill:#e74c3c,color:#fff</code></pre> <p>What makes gaze tracking effective:</p> Signal Why It's Hard to Fake Saccadic movements Rapid, involuntary eye jumps between fixation points. Unique to live eyes. Average velocity: 300-500\u00b0/s Smooth pursuit Eyes track moving targets with a slight lag (50-100ms). Videos show no pupil tracking Vergence Eyes converge/diverge based on target distance. This requires real binocular vision Pupillary response Pupils dilate/constrict with light changes. Moving from bright to dark areas of screen produces measurable pupil change Vestibulo-ocular reflex Eyes counter-rotate when head moves to stabilize gaze. Photos can't exhibit this Microsaccades Tiny involuntary eye movements during fixation (0.2-1\u00b0). Present in all live eyes, absent in photos/videos"},{"location":"02-fundamentals/active-liveness/#4-color-sequence-illumination","title":"4. Color Sequence Illumination","text":"<p>The device screen flashes a random sequence of colors while the front camera captures the user's face.</p> <p>How It Works:</p> <pre><code>graph TD\n    A[\"Generate random&lt;br&gt;color sequence:&lt;br&gt;R\u2192G\u2192B\u2192W\u2192R\u2192B\"] --&gt; B[\"Flash each color&lt;br&gt;for 200-500ms\"]\n    B --&gt; C[\"Capture face under&lt;br&gt;each illumination\"]\n    C --&gt; D[\"Analyze per-color&lt;br&gt;reflectance response\"]\n    D --&gt; E{\"Response matches&lt;br&gt;expected skin&lt;br&gt;reflectance model?\"}\n    E --&gt;|Yes| F[\"\u2705 Live\"]\n    E --&gt;|No| G[\"\u274c Spoof\"]\n\n    style F fill:#27ae60,color:#fff\n    style G fill:#e74c3c,color:#fff</code></pre> <p>Why it works:</p> <ul> <li>Subsurface scattering: Real skin absorbs, scatters, and re-emits light differently at different wavelengths. Red light penetrates deeper into skin than blue light, causing different color-dependent reflectance patterns.</li> <li>Screen attacks fail: A screen displaying a face cannot react to the illumination from another screen. The pre-recorded face image has fixed lighting, so it doesn't change color naturally when illuminated.</li> <li>Print attacks fail: Paper has fundamentally different spectral reflectance than skin at every wavelength.</li> <li>3D mask attacks: Silicone and latex have different spectral absorption characteristics than skin, though this is the hardest case.</li> </ul> <p>Implementation considerations:</p> Parameter Value Notes Number of colors 4-8 More colors = higher security, longer process Duration per color 200-500ms Must be long enough for camera exposure adjustment Color randomization Per-session random sequence Prevents replay of pre-recorded color responses Color choices Red, Green, Blue, White, Cyan, Magenta, Yellow Primary and secondary colors for maximum spectral diversity Ambient light baseline Captured before sequence starts Used to normalize color response measurements"},{"location":"02-fundamentals/active-liveness/#5-speech-based-challenges","title":"5. Speech-Based Challenges","text":"<p>The user reads a randomly generated phrase, number sequence, or one-time code displayed on screen.</p> <p>How It Works:</p> <p>The system performs multi-modal analysis:</p> Analysis Layer What's Checked Lip-sync correlation Mouth movements match the spoken audio timing and phoneme shapes Voice liveness Audio characteristics confirm a live voice (not text-to-speech or replay) Content verification The spoken words match the displayed prompt (ASR verification) Temporal alignment Audio and video are synchronized within acceptable tolerance (&lt; 100ms) Replay detection Environmental acoustics match the visual environment; no echo/reverberation anomalies <p>Multi-Modal Strength</p> <p>Speech-based challenges are particularly powerful because they require coordination of visual (lip movements), audio (speech), and cognitive (reading comprehension) channels simultaneously. This makes it extremely difficult for any single attack modality to succeed.</p> <p>Limitations</p> <ul> <li>Not suitable for users with speech impairments, hearing loss, or in noisy environments</li> <li>Privacy concerns with voice capture in some jurisdictions</li> <li>Real-time lip-sync deepfakes (Wav2Lip) can now defeat basic lip-sync analysis</li> <li>Adds 5-10 seconds to the verification process</li> </ul>"},{"location":"02-fundamentals/active-liveness/#challenge-randomization-strategy","title":"Challenge Randomization Strategy","text":"<p>The randomization of challenges is critical \u2014 predictable challenges can be pre-recorded and replayed.</p> <pre><code>graph TD\n    A[\"Server generates&lt;br&gt;challenge pool\"] --&gt; B[\"Random selection&lt;br&gt;(cryptographically secure)\"]\n    B --&gt; C[\"Challenge sequence&lt;br&gt;generated\"]\n    C --&gt; D{\"Challenge type&lt;br&gt;randomization\"}\n    D --&gt; E[\"Random direction&lt;br&gt;(head movement)\"]\n    D --&gt; F[\"Random expression&lt;br&gt;(smile/blink/brow)\"]\n    D --&gt; G[\"Random gaze target&lt;br&gt;position &amp; path\"]\n    D --&gt; H[\"Random color&lt;br&gt;sequence\"]\n    D --&gt; I[\"Random speech&lt;br&gt;phrase\"]\n\n    E --&gt; J[\"Sequence: 2-4&lt;br&gt;challenges from&lt;br&gt;different types\"]\n    F --&gt; J\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n\n    J --&gt; K[\"Bound to session&lt;br&gt;with nonce +&lt;br&gt;timestamp\"]</code></pre> <p>Randomization principles:</p> <ol> <li>Challenge type randomization: Don't always use the same challenge type. Mix head movements with expressions with gaze tracking.</li> <li>Parameter randomization: Within each type, randomize direction (left vs right), expression (smile vs blink), target position, color sequence, and speech content.</li> <li>Sequence length randomization: Vary the number of challenges (2-4) per session.</li> <li>Timing unpredictability: Vary the delay between challenges (0.5-2 seconds).</li> <li>Server-side generation: Challenges must be generated server-side and bound to the session with cryptographic nonces. Client-side generation is trivially bypassable.</li> </ol>"},{"location":"02-fundamentals/active-liveness/#scoring-decision-logic","title":"Scoring &amp; Decision Logic","text":"<pre><code>graph TD\n    A[\"Challenge 1&lt;br&gt;Score: 0.92\"] --&gt; E[\"Weighted&lt;br&gt;Aggregation\"]\n    B[\"Challenge 2&lt;br&gt;Score: 0.87\"] --&gt; E\n    C[\"Challenge 3&lt;br&gt;Score: 0.95\"] --&gt; E\n    D[\"Temporal Consistency&lt;br&gt;Score: 0.90\"] --&gt; E\n\n    E --&gt; F[\"Active Liveness&lt;br&gt;Score: 0.91\"]\n    F --&gt; G{\"Threshold&lt;br&gt;Check\"}\n    G --&gt;|\"\u2265 0.85\"| H[\"\u2705 Active&lt;br&gt;Liveness Pass\"]\n    G --&gt;|\"0.60 - 0.85\"| I[\"\u26a0\ufe0f Additional&lt;br&gt;Challenge\"]\n    G --&gt;|\"&lt; 0.60\"| J[\"\u274c Active&lt;br&gt;Liveness Fail\"]\n\n    style H fill:#27ae60,color:#fff\n    style I fill:#f39c12,color:#fff\n    style J fill:#e74c3c,color:#fff</code></pre>"},{"location":"02-fundamentals/active-liveness/#advantages-disadvantages-summary","title":"Advantages &amp; Disadvantages Summary","text":"Aspect Rating Details Security against 2D attacks \u2b50\u2b50\u2b50\u2b50\u2b50 Challenge randomization makes photo/screen replay extremely difficult Security against 3D masks \u2b50\u2b50\u2b50\u2b50 Effective for rigid masks; flexible masks may simulate some movements Security against deepfakes \u2b50\u2b50\u2b50\u2b50 Multi-modal challenges increase difficulty; real-time deepfakes can partially respond User experience \u2b50\u2b50\u2b50 Adds 5-15 seconds; clear instructions needed; some users find it confusing Accessibility \u2b50\u2b50 Challenging for users with motor/visual/speech impairments Drop-off rate \u2b50\u2b50\u2b50 Typically 10-25% drop-off depending on challenge complexity and UX quality Processing speed \u2b50\u2b50\u2b50 5-15 seconds for full challenge sequence Device requirements \u2b50\u2b50\u2b50\u2b50 Standard front camera + display; no special hardware needed Regulatory acceptance \u2b50\u2b50\u2b50\u2b50\u2b50 Widely accepted; explainable security model that regulators understand"},{"location":"02-fundamentals/active-liveness/#best-practices-for-banking-deployment","title":"Best Practices for Banking Deployment","text":"<p>Implementation Recommendations</p> <ol> <li>Always combine with passive liveness \u2014 Active challenges alone miss texture/frequency signals</li> <li>Limit to 2-3 challenges per session to minimize drop-off</li> <li>Provide clear visual guidance \u2014 Animated overlays showing expected motion</li> <li>Implement progressive difficulty \u2014 Start easy, escalate if risk signals detected</li> <li>Offer accessibility alternatives \u2014 Passive-only mode or Video KYC fallback for users who can't perform challenges</li> <li>Server-side validation is mandatory \u2014 Never trust client-side challenge verification alone</li> <li>Monitor challenge completion rates \u2014 If a specific challenge has &gt;15% failure rate among genuine users, recalibrate or replace it</li> <li>Randomize everything \u2014 Challenge type, direction, sequence, timing</li> </ol> <p>Next: Passive Liveness Detection \u2192</p>"},{"location":"02-fundamentals/comparison-matrix/","title":"2.5 Comparison Matrix","text":""},{"location":"02-fundamentals/comparison-matrix/#comprehensive-comparison","title":"Comprehensive Comparison","text":"Criteria Active Passive Hybrid Hardware-Assisted User Experience \u2b50\u2b50\u2b50 Moderate \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent \u2b50\u2b50\u2b50\u2b50 Good \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent Security (2D) \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Security (3D Masks) \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Security (Deepfakes) \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Security (Injection) \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Accessibility \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 (device dependent) Drop-off Rate 10-25% 2-5% 5-10% 2-5% Processing Time 5-15s 1-3s 2-8s 1-2s Device Requirements Camera + Display Camera only Camera + Display Specialized sensors Regulatory Acceptance \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Growing \u2b50\u2b50\u2b50\u2b50\u2b50 Highest \u2b50\u2b50\u2b50\u2b50 Implementation Cost Medium Medium High Low (SDK) Maintenance Cost Medium Medium High Low Scalability Good Excellent Good Limited by device"},{"location":"02-fundamentals/comparison-matrix/#decision-matrix-for-banking","title":"Decision Matrix for Banking","text":"<pre><code>graph TD\n    A[\"What is your&lt;br&gt;primary deployment?\"] --&gt; B{\"Mobile App?\"}\n    B --&gt;|\"Yes\"| C{\"User base&lt;br&gt;device diversity?\"}\n    B --&gt;|\"No (Web/Kiosk)\"| D{\"Controlled&lt;br&gt;hardware?\"}\n\n    C --&gt;|\"High diversity&lt;br&gt;(India, SEA, Africa)\"| E[\"Hybrid:&lt;br&gt;Passive-first +&lt;br&gt;Active escalation\"]\n    C --&gt;|\"Flagship devices&lt;br&gt;(Developed markets)\"| F[\"Hybrid with&lt;br&gt;hardware signals&lt;br&gt;when available\"]\n\n    D --&gt;|\"Yes (ATM/Kiosk)\"| G[\"Hardware-assisted&lt;br&gt;(NIR + RGB)\"]\n    D --&gt;|\"No (Web browser)\"| H[\"Passive primary +&lt;br&gt;Device attestation\"]\n\n    style E fill:#1B4F72,color:#fff\n    style F fill:#2471A3,color:#fff\n    style G fill:#27ae60,color:#fff\n    style H fill:#f39c12,color:#fff</code></pre> <p>The Banking Recommendation</p> <p>Hybrid (passive-first with active escalation) is the recommended approach for most banking deployments. It provides the best balance of security, user experience, accessibility, and regulatory compliance.</p> <p>Next: Part III \u2014 Attack Taxonomy Overview \u2192</p>"},{"location":"02-fundamentals/hardware-assisted/","title":"2.4 Hardware-Assisted Liveness","text":""},{"location":"02-fundamentals/hardware-assisted/#overview","title":"Overview","text":"<p>Hardware-assisted liveness leverages specialized sensors beyond the standard RGB camera to provide additional modalities for distinguishing live faces from spoofs.</p>"},{"location":"02-fundamentals/hardware-assisted/#structured-light-apple-truedepth-android-equivalents","title":"Structured Light (Apple TrueDepth / Android Equivalents)","text":""},{"location":"02-fundamentals/hardware-assisted/#how-it-works","title":"How It Works","text":"<p>A projector emits a known pattern of infrared dots (30,000+ dots for Face ID) onto the face. An IR camera captures the deformed pattern. The deformation encodes 3D depth information.</p> <pre><code>graph LR\n    A[\"IR Dot&lt;br&gt;Projector\"] --&gt; B[\"30,000+ IR dots&lt;br&gt;projected onto face\"]\n    B --&gt; C[\"IR Camera&lt;br&gt;captures deformed&lt;br&gt;pattern\"]\n    C --&gt; D[\"Triangulation&lt;br&gt;algorithm\"]\n    D --&gt; E[\"Dense 3D&lt;br&gt;Depth Map\"]\n    E --&gt; F{\"3D face&lt;br&gt;geometry&lt;br&gt;consistent?\"}\n    F --&gt;|\"Yes\"| G[\"\u2705 Live\"]\n    F --&gt;|\"No\"| H[\"\u274c Flat surface&lt;br&gt;detected\"]\n\n    style G fill:#27ae60,color:#fff\n    style H fill:#e74c3c,color:#fff</code></pre> Property Details Accuracy Sub-millimeter depth precision Range 25-50cm (optimal for selfie distance) Speed Real-time (30 FPS depth maps) Attack resistance Defeats all 2D attacks; most 3D attacks except high-quality silicone masks Availability iPhone X and later (TrueDepth); select Android flagships (limited)"},{"location":"02-fundamentals/hardware-assisted/#limitations-for-banking","title":"Limitations for Banking","text":"<ul> <li>Device dependency: Only ~35% of global smartphone users have structured light sensors</li> <li>Cannot be mandated: Inclusive banking requires supporting all device types</li> <li>Use as enhancement: Use when available, fall back to RGB-only liveness when not</li> </ul>"},{"location":"02-fundamentals/hardware-assisted/#time-of-flight-tof-sensors","title":"Time-of-Flight (ToF) Sensors","text":"<p>Emits modulated light and measures the phase shift of returned light to calculate distance per pixel.</p> Property Details Accuracy 1-5mm depth precision Range 0.2-5 meters Speed 30-60 FPS Availability Samsung Galaxy S20+, Huawei P30 Pro, select others Limitation Lower resolution than structured light; susceptible to ambient IR interference"},{"location":"02-fundamentals/hardware-assisted/#near-infrared-nir-imaging","title":"Near-Infrared (NIR) Imaging","text":"<p>NIR cameras capture reflectance at 850-940nm wavelength, where materials behave very differently than in visible light.</p> Material NIR Behavior Visible Light Behavior Live skin Moderate absorption, subsurface scattering visible Normal skin appearance Paper High reflectance, uniform Shows printed texture LCD screen Very low emission (backlight doesn't emit NIR) Displays image OLED screen Near-zero NIR emission Displays image Silicone Different absorption pattern than skin May look similar to skin Latex High reflectance, different from skin May look similar to skin <p>NIR is Extremely Effective</p> <p>NIR imaging can distinguish live skin from screens with near-perfect accuracy because screens emit virtually no NIR light. This is why ATM and kiosk-based systems prefer NIR cameras.</p>"},{"location":"02-fundamentals/hardware-assisted/#recommendation-for-banking","title":"Recommendation for Banking","text":"Deployment Context Recommended Approach Mobile app (diverse devices) RGB-only liveness (passive + active), with hardware features as bonus signals when available ATM / Kiosk NIR camera + RGB (controlled hardware, can mandate sensor) Branch tablet Structured light if available (iPad Pro), else RGB liveness Web browser RGB only (no hardware sensor access) <p>Next: Comparison Matrix \u2192</p>"},{"location":"02-fundamentals/hybrid-liveness/","title":"2.3 Hybrid &amp; Adaptive Liveness","text":""},{"location":"02-fundamentals/hybrid-liveness/#overview","title":"Overview","text":"<p>Hybrid liveness detection combines active and passive methods into a unified system that adapts its behavior based on risk signals, environmental conditions, and confidence levels. This is the recommended approach for banking deployments \u2014 it provides the best balance of security, user experience, and regulatory compliance.</p>"},{"location":"02-fundamentals/hybrid-liveness/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"02-fundamentals/hybrid-liveness/#pattern-1-passive-first-with-active-escalation-recommended","title":"Pattern 1: Passive-First with Active Escalation (Recommended)","text":"<pre><code>graph TD\n    A[\"User takes selfie&lt;br&gt;or looks at camera\"] --&gt; B[\"Passive Liveness&lt;br&gt;Analysis\"]\n    B --&gt; C{\"Passive Score\"}\n    C --&gt;|\"\u2265 0.90&lt;br&gt;(High Confidence Live)\"| D[\"\u2705 Pass&lt;br&gt;(No active needed)\"]\n    C --&gt;|\"0.55 - 0.90&lt;br&gt;(Uncertain)\"| E[\"Trigger Active&lt;br&gt;Challenge\"]\n    C --&gt;|\"&lt; 0.55&lt;br&gt;(Likely Spoof)\"| F[\"\u274c Reject&lt;br&gt;(No recovery)\"]\n\n    E --&gt; G[\"Active Challenge&lt;br&gt;Score\"]\n    G --&gt; H{\"Combined&lt;br&gt;Score\"}\n    H --&gt;|\"Pass\"| I[\"\u2705 Verified\"]\n    H --&gt;|\"Fail\"| J[\"\u274c Reject\"]\n    H --&gt;|\"Still Uncertain\"| K[\"\u26a0\ufe0f Escalate to&lt;br&gt;Manual Review / V-CIP\"]\n\n    style D fill:#27ae60,color:#fff\n    style I fill:#27ae60,color:#fff\n    style F fill:#e74c3c,color:#fff\n    style J fill:#e74c3c,color:#fff\n    style K fill:#f39c12,color:#fff</code></pre> <p>Why this works best:</p> <ul> <li>80-90% of genuine users pass passive liveness with high confidence \u2014 they never see an active challenge</li> <li>Only uncertain cases (10-20%) are escalated to active challenges, minimizing friction</li> <li>Clear spoofs are rejected immediately without wasting the user's time on challenges</li> <li>Overall drop-off: 3-8% (vs. 10-25% for always-active)</li> </ul>"},{"location":"02-fundamentals/hybrid-liveness/#pattern-2-parallel-analysis","title":"Pattern 2: Parallel Analysis","text":"<p>Both active and passive analysis run simultaneously on the same video capture.</p> <pre><code>graph TD\n    A[\"Video Capture&lt;br&gt;(5-10 seconds)\"] --&gt; B[\"Passive Analysis&lt;br&gt;(every frame)\"]\n    A --&gt; C[\"Active Challenge&lt;br&gt;(during capture)\"]\n\n    B --&gt; D[\"Passive Score:&lt;br&gt;Texture + Depth +&lt;br&gt;Frequency\"]\n    C --&gt; E[\"Active Score:&lt;br&gt;Challenge compliance +&lt;br&gt;Motion naturalness\"]\n    A --&gt; F[\"Deepfake Score:&lt;br&gt;Temporal consistency +&lt;br&gt;Artifact detection\"]\n    A --&gt; G[\"Device Integrity&lt;br&gt;Score\"]\n\n    D --&gt; H[\"Score Fusion&lt;br&gt;Engine\"]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I{\"Final Decision\"}</code></pre>"},{"location":"02-fundamentals/hybrid-liveness/#pattern-3-risk-adaptive-most-sophisticated","title":"Pattern 3: Risk-Adaptive (Most Sophisticated)","text":"<p>The system dynamically adjusts its verification intensity based on pre-computed risk signals.</p> Risk Signal Low Risk Medium Risk High Risk Transaction value &lt; $1,000 $1,000 - $50,000 &gt; $50,000 Account age &gt; 2 years 6 months - 2 years New account Device Known device New but attested device Unknown / emulator / rooted Geolocation Home location Domestic travel International / VPN / proxy Behavioral biometrics Normal typing/swipe patterns Slightly anomalous Highly anomalous Time of day Normal hours Off-hours 2-5 AM local time Previous attempts First attempt 2nd attempt 3+ attempts Risk Level Liveness Approach Low Passive only (single selfie) Medium Passive + 1 active challenge High Passive + 2-3 active challenges + deepfake detection Critical Full active sequence + deepfake + device attestation + manual review queue"},{"location":"02-fundamentals/hybrid-liveness/#score-fusion-strategies","title":"Score Fusion Strategies","text":""},{"location":"02-fundamentals/hybrid-liveness/#weighted-linear-fusion","title":"Weighted Linear Fusion","text":"<pre><code>Final_Score = w\u2081 \u00d7 Passive_Score + w\u2082 \u00d7 Active_Score + w\u2083 \u00d7 Deepfake_Score + w\u2084 \u00d7 Device_Score\n</code></pre> <p>Typical weights for banking:</p> Component Weight Rationale Passive Liveness 0.35 Strong baseline signal from texture/depth/frequency Active Challenge 0.30 High-confidence signal from challenge-response Deepfake Detection 0.20 Essential for emerging threats Device Integrity 0.15 Prevents injection and tampering attacks"},{"location":"02-fundamentals/hybrid-liveness/#decision-level-fusion-voting","title":"Decision-Level Fusion (Voting)","text":"<p>Each subsystem makes an independent live/spoof decision. Final decision uses voting:</p> <ul> <li>All agree LIVE: Accept</li> <li>Majority LIVE, minority uncertain: Accept with monitoring flag</li> <li>Any subsystem says SPOOF with high confidence: Reject</li> <li>Split decision: Escalate to manual review</li> </ul>"},{"location":"02-fundamentals/hybrid-liveness/#learned-fusion-best-performance","title":"Learned Fusion (Best Performance)","text":"<p>A small neural network or gradient-boosted classifier learns to combine scores optimally from labeled data:</p> <pre><code>Input: [passive_score, active_score, deepfake_score, device_score, \n        face_quality, ambient_light, device_model, ...]\nOutput: final_liveness_probability\n</code></pre> <p>This learns non-linear relationships (e.g., \"low passive score is acceptable IF device integrity is perfect AND active score is very high\").</p>"},{"location":"02-fundamentals/hybrid-liveness/#implementation-recommendations-for-banking","title":"Implementation Recommendations for Banking","text":"<p>Best Practices</p> <ol> <li>Default to Pattern 1 (Passive-first with active escalation) for onboarding</li> <li>Use Pattern 3 (Risk-adaptive) for transaction authentication and step-up</li> <li>Always include deepfake detection as a parallel signal regardless of pattern</li> <li>Set aggressive passive thresholds for auto-pass (\u2265 0.90) and auto-reject (&lt; 0.55)</li> <li>Log everything \u2014 every score, every decision, every frame for audit</li> <li>A/B test thresholds with fraud rate and drop-off rate as dual metrics</li> <li>Review monthly: Attack landscape evolves; thresholds need recalibration</li> </ol> <p>Next: Hardware-Assisted Liveness \u2192</p>"},{"location":"02-fundamentals/passive-liveness/","title":"2.2 Passive Liveness Detection","text":""},{"location":"02-fundamentals/passive-liveness/#overview","title":"Overview","text":"<p>Passive liveness detection analyzes a single image or short video clip without requiring any explicit user interaction. The system extracts features from the captured face that distinguish live presentations from spoofed ones \u2014 all transparently, in the background.</p> <p>The User's Experience</p> <p>From the user's perspective, passive liveness is invisible. They simply take a selfie or look at the camera, and the system makes its determination. This results in a seamless, frictionless experience with the highest completion rates.</p> <pre><code>graph LR\n    A[\"\ud83d\udcf8 User takes&lt;br&gt;selfie / looks&lt;br&gt;at camera\"] --&gt; B[\"Face Detection&lt;br&gt;&amp; Quality Check\"]\n    B --&gt; C[\"Feature&lt;br&gt;Extraction\"]\n    C --&gt; D[\"Multi-Signal&lt;br&gt;Analysis\"]\n    D --&gt; E[\"Liveness&lt;br&gt;Score\"]\n    E --&gt; F{\"Decision\"}\n    F --&gt;|\"Live\"| G[\"\u2705 Continue&lt;br&gt;to face matching\"]\n    F --&gt;|\"Spoof\"| H[\"\u274c Reject\"]\n\n    style G fill:#27ae60,color:#fff\n    style H fill:#e74c3c,color:#fff</code></pre>"},{"location":"02-fundamentals/passive-liveness/#core-analysis-methods","title":"Core Analysis Methods","text":""},{"location":"02-fundamentals/passive-liveness/#1-texture-analysis-spatial-domain","title":"1. Texture Analysis (Spatial Domain)","text":"<p>The most fundamental passive liveness signal. Deep learning models analyze micro-texture patterns at the pixel level.</p> <p>What the model looks for:</p> Feature Live Skin Printed Photo Screen Display 3D Mask Pore structure Natural, irregular pore distribution Halftone dot patterns replace pores Pixel grid visible under analysis Smooth or artificially textured Skin micro-texture Rich, multi-scale texture with natural variation Ink dot patterns, paper fiber texture RGB sub-pixel patterns, aliasing Silicone/latex grain, paint texture Specular highlights Single, consistent environmental reflections on oily regions Matte paper: absent. Glossy: paper-like reflectance Glass/screen reflections overlaid on face Material-dependent, often too uniform Color distribution Wide, natural skin color gamut with subsurface warmth Limited printer gamut, shifted color balance Backlit colors, potential color banding Paint/pigment gamut, often slightly off Edge characteristics Smooth 3D-to-background transition with natural depth-of-field Sharp 2D cutout edges, paper boundary visible Screen bezel boundary, Moir\u00e9 at edges Mask-to-skin boundary visible <p>How texture models work internally:</p> <pre><code>graph TD\n    A[\"Input Face Image&lt;br&gt;224\u00d7224 or 256\u00d7256\"] --&gt; B[\"Backbone CNN&lt;br&gt;(MobileNetV3 / EfficientNet)\"]\n    B --&gt; C[\"Multi-Scale&lt;br&gt;Feature Maps\"]\n    C --&gt; D[\"Low-level features:&lt;br&gt;Edge patterns, noise&lt;br&gt;(Layers 1-3)\"]\n    C --&gt; E[\"Mid-level features:&lt;br&gt;Texture patterns, pores&lt;br&gt;(Layers 4-8)\"]\n    C --&gt; F[\"High-level features:&lt;br&gt;Semantic understanding&lt;br&gt;(Layers 9+)\"]\n    D --&gt; G[\"Feature&lt;br&gt;Aggregation\"]\n    E --&gt; G\n    F --&gt; G\n    G --&gt; H[\"Classification Head\"]\n    H --&gt; I[\"Liveness Score&lt;br&gt;0.0 \u2192 1.0\"]</code></pre>"},{"location":"02-fundamentals/passive-liveness/#2-depth-estimation","title":"2. Depth Estimation","text":"<p>Neural networks estimate the 3D geometry of the face from a single 2D image. Live faces produce consistent 3D depth maps; flat attacks produce anomalous depth.</p> <p>Depth map comparison:</p> Scenario Expected Depth Map Key Characteristics Live face Nose protrudes (closest to camera), eyes recessed in orbits, cheeks curve away, chin projects forward Smooth, anatomically consistent depth gradients; 40-80mm depth range across face Printed photo Flat plane with minor curvature from paper bending Near-uniform depth; no anatomical depth structure; &lt; 5mm depth variation Screen display Flat plane matching screen surface Perfectly uniform depth; possible slight concavity from screen curvature 3D rigid mask Approximate facial geometry but incorrect in fine details Exaggerated or incorrect nasal bridge, missing orbital depth, uniform surface without fine detail 3D flexible mask Close to real but with detectable differences Subtle geometric deviations, especially around eyes, nostrils, and mouth <p>Training approach for depth-based liveness:</p> <p>The model is trained with auxiliary depth supervision \u2014 alongside the liveness label, the model learns to predict a depth map:</p> <ul> <li>For live faces: Ground truth depth map generated from a 3D face reconstruction model (e.g., 3DDFA, DECA)</li> <li>For spoof faces: Ground truth depth is a zero/flat map</li> <li>The model learns that \"being live\" is associated with \"having valid 3D structure\"</li> </ul>"},{"location":"02-fundamentals/passive-liveness/#3-frequency-domain-analysis","title":"3. Frequency Domain Analysis","text":"<p>Fourier Transform and Wavelet analysis reveal frequency signatures characteristic of different media.</p> <pre><code>graph LR\n    A[\"Face Image\"] --&gt; B[\"2D FFT&lt;br&gt;(Fourier Transform)\"]\n    B --&gt; C[\"Power Spectrum\"]\n    C --&gt; D[\"Frequency&lt;br&gt;Signature Analysis\"]\n\n    D --&gt; E[\"High-frequency peaks&lt;br&gt;at regular intervals?&lt;br&gt;\u2192 Print halftone\"]\n    D --&gt; F[\"RGB sub-pixel&lt;br&gt;frequency peaks?&lt;br&gt;\u2192 Screen display\"]\n    D --&gt; G[\"Natural 1/f&lt;br&gt;noise falloff?&lt;br&gt;\u2192 Live face\"]\n    D --&gt; H[\"GAN upsampling&lt;br&gt;artifacts?&lt;br&gt;\u2192 Deepfake\"]</code></pre> <p>Frequency signatures by attack type:</p> Attack Type Frequency Domain Signature Live face Natural 1/f noise spectrum (power decreases inversely with frequency); sensor noise pattern consistent with camera model Laser/inkjet print Periodic peaks corresponding to halftone screen frequency (typically 150-300 LPI); paper texture frequency LCD screen Peaks at pixel pitch frequency (varies with screen PPI); RGB sub-pixel pattern frequencies; backlight frequency OLED screen Sub-pixel pattern (PenTile or RGB stripe); different from LCD due to pixel layout GAN-generated Upsampling artifacts at specific frequencies; checkerboard patterns from transposed convolutions; GAN fingerprint Deepfake (face swap) Blending boundary frequencies; inconsistent noise patterns between swapped face and surrounding area"},{"location":"02-fundamentals/passive-liveness/#4-reflection-specularity-analysis","title":"4. Reflection &amp; Specularity Analysis","text":"<p>Analysis of how light interacts with the presentation surface.</p> <p>Key signals:</p> <ul> <li>Corneal reflections: Live eyes show clear reflections of the environment (lights, windows, screens). The reflection should be consistent with the ambient scene and identical in both eyes. Screens show screen-within-screen reflections.</li> <li>Skin specularity: Oily skin areas (T-zone: forehead, nose, chin) show specular highlights that move consistently with head motion and lighting direction.</li> <li>Double reflection: Screen-based attacks often show two reflection sources \u2014 one from the original scene captured in the photo/video, and one from the attack screen surface.</li> <li>Polarization cues: Though not capturable with standard cameras, the principle applies \u2014 reflections from glass (screens) are partially polarized differently than reflections from skin.</li> </ul>"},{"location":"02-fundamentals/passive-liveness/#5-image-quality-artifact-detection","title":"5. Image Quality &amp; Artifact Detection","text":"<p>Detection of artifacts that indicate a non-live source.</p> Artifact Indicates Detection Method Moir\u00e9 patterns Screen capture of another screen Frequency analysis for periodic interference patterns JPEG compression artifacts Re-compressed image (not fresh capture) Block boundary analysis, quantization table detection Color banding Limited bit depth or color gamut compression Gradient analysis in smooth regions (cheeks, forehead) Pixel repetition Digital zoom or upscaling Auto-correlation analysis for repeating pixel patterns Edge ringing Sharpening artifacts from processing High-pass filter analysis near strong edges Noise inconsistency Composited image (different noise levels in face vs background) Local noise estimation across image regions Lens distortion absence Non-camera source (rendered or stitched image) Radial distortion model fitting EXIF metadata anomalies Modified or fabricated image Metadata consistency checks (though unreliable \u2014 easily faked)"},{"location":"02-fundamentals/passive-liveness/#6-remote-photoplethysmography-rppg","title":"6. Remote Photoplethysmography (rPPG)","text":"<p>Even from a \"single image\" passive approach, short video clips (2-5 seconds) enable rPPG analysis \u2014 one of the strongest passive liveness signals.</p> <pre><code>graph TD\n    A[\"Short video clip&lt;br&gt;(2-5 seconds, 15-30 FPS)\"] --&gt; B[\"Face region&lt;br&gt;of interest (ROI)&lt;br&gt;extraction\"]\n    B --&gt; C[\"Per-frame mean&lt;br&gt;color values&lt;br&gt;(R, G, B channels)\"]\n    C --&gt; D[\"Temporal signal&lt;br&gt;extraction\"]\n    D --&gt; E[\"Bandpass filter&lt;br&gt;(0.7 - 4.0 Hz)&lt;br&gt;= 42-240 BPM\"]\n    E --&gt; F[\"FFT / Peak&lt;br&gt;detection\"]\n    F --&gt; G{\"Periodic signal&lt;br&gt;detected at&lt;br&gt;heart rate frequency?\"}\n    G --&gt;|\"Yes: Clear periodic signal\"| H[\"\u2705 Live&lt;br&gt;(heart beating)\"]\n    G --&gt;|\"No: Flat/noisy signal\"| I[\"\u274c Spoof&lt;br&gt;(no blood flow)\"]\n\n    style H fill:#27ae60,color:#fff\n    style I fill:#e74c3c,color:#fff</code></pre> <p>Why rPPG is powerful:</p> <ul> <li>Blood flow causes micro-color changes in skin (imperceptible to the human eye but detectable by cameras) synchronized with the heartbeat</li> <li>No known attack can synthetically reproduce physiologically accurate rPPG signals in real-time</li> <li>Works as a strong supplementary signal even when other passive methods are uncertain</li> <li>Limitation: Requires 2-5 second video, not a single frame; affected by motion, compression, and very dark skin tones</li> </ul>"},{"location":"02-fundamentals/passive-liveness/#model-architecture-for-passive-liveness","title":"Model Architecture for Passive Liveness","text":""},{"location":"02-fundamentals/passive-liveness/#recommended-architecture-multi-task-learning","title":"Recommended Architecture: Multi-Task Learning","text":"<pre><code>graph TD\n    A[\"Input: Face Image&lt;br&gt;256\u00d7256\u00d73\"] --&gt; B[\"Shared Backbone&lt;br&gt;(EfficientNet-B0 /&lt;br&gt;MobileNetV3-Large)\"]\n    B --&gt; C[\"Feature Maps&lt;br&gt;Multi-scale\"]\n\n    C --&gt; D[\"Head 1:&lt;br&gt;Binary Liveness&lt;br&gt;(Live / Spoof)\"]\n    C --&gt; E[\"Head 2:&lt;br&gt;Depth Map&lt;br&gt;Estimation\"]\n    C --&gt; F[\"Head 3:&lt;br&gt;Attack Type&lt;br&gt;Classification\"]\n    C --&gt; G[\"Head 4:&lt;br&gt;Domain Classifier&lt;br&gt;(for DG training)\"]\n\n    D --&gt; H[\"Liveness&lt;br&gt;Score\"]\n    E --&gt; I[\"Depth Map&lt;br&gt;32\u00d732\"]\n    F --&gt; J[\"Attack Type&lt;br&gt;Probabilities\"]\n    G --&gt; K[\"Domain&lt;br&gt;Prediction\"]\n\n    H --&gt; L[\"Score Fusion&lt;br&gt;&amp; Decision\"]\n    I --&gt; L\n    J --&gt; L</code></pre> <p>Multi-task benefits:</p> <ul> <li>Depth map supervision provides better gradient signal than binary classification alone</li> <li>Attack type classification enables interpretable decisions (know what type of attack was detected)</li> <li>Domain classifier (with gradient reversal) enables domain-invariant features for better generalization</li> </ul>"},{"location":"02-fundamentals/passive-liveness/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Aspect Rating Details User experience \u2b50\u2b50\u2b50\u2b50\u2b50 Zero friction \u2014 user just takes a selfie; highest completion rates Accessibility \u2b50\u2b50\u2b50\u2b50\u2b50 No motor/speech/cognitive requirements; works for all users Processing speed \u2b50\u2b50\u2b50\u2b50\u2b50 100-500ms for single-frame; 1-3s with short video Drop-off rate \u2b50\u2b50\u2b50\u2b50\u2b50 2-5% (mostly due to camera quality, not liveness UX) Security (2D attacks) \u2b50\u2b50\u2b50\u2b50 Strong against prints and basic screen replays Security (3D masks) \u2b50\u2b50\u2b50 Moderate \u2014 texture analysis helps but geometry can fool depth models Security (deepfakes) \u2b50\u2b50\u2b50 Moderate \u2014 depends on deepfake quality and detector sophistication Security (injection attacks) \u2b50\u2b50 Weak if only analyzing image content without device/session validation Regulatory acceptance \u2b50\u2b50\u2b50\u2b50 Growing acceptance; some regulators still prefer active challenge evidence Explainability \u2b50\u2b50\u2b50 Harder to explain \"why rejected\" to regulators compared to active methods"},{"location":"02-fundamentals/passive-liveness/#when-to-use-passive-liveness","title":"When to Use Passive Liveness","text":"<p>Ideal For</p> <ul> <li>High-volume, low-friction onboarding where drop-off reduction is critical</li> <li>First-pass screening before active challenge escalation</li> <li>Inclusive deployments where accessibility is a hard requirement</li> <li>Markets where users are less tech-savvy and active challenges cause confusion</li> <li>Transaction authentication where speed is essential</li> </ul> <p>Not Sufficient Alone For</p> <ul> <li>High-value banking transactions (combine with active or multi-modal)</li> <li>Jurisdictions where regulators explicitly require active challenge evidence</li> <li>Deployments where sophisticated deepfake attacks are a primary threat</li> <li>Scenarios where virtual camera injection is a known attack vector</li> </ul> <p>Next: Hybrid &amp; Adaptive Liveness \u2192</p>"},{"location":"03-attack-landscape/adversarial-ml/","title":"3.7 Adversarial Machine Learning Attacks","text":""},{"location":"03-attack-landscape/adversarial-ml/#overview","title":"Overview","text":"<p>Adversarial ML attacks target the machine learning models powering the liveness system. Instead of fooling the camera with physical artifacts, they fool the neural network with carefully crafted input modifications.</p>"},{"location":"03-attack-landscape/adversarial-ml/#attack-types","title":"Attack Types","text":""},{"location":"03-attack-landscape/adversarial-ml/#white-box-attacks-attacker-has-model-access","title":"White-Box Attacks (Attacker Has Model Access)","text":"<pre><code>graph LR\n    A[\"Attacker obtains&lt;br&gt;model weights\"] --&gt; B[\"Computes gradient&lt;br&gt;of loss w.r.t.&lt;br&gt;input pixels\"]\n    B --&gt; C[\"Adds small&lt;br&gt;perturbation&lt;br&gt;to spoof image\"]\n    C --&gt; D[\"Perturbed image&lt;br&gt;fools model into&lt;br&gt;predicting 'live'\"]</code></pre> Method Perturbation Size Success Rate Detectability FGSM (Fast Gradient Sign) \u03b5 = 4/255 60-80% Low perturbation \u2014 nearly invisible PGD (Projected Gradient Descent) \u03b5 = 8/255 85-95% Moderate perturbation C&amp;W Attack Minimal (optimized) 95-99% Minimal \u2014 optimized for imperceptibility AutoAttack Varies 90-98% Ensemble of attacks"},{"location":"03-attack-landscape/adversarial-ml/#black-box-attacks-no-model-access","title":"Black-Box Attacks (No Model Access)","text":"Method Approach Queries Needed Success Rate Transfer attack Craft adversarial on surrogate model; hope it transfers 0 (transfer) 30-60% Score-based Use returned scores to estimate gradients 1,000-10,000 50-80% Decision-based Only uses accept/reject decisions 10,000-100,000 40-70% Query-efficient Combines transfer + score-based 100-1,000 60-85%"},{"location":"03-attack-landscape/adversarial-ml/#model-extraction","title":"Model Extraction","text":"<pre><code>graph TD\n    A[\"Attacker queries&lt;br&gt;liveness API&lt;br&gt;with diverse inputs\"] --&gt; B[\"Collects input-score&lt;br&gt;pairs (thousands)\"]\n    B --&gt; C[\"Trains surrogate&lt;br&gt;model to mimic&lt;br&gt;the API\"]\n    C --&gt; D[\"Crafts white-box&lt;br&gt;adversarial examples&lt;br&gt;on surrogate\"]\n    D --&gt; E[\"Adversarial examples&lt;br&gt;transfer to real model\"]</code></pre>"},{"location":"03-attack-landscape/adversarial-ml/#defenses","title":"Defenses","text":"Defense Mechanism Effectiveness Cost Adversarial training Include adversarial examples in training data \ud83d\udfe2 High \u2014 most effective single defense Training time +50-100% Input preprocessing JPEG compression, spatial smoothing, bit-depth reduction before model \ud83d\udfe1 Moderate \u2014 destroys some perturbations Minimal Ensemble models Multiple diverse models; adversarial for one unlikely to fool all \ud83d\udfe2 High \u2014 diversity is key Inference cost \u00d7N Randomized smoothing Add random noise to input; average predictions over multiple samples \ud83d\udfe1 Moderate \u2014 certified robustness Inference time \u00d7K Rate limiting Limit API queries per device/IP/session \ud83d\udfe2 High against query-based attacks Minimal Score obfuscation Return binary decision instead of continuous score; add noise to scores \ud83d\udfe2 High against score-based attacks Minimal Model diversity Rotate between different model architectures periodically \ud83d\udfe1 Moderate \u2014 prevents persistent extraction Maintenance overhead Input anomaly detection Detect statistically anomalous input patterns that suggest adversarial manipulation \ud83d\udfe1 Moderate Additional model required"},{"location":"03-attack-landscape/adversarial-ml/#banking-recommendation","title":"Banking Recommendation","text":"<p>Defense-in-Depth Against Adversarial Attacks</p> <ol> <li>Adversarial training as standard practice in model training pipeline</li> <li>Never return raw scores to the client \u2014 binary decisions only via API</li> <li>Rate limit aggressively \u2014 max 3-5 attempts per session, 10 per device per day</li> <li>Ensemble of 2-3 diverse models for production inference</li> <li>Monitor score distributions \u2014 sudden shifts indicate possible adversarial probing</li> <li>Rotate models quarterly with architectural diversity</li> </ol> <p>Back to: Attack Taxonomy Overview \u2192</p>"},{"location":"03-attack-landscape/ai-generative-attacks/","title":"3.4 AI &amp; Generative Attacks","text":""},{"location":"03-attack-landscape/ai-generative-attacks/#overview","title":"Overview","text":"<p>AI-powered attacks represent the fastest-evolving threat to face liveness systems. The quality of AI-generated facial content improves dramatically every few months, consistently outpacing detection capabilities.</p>"},{"location":"03-attack-landscape/ai-generative-attacks/#deepfake-subtypes","title":"Deepfake Subtypes","text":""},{"location":"03-attack-landscape/ai-generative-attacks/#face-swap","title":"Face Swap","text":"<p>Replaces one person's face with another's while maintaining original head movements.</p> Tool Quality Real-time? Free? DeepFaceLab Very High No (offline) Yes FaceSwap High No (offline) Yes Roop / ReActor High Near real-time Yes InsightFace inswapper Very High Yes Yes DeepFaceLive High Yes (real-time) Yes"},{"location":"03-attack-landscape/ai-generative-attacks/#face-reenactment","title":"Face Reenactment","text":"<p>Transfers expressions from attacker to target face identity in real-time.</p> Tool Quality Latency Key Risk First Order Motion Model Good ~100ms Can drive any face from a single photo LivePortrait Very High ~50ms Extremely realistic expression transfer MegaPortraits Excellent ~80ms State-of-the-art quality Thin-Plate Spline Motion Good ~120ms Works from a single source image"},{"location":"03-attack-landscape/ai-generative-attacks/#lip-sync","title":"Lip Sync","text":"<p>Synchronizes mouth movements to arbitrary audio \u2014 used to bypass speech-based liveness challenges.</p> Tool Quality Use Case Wav2Lip Good Lip sync to any audio SadTalker Very Good Full head animation + lip sync VideoReTalking Excellent High-fidelity talking head EMO Excellent Emotion-preserving animation"},{"location":"03-attack-landscape/ai-generative-attacks/#synthetic-identity-generation","title":"Synthetic Identity Generation","text":"<pre><code>graph TD\n    A[\"GAN / Diffusion Model&lt;br&gt;generates face that&lt;br&gt;doesn't exist\"] --&gt; B[\"Paired with&lt;br&gt;fabricated identity&lt;br&gt;documents\"]\n    B --&gt; C[\"Complete synthetic&lt;br&gt;identity package\"]\n    C --&gt; D[\"Passes liveness&lt;br&gt;(face is 'real'&lt;br&gt;in liveness sense)\"]\n    C --&gt; E[\"Passes face matching&lt;br&gt;(document and face&lt;br&gt;were created together)\"]\n    D --&gt; F[\"Account opened under&lt;br&gt;completely fake identity\"]\n    E --&gt; F</code></pre> <p>Detection approaches: - GAN fingerprint detection: GANs leave characteristic frequency-domain artifacts - Face quality anomaly: Synthetic faces often have subtle quality anomalies (too perfect symmetry, unusual ear/hair detail) - Cross-database checks: Duplicate face across multiple identities - Document forensics: Detect the fabricated document independently</p>"},{"location":"03-attack-landscape/ai-generative-attacks/#morphing-attacks","title":"Morphing Attacks","text":"<p>Blending two faces so the resulting image matches both identities. Used primarily for document fraud (putting a morphed photo on an ID that can verify against two different people).</p> Morphing Method Quality Detection Difficulty Landmark-based warping Moderate \ud83d\udfe1 Medium \u2014 visible artifacts at blending boundaries GAN-based morphing (MorGAN, MIPGAN) High \ud83d\udd34 High \u2014 fewer visible artifacts Diffusion-based morphing Very High \ud83d\udd34\ud83d\udd34 Very High \u2014 state of the art quality"},{"location":"03-attack-landscape/ai-generative-attacks/#defense-strategy","title":"Defense Strategy","text":"<p>Multi-Layer AI Attack Defense</p> <ol> <li>Temporal consistency: Analyze frame-to-frame consistency \u2014 deepfakes flicker at face boundaries</li> <li>Physiological signals (rPPG): Blood flow detection is extremely hard to synthesize</li> <li>Forensic frequency analysis: GAN fingerprints, upsampling artifacts detectable in FFT</li> <li>Environmental consistency: Lighting, reflections, and background should be physically plausible</li> <li>Ensemble detection: Multiple detectors trained on different architectures</li> <li>Continuous retraining: Monthly model updates with latest attack samples</li> <li>Active challenges: Randomized, multi-modal challenges increase attack difficulty</li> </ol> <p>Next: Deepfakes \u2014 The Evolving Threat \u2192</p>"},{"location":"03-attack-landscape/deepfakes/","title":"3.5 Deepfakes \u2014 The Evolving Threat","text":""},{"location":"03-attack-landscape/deepfakes/#the-arms-race","title":"The Arms Race","text":"<pre><code>graph LR\n    A[\"2017: Basic&lt;br&gt;face swaps&lt;br&gt;(minutes to create,&lt;br&gt;easy to detect)\"] --&gt; B[\"2019: DeepFaceLab&lt;br&gt;(hours, moderate&lt;br&gt;detection)\"]\n    B --&gt; C[\"2021: Real-time&lt;br&gt;face swap&lt;br&gt;(instant, harder&lt;br&gt;to detect)\"]\n    C --&gt; D[\"2023: LivePortrait&lt;br&gt;One-shot animation&lt;br&gt;(seconds from single&lt;br&gt;photo, very hard)\"]\n    D --&gt; E[\"2025+: Neural&lt;br&gt;avatars, 3D Gaussians&lt;br&gt;(real-time, near&lt;br&gt;indistinguishable)\"]</code></pre>"},{"location":"03-attack-landscape/deepfakes/#why-deepfakes-are-uniquely-dangerous-for-banking","title":"Why Deepfakes Are Uniquely Dangerous for Banking","text":"<ol> <li>Low barrier to entry: Free tools, consumer GPUs, tutorial videos</li> <li>Real-time capability: Can respond to active liveness challenges live</li> <li>Scalability: One model can generate unlimited attack attempts</li> <li>Improving faster than detection: Generative AI advances outpace defensive AI</li> <li>Cross-modal: Can combine face + voice for multi-modal bypass</li> </ol>"},{"location":"03-attack-landscape/deepfakes/#detection-signals","title":"Detection Signals","text":"Signal What To Look For Reliability Temporal flickering Face boundary flickers between frames Good for lower-quality deepfakes; fading for state-of-the-art Blending boundary Visible seam where swapped face meets original Good \u2014 most swaps still show subtle boundaries Frequency artifacts GAN upsampling creates checkerboard patterns in FFT Good but increasingly addressed by newer models Eye reflection inconsistency Reflections in left vs right eye should match; deepfakes often don't Moderate \u2014 improving in newer methods Physiological absence (rPPG) No detectable heart rate signal in synthetic face Excellent \u2014 extremely hard to fake Lip sync quality Subtle timing/shape mismatches between audio and mouth Moderate \u2014 tools like Wav2Lip are very good Hair/ear/neck artifacts Deepfakes struggle with fine hair detail, ear consistency, neck blending Good supplementary signal Background consistency Face processing may leave background untouched, creating lighting/color mismatch Moderate"},{"location":"03-attack-landscape/deepfakes/#anti-deepfake-architecture","title":"Anti-Deepfake Architecture","text":"<pre><code>graph TD\n    A[\"Input Video&lt;br&gt;(N frames)\"] --&gt; B[\"Frame-level&lt;br&gt;Deepfake Detector&lt;br&gt;(per-frame score)\"]\n    A --&gt; C[\"Temporal&lt;br&gt;Consistency&lt;br&gt;Analyzer\"]\n    A --&gt; D[\"rPPG Extractor&lt;br&gt;(physiological&lt;br&gt;signal)\"]\n    A --&gt; E[\"Forensic&lt;br&gt;Frequency&lt;br&gt;Analyzer\"]\n\n    B --&gt; F[\"Ensemble&lt;br&gt;Fusion\"]\n    C --&gt; F\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt; G{\"Deepfake&lt;br&gt;Probability\"}\n    G --&gt;|\"&gt; 0.7\"| H[\"\u274c Deepfake&lt;br&gt;Detected\"]\n    G --&gt;|\"0.3 - 0.7\"| I[\"\u26a0\ufe0f Uncertain&lt;br&gt;Escalate\"]\n    G --&gt;|\"&lt; 0.3\"| J[\"\u2705 Likely&lt;br&gt;Genuine\"]</code></pre>"},{"location":"03-attack-landscape/deepfakes/#recommended-research-papers","title":"Recommended Research Papers","text":"<ul> <li>FaceForensics++ (R\u00f6ssler et al., 2019) \u2014 benchmark dataset</li> <li>Thinking in Frequency (Qian et al., 2020) \u2014 frequency analysis approach</li> <li>Multi-Attentional Deepfake Detection (Zhao et al., 2021) \u2014 attention-based</li> <li>DeepfakeBench (Yan et al., 2023) \u2014 comprehensive benchmark</li> <li>Implicit Identity Leakage (Dong et al., 2023) \u2014 identity-based detection</li> </ul> <p>Next: Social Engineering &amp; Process Attacks \u2192</p>"},{"location":"03-attack-landscape/digital-attacks/","title":"3.3 Digital &amp; Injection Attacks","text":""},{"location":"03-attack-landscape/digital-attacks/#overview","title":"Overview","text":"<p>Digital attacks bypass the physical camera entirely, injecting manipulated content into the capture pipeline at various points. These are particularly dangerous because they circumvent all physical-level defenses (texture, specularity, depth from structured light).</p>"},{"location":"03-attack-landscape/digital-attacks/#virtual-camera-injection","title":"Virtual Camera Injection","text":""},{"location":"03-attack-landscape/digital-attacks/#attack-flow","title":"Attack Flow","text":"<pre><code>graph TD\n    A[\"Attacker installs&lt;br&gt;virtual camera software&lt;br&gt;(OBS, ManyCam, v4l2loopback)\"] --&gt; B[\"Feeds pre-recorded&lt;br&gt;or deepfake video&lt;br&gt;as camera input\"]\n    B --&gt; C[\"Banking app opens&lt;br&gt;'camera' \u2014 sees&lt;br&gt;virtual camera feed\"]\n    C --&gt; D[\"Liveness system&lt;br&gt;receives injected&lt;br&gt;frames\"]</code></pre>"},{"location":"03-attack-landscape/digital-attacks/#detection-methods","title":"Detection Methods","text":"Method How It Works Effectiveness Virtual camera driver detection Enumerate camera devices; flag known virtual camera names (OBS, ManyCam, DroidCam) \ud83d\udfe1 Moderate \u2014 can be renamed Camera API integrity check Verify frames come from hardware camera API, not software source \ud83d\udfe1 Moderate \u2014 hookable Device attestation SafetyNet (Android) / DeviceCheck (iOS) confirms device integrity \ud83d\udfe2 High \u2014 hard to bypass on non-rooted devices Root/Jailbreak detection Detect rooted Android or jailbroken iOS \ud83d\udfe2 High \u2014 required for most injection attacks Frame metadata validation Check EXIF, camera sensor metadata, frame timing consistency \ud83d\udfe1 Moderate \u2014 metadata can be faked Sensor correlation Correlate camera frames with gyroscope/accelerometer data (hand movement should match video motion) \ud83d\udfe2 High \u2014 very difficult to fake"},{"location":"03-attack-landscape/digital-attacks/#camera-api-hooking","title":"Camera API Hooking","text":""},{"location":"03-attack-landscape/digital-attacks/#android-frida-xposed","title":"Android (Frida / Xposed)","text":"<pre><code>graph LR\n    A[\"Camera&lt;br&gt;Hardware\"] --&gt; B[\"Camera HAL&lt;br&gt;(Hardware Abstraction)\"]\n    B --&gt; C[\"Camera2 API&lt;br&gt;/ CameraX\"]\n    C --&gt; D[\"\u26a0\ufe0f HOOK POINT:&lt;br&gt;Frida/Xposed&lt;br&gt;intercepts here\"]\n    D --&gt; E[\"Banking App&lt;br&gt;(receives modified frames)\"]</code></pre> <p>Defense layers:</p> <ol> <li>SDK integrity verification \u2014 Detect if SDK code has been modified</li> <li>Anti-debugging \u2014 Detect Frida, GDB, LLDB, and other debuggers</li> <li>Anti-hooking \u2014 Detect Xposed, Magisk, and substrate frameworks</li> <li>Code obfuscation \u2014 Make hooking targets harder to identify</li> <li>Server-side validation \u2014 Never trust client-side liveness results alone</li> </ol>"},{"location":"03-attack-landscape/digital-attacks/#api-network-attacks","title":"API &amp; Network Attacks","text":"Attack Description Defense API Replay Capture and replay a valid liveness request Session-bound nonces, timestamp validation, one-time tokens Man-in-the-Middle Intercept and modify video frames in transit TLS certificate pinning, end-to-end frame encryption Parameter Tampering Modify liveness scores in API requests Server-side score computation, signed payloads, HMAC verification Rate Limiting Bypass Brute-force multiple attack attempts Per-device, per-IP, per-session rate limiting with exponential backoff"},{"location":"03-attack-landscape/digital-attacks/#sdk-tampering","title":"SDK Tampering","text":"Attack Description Defense APK Repackaging Decompile app, disable liveness, recompile Code obfuscation (ProGuard/R8), integrity checks, Google Play Integrity API Runtime Method Hooking Hook liveness SDK methods to return \"pass\" Anti-hooking detection, method integrity verification WebRTC Manipulation Modify browser media stream for web liveness Server-side analysis, challenge-response binding, no client-side trust"},{"location":"03-attack-landscape/digital-attacks/#relay-proxy-attacks","title":"Relay / Proxy Attacks","text":"<p>A legitimate person completes liveness at Location A, but their camera feed is relayed to Location B where the KYC is being performed.</p> <p>Particularly Dangerous</p> <p>Relay attacks pass all liveness checks because a real person IS performing them. The system must detect that the location/device context is wrong, not that the face is fake.</p> <p>Detection: - Geolocation verification (GPS + IP + cell tower triangulation) - Network latency analysis (relay introduces measurable delay) - Device fingerprinting (ensure consistent device across session)</p> <p>Next: AI &amp; Generative Attacks \u2192</p>"},{"location":"03-attack-landscape/physical-attacks/","title":"3.2 Physical Presentation Attacks","text":""},{"location":"03-attack-landscape/physical-attacks/#overview","title":"Overview","text":"<p>Physical presentation attacks involve presenting a tangible artifact to the camera sensor. These are the most common attack type, accounting for approximately 70-80% of real-world liveness attack attempts against banking systems.</p>"},{"location":"03-attack-landscape/physical-attacks/#2d-flat-attacks","title":"2D Flat Attacks","text":""},{"location":"03-attack-landscape/physical-attacks/#print-attacks","title":"Print Attacks","text":"<pre><code>graph TD\n    subgraph \"Print Attack Variants\"\n        A[\"Standard office&lt;br&gt;print (A4)&lt;br&gt;\ud83d\udfe2 Easy to detect\"]\n        B[\"Photo lab print&lt;br&gt;(glossy/matte)&lt;br&gt;\ud83d\udfe1 Moderate\"]\n        C[\"Large format&lt;br&gt;poster print&lt;br&gt;\ud83d\udfe1 Moderate\"]\n        D[\"Curved/bent&lt;br&gt;photo on form&lt;br&gt;\ud83d\udfe1 Medium\"]\n        E[\"Photo with eye&lt;br&gt;cutouts (attacker&lt;br&gt;blinks through)&lt;br&gt;\ud83d\udfe1 Medium\"]\n    end</code></pre> Variant Detection Signals Recommended Defenses Standard print Halftone dot patterns visible in frequency analysis; paper texture in micro-texture; flat depth map; no blink/motion; paper edges visible Texture CNN + frequency analysis + depth estimation Photo lab print (glossy) Glossy surface reflections differ from skin specularity; still shows printer artifacts at high magnification; flat depth Specularity analysis + depth estimation + Moir\u00e9 detection Photo lab print (matte) Matte paper lacks skin subsurface scattering; paper fiber texture detectable; flat depth Texture analysis + subsurface scattering estimation Large format Same as standard but more detail available; may have visible seams/folds Same as standard \u2014 larger prints don't fool modern systems Curved photo Partial depth signal present but doesn't match anatomical facial geometry; creasing/folding visible; unnatural curvature Depth consistency check (is depth map anatomically valid?) Eye cutout mask Discontinuity at eye boundary (real eyes in paper face); lighting mismatch between real eyes and printed face; unnatural face boundary Edge discontinuity detection + lighting consistency + face boundary analysis"},{"location":"03-attack-landscape/physical-attacks/#screen-replay-attacks","title":"Screen Replay Attacks","text":"Variant Detection Signals Recommended Defenses Phone screen (photo) Screen Moir\u00e9 patterns; pixel grid in frequency domain; screen bezel visible; color gamut limited; screen reflections Moir\u00e9 detection + frequency analysis + bezel detection Phone screen (video) Same as photo + video may show compression artifacts; temporal patterns from screen refresh rate Above + refresh rate detection + compression artifact analysis Tablet screen (photo/video) Higher resolution makes Moir\u00e9 weaker; larger display more convincing; same fundamental artifacts More sensitive Moir\u00e9 detection + multi-scale frequency analysis Monitor screen (photo/video) Highest resolution; weakest Moir\u00e9; but color temperature, viewing angle effects detectable Advanced frequency analysis + color temperature analysis + environmental reflection detection 4K/OLED screen Minimal Moir\u00e9; wide color gamut; deep blacks; hardest screen to detect rPPG analysis + active challenges + color illumination response + device attestation <p>4K OLED Screens Are a Growing Threat</p> <p>As screen technology improves, texture and frequency-based screen detection becomes less reliable. Multi-signal approaches (rPPG, active challenges, device attestation) become essential when facing high-end screen replay attacks.</p>"},{"location":"03-attack-landscape/physical-attacks/#3d-attacks","title":"3D Attacks","text":""},{"location":"03-attack-landscape/physical-attacks/#mask-attack-hierarchy","title":"Mask Attack Hierarchy","text":"<pre><code>graph LR\n    A[\"Paper Mask&lt;br&gt;(L1 Sophistication)&lt;br&gt;$5\"] --&gt; B[\"Latex Mask&lt;br&gt;(L2)&lt;br&gt;$50-200\"]\n    B --&gt; C[\"3D-Printed&lt;br&gt;Rigid Mask&lt;br&gt;(L3)&lt;br&gt;$500-2000\"]\n    C --&gt; D[\"Silicone Mask&lt;br&gt;(L4)&lt;br&gt;$3000-15000\"]\n    D --&gt; E[\"Animatronic&lt;br&gt;Silicone Mask&lt;br&gt;(L5)&lt;br&gt;$10000+\"]\n\n    style A fill:#27ae60,color:#fff\n    style B fill:#f1c40f,color:#000\n    style C fill:#e67e22,color:#fff\n    style D fill:#e74c3c,color:#fff\n    style E fill:#8e44ad,color:#fff</code></pre>"},{"location":"03-attack-landscape/physical-attacks/#detection-signals-by-mask-type","title":"Detection Signals by Mask Type","text":"Signal Paper Mask Latex Mask Rigid 3D Silicone Mask Animatronic Skin texture Paper texture Latex sheen Plastic/resin texture Close to real but too uniform Very close to real Depth map Mostly flat Conforms to attacker's face Fixed 3D shape Very realistic Very realistic Skin deformation None Some (stretchy material) None (rigid) Limited (silicone is less elastic) Can simulate Boundary detection Obvious edges Visible at hairline, neck Visible at edges Can be concealed with makeup Can be concealed rPPG signal None None (blocks blood flow) None None (blocks blood flow) None Material classification Easy \u2014 paper Moderate \u2014 latex vs skin Moderate \u2014 plastic vs skin Hard \u2014 silicone mimics skin Very hard Eye region Cutouts or printed Printed or cutouts Printed or separate eyes Can have realistic eye holes Can have moving eyes NIR response Paper (high reflectance) Latex (different from skin) Resin/plastic Different from skin (detectable with NIR) Different from skin Thermal Ambient temperature Attacker's warmth partially transmitted Cold Attacker's warmth partially transmitted Warm"},{"location":"03-attack-landscape/physical-attacks/#mannequin-dummy-attacks","title":"Mannequin &amp; Dummy Attacks","text":"Type Detection Difficulty Basic mannequin No skin texture, unrealistic eyes, uniform surface \ud83d\udfe2 Low Mannequin with photo applied Photo texture on 3D form \u2014 combination of print and 3D signals \ud83d\udfe1 Medium Mannequin with realistic makeup Better texture but still lacks pores, subsurface scattering \ud83d\udfe1 Medium-High Wax figure Realistic geometry but waxy texture, no rPPG, no motion \ud83d\udd34 High"},{"location":"03-attack-landscape/physical-attacks/#partial-hybrid-physical-attacks","title":"Partial &amp; Hybrid Physical Attacks","text":"<p>These are combination attacks that use real human elements alongside fake ones.</p>"},{"location":"03-attack-landscape/physical-attacks/#partial-face-overlay","title":"Partial Face Overlay","text":"<p>Attacker covers part of their face with a screen or print showing the target's features while keeping part of their own face visible.</p> <pre><code>graph LR\n    A[\"Attacker's&lt;br&gt;lower face&lt;br&gt;(real, live)\"] --&gt; C[\"Combined&lt;br&gt;presentation\"]\n    B[\"Target's upper&lt;br&gt;face on screen&lt;br&gt;(spoofed)\"] --&gt; C\n    C --&gt; D{\"Liveness&lt;br&gt;System\"}\n    D --&gt; E[\"Must detect&lt;br&gt;boundary between&lt;br&gt;real and fake regions\"]</code></pre> <p>Detection approach:</p> <ul> <li>Analyze texture consistency across face regions</li> <li>Look for boundary discontinuities (lighting mismatch, resolution change, color temperature shift)</li> <li>Verify geometric consistency (do the face halves form a valid 3D face?)</li> <li>Check for Moir\u00e9 patterns in only part of the face</li> </ul>"},{"location":"03-attack-landscape/physical-attacks/#prosthetic-augmentation","title":"Prosthetic Augmentation","text":"<p>SFX-grade prosthetic pieces (nose, chin, cheekbones, brow ridge) applied over the attacker's real face to alter bone structure appearance.</p> Feature Modified Difficulty to Detect Why Nose shape \ud83d\udd34 High Realistic silicone prosthetics match skin texture Chin/jaw \ud83d\udd34 High Blends with natural jaw movement Cheekbones \ud83d\udd34\ud83d\udd34 Very High Subtle change, hard to distinguish Brow ridge \ud83d\udd34 High Doesn't affect eye region liveness signals <p>This Is Identity Fraud, Not Liveness Spoofing</p> <p>Prosthetic attacks pass liveness because the person IS live \u2014 they're just altering their appearance to match someone else's identity. This is primarily a face matching problem, not a liveness problem. Defense lies in high-precision face recognition and document-face cross-verification.</p>"},{"location":"03-attack-landscape/physical-attacks/#defense-summary","title":"Defense Summary","text":"Attack Level Primary Defense Secondary Defense Tertiary Defense L1: Basic prints/screens Texture CNN Frequency analysis Depth estimation L2: Quality prints/video/paper masks Multi-signal passive + active challenge Moir\u00e9 + specularity analysis Device attestation L3: 3D rigid masks/latex Material classification + rPPG Active multi-challenge NIR imaging (if available) L4: Silicone masks rPPG (strongest signal) + thermal (if available) Multi-frame temporal analysis Environmental consistency L5: Animatronic/high-end Multi-modal ensemble + manual review Behavioral biometrics Physical presence verification <p>Next: Digital &amp; Injection Attacks \u2192</p>"},{"location":"03-attack-landscape/social-engineering/","title":"3.6 Social Engineering &amp; Process Attacks","text":""},{"location":"03-attack-landscape/social-engineering/#overview","title":"Overview","text":"<p>These attacks exploit human factors and process weaknesses rather than technology. They are particularly dangerous because the person presenting to the camera IS a live human \u2014 so liveness detection alone cannot stop them.</p>"},{"location":"03-attack-landscape/social-engineering/#attack-types","title":"Attack Types","text":""},{"location":"03-attack-landscape/social-engineering/#coercion-duress","title":"Coercion &amp; Duress","text":"Attack Description Detection Mitigation Physical coercion Victim forced to complete liveness under threat Stress detection (micro-expressions, pupil dilation) \u2014 unreliable Duress codes, behavioral analytics, post-verification checks Social engineering of victim Victim tricked into completing liveness (\"verify your account\") N/A \u2014 victim cooperates willingly Customer education, transaction confirmation via separate channel Insider collusion Bank employee manipulates verification or overrides results N/A at technology level Dual-approval workflows, audit trails, insider threat monitoring"},{"location":"03-attack-landscape/social-engineering/#identity-exploitation","title":"Identity Exploitation","text":"Attack Description Why Liveness Can't Help Mitigation Identical twins Twin passes liveness and face matching for sibling Twin IS live and looks like the target Secondary verification (OTP to registered phone), behavioral biometrics, knowledge-based verification Lookalike Person with similar appearance attempts verification Person IS live High-precision face matching (tighter thresholds), document verification Willing account mule Legitimate person knowingly opens account for criminal use Person is genuinely who they claim to be Transaction monitoring, behavioral analytics, network analysis"},{"location":"03-attack-landscape/social-engineering/#process-manipulation","title":"Process Manipulation","text":"Attack Description Mitigation Fallback exploitation Deliberately fail digital liveness to trigger weaker manual process Ensure fallback processes are equally secure; don't make manual review \"easier\" Session timing exploit Pass liveness, then manipulate data before face matching runs Atomic session processing; server-side orchestration; no client-controlled timing Review queue manipulation Flood manual review queue with fraudulent applications to overwhelm reviewers Prioritized queuing, automated pre-screening, reviewer capacity planning"},{"location":"03-attack-landscape/social-engineering/#key-insight","title":"Key Insight","text":"<p>Technology Alone Is Insufficient</p> <p>Social engineering attacks require process-level defenses: dual approvals, audit trails, behavioral monitoring, customer education, and insider threat programs. No liveness system, no matter how advanced, can detect a willing participant or an insider.</p> <p>Next: Adversarial ML Attacks \u2192</p>"},{"location":"03-attack-landscape/taxonomy-overview/","title":"3.1 Attack Taxonomy Overview","text":""},{"location":"03-attack-landscape/taxonomy-overview/#the-complete-attack-landscape","title":"The Complete Attack Landscape","text":"<p>Understanding every possible attack vector is the foundation of building a secure liveness system. This section provides the most comprehensive classification of face liveness attacks, organized by category, sophistication level, and delivery method.</p>"},{"location":"03-attack-landscape/taxonomy-overview/#master-taxonomy","title":"Master Taxonomy","text":"<pre><code>graph TD\n    ROOT[\"Face Liveness&lt;br&gt;Attack Vectors\"] --&gt; A[\"\ud83d\uddbc\ufe0f Physical&lt;br&gt;Presentation Attacks\"]\n    ROOT --&gt; B[\"\ud83d\udcbb Digital &amp;&lt;br&gt;Injection Attacks\"]\n    ROOT --&gt; C[\"\ud83e\udd16 AI &amp; Generative&lt;br&gt;Attacks\"]\n    ROOT --&gt; D[\"\ud83e\udde0 Social Engineering&lt;br&gt;&amp; Process Attacks\"]\n    ROOT --&gt; E[\"\u2694\ufe0f Adversarial&lt;br&gt;ML Attacks\"]\n\n    A --&gt; A1[\"2D Attacks\"]\n    A --&gt; A2[\"3D Attacks\"]\n    A --&gt; A3[\"Partial/Hybrid&lt;br&gt;Physical\"]\n\n    B --&gt; B1[\"Virtual Camera&lt;br&gt;Injection\"]\n    B --&gt; B2[\"API/Network&lt;br&gt;Attacks\"]\n    B --&gt; B3[\"SDK/App&lt;br&gt;Tampering\"]\n\n    C --&gt; C1[\"Deepfakes\"]\n    C --&gt; C2[\"Synthetic&lt;br&gt;Identity\"]\n    C --&gt; C3[\"Face Morphing\"]\n    C --&gt; C4[\"AI Enhancement\"]\n\n    D --&gt; D1[\"Coercion &amp;&lt;br&gt;Collusion\"]\n    D --&gt; D2[\"Process&lt;br&gt;Manipulation\"]\n    D --&gt; D3[\"Identity&lt;br&gt;Exploitation\"]\n\n    E --&gt; E1[\"White-box&lt;br&gt;Attacks\"]\n    E --&gt; E2[\"Black-box&lt;br&gt;Attacks\"]\n    E --&gt; E3[\"Model&lt;br&gt;Extraction\"]</code></pre>"},{"location":"03-attack-landscape/taxonomy-overview/#sophistication-levels","title":"Sophistication Levels","text":"<p>All attacks are classified into 5 levels of sophistication. Each level assumes access to the tools and expertise of the previous levels.</p> Level Name Attacker Profile Tools Required Cost Prevalence L1 Script Kiddie No technical expertise; opportunistic Printer, smartphone $0-50 Very High (70% of attempts) L2 Informed Amateur Basic technical knowledge; follows online tutorials HD screen, basic software, paper masks $50-500 High (20% of attempts) L3 Skilled Attacker Strong technical skills; experience with ML/CV tools GPU, deepfake software, 3D printer $500-5,000 Moderate (8% of attempts) L4 Professional / Organized Crime Dedicated team with specialized skills and resources Custom hardware, ML expertise, inside knowledge $5,000-50,000 Low (1.5% of attempts) L5 State Actor / APT Unlimited resources, access to cutting-edge research Custom silicon masks, neural rendering, zero-day exploits $50,000+ Very Low (0.5% of attempts) <p>Don't Ignore Low-Sophistication Attacks</p> <p>While L4-L5 attacks get media attention, L1-L2 attacks account for 90% of real-world attempts. A system that defends against deepfakes but fails against printed photos is fundamentally broken.</p>"},{"location":"03-attack-landscape/taxonomy-overview/#attack-category-summary","title":"Attack Category Summary","text":""},{"location":"03-attack-landscape/taxonomy-overview/#a-physical-presentation-attacks","title":"A. Physical Presentation Attacks","text":"<p>Attacks involving a physical artifact presented to the camera.</p>"},{"location":"03-attack-landscape/taxonomy-overview/#a1-2d-flat-attacks","title":"A1. 2D Flat Attacks","text":"# Attack Description Sophistication Materials Detection Difficulty A1.1 Printed Photo (Standard) A4/Letter printed photo held in front of camera L1 Home printer, paper ($2) \ud83d\udfe2 Low A1.2 Printed Photo (Professional) High-quality photo lab print on glossy/matte stock L1 Photo lab print ($5-15) \ud83d\udfe2 Low-Medium A1.3 Large Format Print Poster-size print for more realistic size and detail L2 Large format printer ($20-50) \ud83d\udfe1 Medium A1.4 Screen Replay (Phone) Photo displayed on smartphone screen L1 Smartphone ($0) \ud83d\udfe2 Low-Medium A1.5 Screen Replay (Tablet) Photo/video on tablet (higher resolution, larger) L1 Tablet ($200-500) \ud83d\udfe1 Medium A1.6 Screen Replay (Monitor) Photo/video on HD/4K monitor L2 HD monitor ($200-800) \ud83d\udfe1 Medium A1.7 Video Replay (Pre-recorded) Pre-recorded video showing natural motion, blinking L2 Camera + screen ($200-500) \ud83d\udfe1 Medium-High A1.8 Video Replay (Looping) Short loop designed to repeat blinks and micro-movements L2 Video editing software ($0) \ud83d\udfe1 Medium A1.9 Warped/Bent Photo Photo curved around a cylinder to simulate 3D curvature L2 High-quality print + backing ($20) \ud83d\udfe1 Medium A1.10 Photo on Transparent OLED Face displayed on a transparent screen overlaid on real scene L3 Transparent display ($500+) \ud83d\udd34 High"},{"location":"03-attack-landscape/taxonomy-overview/#a2-3d-attacks","title":"A2. 3D Attacks","text":"# Attack Description Sophistication Materials Detection Difficulty A2.1 Paper Mask (Basic) Printed face cut out and worn as a flat mask L1 Printer, scissors ($5) \ud83d\udfe2 Low A2.2 Paper Mask (Eye/Mouth Cutouts) Print with holes for eyes and mouth \u2014 attacker blinks/speaks through L1 Printer, scissors ($5) \ud83d\udfe1 Medium A2.3 Mannequin/Dummy Head Department store mannequin with photo or makeup applied L2 Mannequin head ($50-200) \ud83d\udfe1 Medium A2.4 Wax Figure Head Custom wax sculpture of target's face L4 Wax sculpting ($2000+) \ud83d\udd34 High A2.5 3D-Printed Rigid Mask Hard plastic mask 3D-printed from a 3D face scan L3 3D scanner + printer ($500-2000) \ud83d\udd34 High A2.6 Resin/Plaster Cast Mask Rigid mask cast from a mold L3 Casting materials ($200-500) \ud83d\udd34 High A2.7 Latex Mask (Commercial) Off-the-shelf latex mask (realistic Halloween-type) L2 Commercial mask ($50-200) \ud83d\udfe1 Medium-High A2.8 Silicone Mask (Custom) Custom-made full-face silicone mask with realistic skin texture, hand-painted L4 Custom fabrication ($3000-15000) \ud83d\udd34\ud83d\udd34 Very High A2.9 Silicone Mask (Animatronic) Custom silicone mask with embedded servos for eye/mouth movement L5 Advanced fabrication ($10000+) \ud83d\udd34\ud83d\udd34\ud83d\udd34 Extreme A2.10 Projection on 3D Form Face projected onto a white 3D head form or mannequin L3 Projector + head form ($300-1000) \ud83d\udd34 High"},{"location":"03-attack-landscape/taxonomy-overview/#a3-partial-hybrid-physical-attacks","title":"A3. Partial &amp; Hybrid Physical Attacks","text":"# Attack Description Sophistication Detection Difficulty A3.1 Fake Eyes on Photo Glass/doll eyes placed on a printed photo to simulate eye reflection and blink L2 \ud83d\udfe1 Medium A3.2 Partial Face Overlay Screen/print covering upper face while lower face is real (or vice versa) L2 \ud83d\udd34 High A3.3 Contact Lens Attacks Patterned/colored lenses to alter iris appearance for iris-involved checks L2 \ud83d\udfe1 Medium A3.4 Makeup/Prosthetic Transformation SFX makeup to transform the attacker's face to resemble the target L3 \ud83d\udd34 High A3.5 Prosthetic Augmentation Prosthetic nose, chin, cheekbones to alter bone structure appearance L4 \ud83d\udd34\ud83d\udd34 Very High"},{"location":"03-attack-landscape/taxonomy-overview/#b-digital-injection-attacks","title":"B. Digital &amp; Injection Attacks","text":"<p>Attacks that bypass the physical camera entirely by injecting digital content into the capture pipeline.</p> # Attack Description Sophistication Detection Difficulty B1.1 Virtual Camera (OBS/ManyCam) Virtual camera driver feeds pre-recorded or generated video as camera input L2 \ud83d\udfe1 Medium B1.2 Camera API Hooking (Android) Frida/Xposed framework intercepts camera API calls and injects modified frames L3 \ud83d\udd34 High B1.3 Camera API Hooking (iOS) Substrate/Frida intercepts iOS camera pipeline (requires jailbreak) L3 \ud83d\udd34 High B1.4 Emulator-Based Attack Running mobile app in Android emulator with virtual camera feed L2 \ud83d\udfe1 Medium B1.5 Frame Buffer Manipulation OS-level interception of frame buffer to inject modified frames L4 \ud83d\udd34\ud83d\udd34 Very High B2.1 API Replay Attack Capturing and replaying a valid liveness API request to the server L2 \ud83d\udfe1 Medium B2.2 Man-in-the-Middle (MitM) Intercepting video stream between client and server, modifying frames in transit L3 \ud83d\udd34 High B2.3 API Parameter Tampering Modifying liveness scores or decision parameters in API requests L2 \ud83d\udfe1 Medium B3.1 Repackaged APK/IPA Decompiled app with liveness checks disabled, recompiled and signed L3 \ud83d\udd34 High B3.2 Runtime Hooking Hooking liveness SDK functions at runtime to return fake \"pass\" results L3 \ud83d\udd34 High B3.3 Browser WebRTC Manipulation Modifying WebRTC media stream in browser for web-based liveness L3 \ud83d\udd34 High B3.4 Screen Sharing Injection Using screen sharing or remote desktop to inject content into camera feed L2 \ud83d\udfe1 Medium B3.5 Relay/Proxy Attack Real person's camera feed relayed to another device at a different location L2 \ud83d\udd34 High"},{"location":"03-attack-landscape/taxonomy-overview/#c-ai-generative-attacks","title":"C. AI &amp; Generative Attacks","text":"<p>Attacks using artificial intelligence to generate or manipulate facial content.</p> # Attack Description Sophistication Detection Difficulty C1.1 Face Swap (Offline) Face replacement in pre-recorded video using DeepFaceLab, FaceSwap, Roop L3 \ud83d\udd34 High C1.2 Face Swap (Real-time) Live face swap during camera capture using InsightFace, DeepFaceLive L3 \ud83d\udd34\ud83d\udd34 Very High C1.3 Face Reenactment Expression transfer from attacker to target face (First Order Motion, LivePortrait) L3 \ud83d\udd34\ud83d\udd34 Very High C1.4 Lip Sync Deepfake Mouth animation synchronized to arbitrary audio (Wav2Lip, SadTalker, VideoReTalking) L3 \ud83d\udd34\ud83d\udd34 Very High C1.5 Full Face Animation Single photo animated to full video (MegaPortraits, Thin-Plate Spline Motion) L3 \ud83d\udd34 High C1.6 Audio-Visual Deepfake Synchronized face + voice generation for speech-based liveness bypass L4 \ud83d\udd34\ud83d\udd34 Very High C2.1 GAN-Generated Face Completely synthetic face (StyleGAN2/3) \u2014 non-existent person L3 \ud83d\udfe1 Medium C2.2 Diffusion-Generated Face Face generated via Stable Diffusion / SDXL fine-tuned on faces L3 \ud83d\udfe1 Medium-High C2.3 Synthetic Identity Package GAN face + fabricated document + manufactured personal history L4 \ud83d\udd34\ud83d\udd34 Very High C3.1 Face Morphing Blending two faces so the result matches both identities (for document fraud) L3 \ud83d\udd34 High C3.2 Age Manipulation De-aging or aging a face to match an older/newer document photo L3 \ud83d\udfe1 Medium C4.1 Super-Resolution Enhancement Upscaling low-quality spoof images to appear more genuine L3 \ud83d\udfe1 Medium C4.2 Style Transfer Transferring \"live skin\" texture characteristics onto spoof images L4 \ud83d\udd34 High C4.3 Neural Radiance Field (NeRF) Full 3D neural rendering of a face, viewable from any angle L5 \ud83d\udd34\ud83d\udd34\ud83d\udd34 Extreme C4.4 3D Gaussian Splatting Real-time 3D face rendering from multi-view captures L5 \ud83d\udd34\ud83d\udd34\ud83d\udd34 Extreme"},{"location":"03-attack-landscape/taxonomy-overview/#d-social-engineering-process-attacks","title":"D. Social Engineering &amp; Process Attacks","text":"<p>Attacks that exploit human factors and process weaknesses rather than technology.</p> # Attack Description Sophistication Detection Difficulty D1.1 Coercion Forcing a legitimate person to complete liveness verification under duress L2 \ud83d\udd34\ud83d\udd34 Very High (tech can't detect) D1.2 Insider Collusion Bank employee manipulates the review process or overrides liveness results L3 \ud83d\udd34\ud83d\udd34 Very High D1.3 Bribery/Social Engineering of Reviewer Convincing a manual reviewer to approve a failed liveness check L3 \ud83d\udd34 High D2.1 Session Hijacking Taking over a legitimate user's session after they pass liveness L3 \ud83d\udd34 High D2.2 Process Timing Exploit Exploiting the gap between liveness pass and face matching to substitute data L3 \ud83d\udd34 High D2.3 Fallback Exploitation Deliberately failing liveness to trigger a weaker fallback verification method L2 \ud83d\udfe1 Medium D3.1 Identical Twin Twin sibling passing liveness and face matching on behalf of the other L1 \ud83d\udd34\ud83d\udd34 Very High D3.2 Lookalike/Doppelganger Person with similar appearance attempting to pass as the target L1 \ud83d\udd34 High D3.3 Account Mule (Willing Participant) Legitimate person knowingly opens account for criminal use L1 \ud83d\udd34\ud83d\udd34 Very High (passes all checks)"},{"location":"03-attack-landscape/taxonomy-overview/#e-adversarial-machine-learning-attacks","title":"E. Adversarial Machine Learning Attacks","text":"<p>Attacks specifically targeting the ML models powering the liveness system.</p> # Attack Description Sophistication Detection Difficulty E1.1 White-box Adversarial Perturbation Attacker has full model access; crafts optimal pixel perturbations to flip prediction L5 \ud83d\udd34\ud83d\udd34\ud83d\udd34 Extreme E1.2 Adversarial Patch Visible patch placed on/near the face that causes model misclassification L4 \ud83d\udd34\ud83d\udd34 Very High E1.3 Adversarial Glasses/Accessories Specially designed glasses, makeup, or accessories with adversarial patterns L4 \ud83d\udd34\ud83d\udd34 Very High E2.1 Black-box Query Attack Iteratively queries the API with modified inputs to find adversarial examples L3 \ud83d\udd34 High E2.2 Transfer Attack Adversarial examples crafted on a surrogate model transfer to target model L3 \ud83d\udd34 High E2.3 Score-based Attack Uses returned liveness scores to optimize attack images via gradient estimation L3 \ud83d\udd34 High E3.1 Model Extraction Reverse-engineers the liveness model through API queries to create a local copy L4 \ud83d\udd34\ud83d\udd34 Very High E3.2 Model Extraction + Adversarial Extracts model, then crafts targeted adversarial attacks against the extracted model L4 \ud83d\udd34\ud83d\udd34\ud83d\udd34 Extreme"},{"location":"03-attack-landscape/taxonomy-overview/#attack-to-defense-mapping","title":"Attack-to-Defense Mapping","text":"Attack Category Primary Defenses Secondary Defenses 2D Flat Attacks (A1) Texture analysis, frequency analysis, depth estimation Moir\u00e9 detection, reflection analysis 3D Masks (A2) Skin texture analysis, material classification, rPPG Active challenges, NIR imaging (if available) Injection Attacks (B1-B3) Device attestation, virtual camera detection, SDK integrity Certificate pinning, session binding, root/jailbreak detection Deepfakes (C1) Temporal consistency analysis, forensic frequency analysis, rPPG Multi-frame analysis, physiological signal detection Synthetic Identity (C2-C3) GAN fingerprint detection, face quality analysis Cross-database duplicate checks, document-face consistency Social Engineering (D) Multi-factor verification, behavioral analytics Process controls, audit trails, insider monitoring Adversarial ML (E) Adversarial training, input preprocessing, ensemble models Rate limiting, score distribution monitoring, model diversity"},{"location":"03-attack-landscape/taxonomy-overview/#attack-prevalence-by-banking-context","title":"Attack Prevalence by Banking Context","text":"Context Most Common Attacks Most Dangerous Attacks Digital Onboarding Screen replay (A1.4-A1.6), printed photo (A1.1-A1.2) Synthetic identity (C2.3), deepfake (C1.2) Transaction Auth Session hijacking (D2.1), API replay (B2.1) Real-time deepfake (C1.2), coercion (D1.1) Video KYC Video replay (A1.7), face reenactment (C1.3) Audio-visual deepfake (C1.6), relay (B3.5) Loan Disbursement Synthetic identity (C2.3), lookalike (D3.2) Organized fraud ring with multiple vectors Account Recovery Screen replay (A1.4), virtual camera (B1.1) Deepfake of account holder (C1.2) <p>The Arms Race Reality</p> <p>The attack landscape evolves continuously. Any static defense will eventually be bypassed. Continuous model updates, red team exercises, and threat intelligence sharing are mandatory \u2014 not optional \u2014 for banking deployments.</p> <p>Detailed exploration of each category continues in the following pages:</p> <ul> <li>Physical Presentation Attacks \u2192</li> <li>Digital &amp; Injection Attacks \u2192</li> <li>AI &amp; Generative Attacks \u2192</li> <li>Deepfakes \u2014 The Evolving Threat \u2192</li> <li>Social Engineering &amp; Process Attacks \u2192</li> <li>Adversarial ML Attacks \u2192</li> </ul>"},{"location":"04-technical-architecture/client-side/","title":"Client Side","text":"<p>See the complete technical architecture content in the MkDocs site. This section covers detailed component specifications, model architectures, training pipelines, and infrastructure requirements for production banking deployments.</p> <p>This page is part of the Face Liveness Verification technical architecture guide.</p>"},{"location":"04-technical-architecture/integration-patterns/","title":"4.6 Integration Patterns","text":""},{"location":"04-technical-architecture/integration-patterns/#pattern-comparison","title":"Pattern Comparison","text":"Aspect SDK-Based API-Based Hybrid SDK+API Client complexity High Low Medium Security High (local checks + server) Medium (server only) Highest Latency Low (local screening) Higher (all server-side) Optimal Bandwidth Low (only selected frames sent) High (all frames sent) Medium Update mechanism App store release required Server-side only Model OTA + server Best for Native mobile apps Web browser Banking apps (recommended) <p>Back to: System Architecture Overview \u2192</p>"},{"location":"04-technical-architecture/model-training/","title":"Model Training","text":"<p>See the complete technical architecture content in the MkDocs site. This section covers detailed component specifications, model architectures, training pipelines, and infrastructure requirements for production banking deployments.</p> <p>This page is part of the Face Liveness Verification technical architecture guide.</p>"},{"location":"04-technical-architecture/score-fusion/","title":"Score Fusion","text":"<p>See the complete technical architecture content in the MkDocs site. This section covers detailed component specifications, model architectures, training pipelines, and infrastructure requirements for production banking deployments.</p> <p>This page is part of the Face Liveness Verification technical architecture guide.</p>"},{"location":"04-technical-architecture/server-side/","title":"Server Side","text":"<p>See the complete technical architecture content in the MkDocs site. This section covers detailed component specifications, model architectures, training pipelines, and infrastructure requirements for production banking deployments.</p> <p>This page is part of the Face Liveness Verification technical architecture guide.</p>"},{"location":"04-technical-architecture/system-overview/","title":"System Overview","text":"<p>See the complete technical architecture content in the MkDocs site. This section covers detailed component specifications, model architectures, training pipelines, and infrastructure requirements for production banking deployments.</p> <p>This page is part of the Face Liveness Verification technical architecture guide.</p>"},{"location":"05-standards-compliance/bias-fairness/","title":"Bias Fairness","text":"<p>Detailed content for this section covers bias fairness requirements and best practices for banking deployments.</p> <p>This page is part of the comprehensive Face Liveness Verification guide.</p>"},{"location":"05-standards-compliance/fido/","title":"Fido","text":"<p>Detailed content for this section covers fido requirements and best practices for banking deployments.</p> <p>This page is part of the comprehensive Face Liveness Verification guide.</p>"},{"location":"05-standards-compliance/ibeta/","title":"5.2 iBeta Certification (Level 1 &amp; Level 2)","text":""},{"location":"05-standards-compliance/ibeta/#overview","title":"Overview","text":"<p>iBeta Quality Assurance (Aurora, Colorado, USA) is the most widely recognized independent testing laboratory for biometric Presentation Attack Detection (PAD). Their certification follows ISO/IEC 30107-3:2023 and has become the de facto industry standard for validating face liveness in banking.</p> <p>Why iBeta Matters</p> <p>When a bank evaluates liveness vendors, iBeta certification is often the first filter. Vendors without it are typically eliminated from regulated financial services procurement.</p>"},{"location":"05-standards-compliance/ibeta/#level-1-2d-attack-testing","title":"Level 1: 2D Attack Testing","text":"<p>Tests against printed photos and screen displays \u2014 the most common real-world attack vectors.</p>"},{"location":"05-standards-compliance/ibeta/#pai-species-tested","title":"PAI Species Tested","text":"ID Type Description Specifications P.1 Print Color laser print, A4/Letter 600 DPI, standard office paper P.2 Print Color inkjet, A4/Letter High-quality inkjet, photo paper P.3 Print Photo lab print, 4\u00d76, glossy Professional lab print P.4 Print Photo lab print, 4\u00d76, matte Professional lab print P.5 Print Photo lab print, 8\u00d710 Larger format P.6 Print Life-size poster print Full face-size P.7 Print Curved/bent photo on form Simulating 3D curvature S.1 Screen Smartphone (5-7\"), photo Various models, max brightness S.2 Screen Smartphone (5-7\"), video Pre-recorded video with movement S.3 Screen Tablet (9-12\"), photo iPad-class devices S.4 Screen Tablet (9-12\"), video HD video with natural motion S.5 Screen Laptop/monitor (13\"+), photo HD/FHD monitors S.6 Screen Laptop/monitor (13\"+), video HD video playback"},{"location":"05-standards-compliance/ibeta/#testing-protocol","title":"Testing Protocol","text":"Parameter Specification Test subjects Minimum 50 unique individuals Demographics Diversity in age, gender, skin tone Attacks per species Minimum 150 per PAI species Total attacks 1,500-2,500+ across all species Bona fide presentations Minimum 300 genuine presentations Environment Controlled indoor lighting + variable conditions Devices Multiple capture devices representative of deployment"},{"location":"05-standards-compliance/ibeta/#pass-criteria","title":"Pass Criteria","text":"<p>Critical: Zero Tolerance</p> <ul> <li>APCER = 0% across ALL Level 1 PAI species (zero successful attacks in entire test set)</li> <li>BPCER \u2264 5% (no more than 5% of genuine users incorrectly rejected)</li> <li>A single successful attack on any species = FAIL</li> </ul>"},{"location":"05-standards-compliance/ibeta/#level-2-2d-3d-attack-testing","title":"Level 2: 2D + 3D Attack Testing","text":"<p>Extends Level 1 with three-dimensional attack instruments \u2014 masks, mannequins, and sculptural reproductions.</p>"},{"location":"05-standards-compliance/ibeta/#additional-pai-species-beyond-level-1","title":"Additional PAI Species (Beyond Level 1)","text":"ID Type Description Specifications M.1 Mask Paper mask with eye cutouts Printed, worn by attacker M.2 Mask Paper mask without cutouts Full face coverage M.3 Mask Latex mask (commercial) Off-the-shelf realistic mask M.4 Mask Resin/plaster cast mask Custom-cast rigid mask M.5 Mask Silicone mask (custom) Custom-fabricated, hand-painted, realistic skin texture M.6 Mannequin Mannequin head with photo Photo attached to mannequin M.7 Mannequin Mannequin with makeup Makeup applied to match target M.8 3D Print 3D-printed face from scan FDM or SLA 3D print, painted"},{"location":"05-standards-compliance/ibeta/#level-2-pass-criteria","title":"Level 2 Pass Criteria","text":"<ul> <li>Same 0% APCER requirement across ALL species (Level 1 + Level 2)</li> <li>BPCER \u2264 5% maintained</li> <li>Significantly harder to achieve \u2014 many vendors pass Level 1 but fail Level 2</li> </ul>"},{"location":"05-standards-compliance/ibeta/#testing-process-timeline","title":"Testing Process &amp; Timeline","text":"<pre><code>graph TD\n    A[\"1. Application &amp;&lt;br&gt;Contract&lt;br&gt;(2-4 weeks)\"] --&gt; B[\"2. SDK/API&lt;br&gt;Submission&lt;br&gt;(1 week)\"]\n    B --&gt; C[\"3. Test Setup &amp;&lt;br&gt;Calibration&lt;br&gt;(1-2 weeks)\"]\n    C --&gt; D[\"4. Data Collection&lt;br&gt;(Attacks + Bona Fide)&lt;br&gt;(2-4 weeks)\"]\n    D --&gt; E[\"5. Testing &amp;&lt;br&gt;Analysis&lt;br&gt;(2-4 weeks)\"]\n    E --&gt; F{\"6. Results\"}\n    F --&gt;|\"Pass\"| G[\"Certificate Issued&lt;br&gt;+ Public Report\"]\n    F --&gt;|\"Fail\"| H[\"Detailed Failure&lt;br&gt;Report \u2192 Fix \u2192 Retest\"]</code></pre> Phase Duration Cost (Approximate) Application &amp; contracting 2-4 weeks \u2014 Level 1 testing 4-8 weeks $20,000 - $35,000 Level 2 testing 6-12 weeks $35,000 - $60,000 Combined L1 + L2 8-14 weeks $45,000 - $80,000 Retest (after failure) 4-6 weeks $10,000 - $25,000"},{"location":"05-standards-compliance/ibeta/#preparing-for-ibeta-testing","title":"Preparing for iBeta Testing","text":"<p>Pre-Testing Checklist</p> <ol> <li>Internal testing against all known PAI species before submission</li> <li>Threshold calibration: Ensure BPCER &lt; 3% internally (leave margin for iBeta's stricter conditions)</li> <li>Device coverage: Test on the exact devices iBeta will use (ask them for their device list)</li> <li>Environmental robustness: Test under varying lighting \u2014 iBeta tests aren't always in perfect conditions</li> <li>Demographic diversity: Ensure your model performs consistently across skin tones and ages</li> <li>Edge cases: Test with glasses, beards, hijabs, masks (medical), and other face variations</li> <li>Documentation: Prepare integration guide, API docs, and configuration instructions for iBeta engineers</li> <li>Regression testing: Run full regression after any model update before submission</li> </ol>"},{"location":"05-standards-compliance/ibeta/#common-failure-reasons","title":"Common Failure Reasons","text":"Reason Frequency Prevention High-quality screen replay accepted 35% of failures Strengthen Moir\u00e9 detection and frequency analysis Specific print type accepted 25% of failures Ensure training data covers glossy, matte, and large formats BPCER too high 20% of failures Calibrate threshold with demographic-diverse validation set 3D mask accepted (Level 2) 15% of failures Add material classification and NIR-simulated features Inconsistent results across devices 5% of failures Test across device diversity before submission"},{"location":"05-standards-compliance/ibeta/#post-certification-maintenance","title":"Post-Certification Maintenance","text":"<p>Certification Is Not Permanent</p> <ul> <li>Certificate applies to the specific algorithm version tested</li> <li>Major model updates require re-certification</li> <li>Minor updates (threshold adjustments, preprocessing changes) may not require retesting \u2014 consult iBeta</li> <li>Annual review recommended even without changes</li> <li>Some banks require re-certification every 2 years as a contractual requirement</li> </ul>"},{"location":"05-standards-compliance/ibeta/#ibeta-vs-other-testing-labs","title":"iBeta vs Other Testing Labs","text":"Lab Location Standard Strengths Limitations iBeta USA ISO 30107-3 Most widely recognized, comprehensive PAI species coverage US-centric, long wait times BioLab Italy ISO 30107-3 European recognition, strong 3D mask testing Less recognition in US/APAC markets CLR Labs France ISO 30107-3, PVID French regulatory alignment, video identification testing Primarily European market NIST (FRVT PAD) USA Custom protocol Free, government authority, ongoing evaluation Not a pass/fail certification FIDO Alliance Global FIDO Biometric Cert Authentication-focused, device-level testing Narrower scope than iBeta <p>Next: NIST FRVT PAD &amp; FATE \u2192</p>"},{"location":"05-standards-compliance/iso-30107/","title":"5.1 ISO/IEC 30107 Series","text":""},{"location":"05-standards-compliance/iso-30107/#overview","title":"Overview","text":"<p>ISO/IEC 30107 is the international standard series for biometric Presentation Attack Detection. It provides the terminology, testing methodology, and reporting framework that underpins all PAD certifications.</p>"},{"location":"05-standards-compliance/iso-30107/#the-four-parts","title":"The Four Parts","text":"Part Title Purpose Key Content 30107-1 Framework Definitions and conceptual framework PAI taxonomy, attack categories, PAD mechanism types 30107-2 Data Formats Reporting format for PAD results Standardized result reporting, APCER/BPCER per species 30107-3 Testing &amp; Reporting Testing methodology How to test PAD systems, statistical requirements, test protocols 30107-4 Mobile Profile Mobile-specific requirements Addresses device diversity, environmental variability, mobile-specific attacks"},{"location":"05-standards-compliance/iso-30107/#key-metrics-defined","title":"Key Metrics Defined","text":"<p>APCER (Attack Presentation Classification Error Rate): $$APCER_{PAI} = \\frac{\\text{Number of attack presentations incorrectly classified as bona fide}}{\\text{Total attack presentations of that PAI species}}$$</p> <p>BPCER (Bona Fide Presentation Classification Error Rate): $$BPCER = \\frac{\\text{Number of bona fide presentations incorrectly classified as attacks}}{\\text{Total bona fide presentations}}$$</p> <p>Critical: APCER Is Per Species</p> <p>APCER must be reported separately for each PAI species. A system that blocks 100% of printed photos but misses 50% of screen replays would have APCER=0% for prints and APCER=50% for screens. Both must be reported.</p>"},{"location":"05-standards-compliance/iso-30107/#how-banks-should-use-iso-30107","title":"How Banks Should Use ISO 30107","text":"<ol> <li>Require ISO 30107-3 compliant testing in all vendor RFPs</li> <li>Request per-species APCER reports \u2014 not just aggregate numbers</li> <li>Verify testing was performed by an accredited lab (iBeta, BioLab, etc.)</li> <li>Map PAI species to your threat model \u2014 which attacks are most relevant to your deployment?</li> <li>Reference 30107 in regulatory filings as evidence of standards compliance</li> </ol> <p>Next: iBeta Certification \u2192</p>"},{"location":"05-standards-compliance/nist/","title":"5.3 NIST FRVT PAD &amp; FATE","text":""},{"location":"05-standards-compliance/nist/#overview","title":"Overview","text":"<p>The National Institute of Standards and Technology (NIST) operates the most authoritative government-backed evaluation programs for face recognition and presentation attack detection technology. Unlike iBeta (which provides pass/fail certification), NIST provides continuous, comparative evaluation \u2014 ranking all participants against each other.</p>"},{"location":"05-standards-compliance/nist/#nist-frvt-face-recognition-vendor-test","title":"NIST FRVT (Face Recognition Vendor Test)","text":""},{"location":"05-standards-compliance/nist/#what-it-is","title":"What It Is","text":"<p>FRVT is NIST's ongoing evaluation of face recognition technology. It has multiple tracks:</p> Track Focus PAD Relevance FRVT 1:1 (Verification) Face matching accuracy for identity verification Indirect \u2014 measures the face matching component of eKYC FRVT 1:N (Identification) Face search against large galleries Less relevant for eKYC liveness FRVT Quality Face image quality assessment Directly relevant \u2014 quality impacts liveness accuracy FRVT PAD Presentation attack detection Directly relevant \u2014 evaluates liveness detection FRVT Demographics Accuracy across demographics Critical for bias assessment in liveness systems"},{"location":"05-standards-compliance/nist/#frvt-pad-track","title":"FRVT PAD Track","text":"<p>Status</p> <p>NIST FRVT PAD was announced and has been collecting submissions. Check NIST FRVT page for the latest status and results.</p> <p>Key differences from iBeta:</p> Aspect NIST FRVT PAD iBeta Type Comparative evaluation (ranking) Pass/fail certification Cost Free to participate $20,000 - $80,000 Duration Ongoing (submit anytime) Project-based (8-14 weeks) Result Detailed performance report with rankings Certificate + summary report Dataset NIST-curated (proprietary, very large) iBeta-collected (per ISO 30107-3) PAI Species Comprehensive, including advanced attacks Defined by level (L1: 2D, L2: 2D+3D) Public Results Published online with vendor names Published with vendor consent Regulatory Weight Highest authority (US government) Industry standard Threshold Control NIST evaluates at multiple operating points Vendor sets their threshold"},{"location":"05-standards-compliance/nist/#how-to-submit-to-nist-frvt","title":"How to Submit to NIST FRVT","text":"<pre><code>graph TD\n    A[\"1. Review NIST FRVT&lt;br&gt;API specifications\"] --&gt; B[\"2. Implement NIST&lt;br&gt;standard API wrapper\"]\n    B --&gt; C[\"3. Package algorithm&lt;br&gt;as Linux library&lt;br&gt;(.so / Docker)\"]\n    C --&gt; D[\"4. Submit via NIST&lt;br&gt;online portal\"]\n    D --&gt; E[\"5. NIST runs evaluation&lt;br&gt;on their infrastructure\"]\n    E --&gt; F[\"6. Results published&lt;br&gt;in NIST report\"]</code></pre> <p>Submission requirements:</p> Requirement Details Format Linux shared library (.so) or Docker container API Must implement NIST-specified C/C++ API Dependencies All dependencies must be self-contained; no network access during evaluation Processing Must run on NIST's hardware (specified CPU/GPU configurations) Timing Must process within specified time limits per image/video Size Model + library size limits apply"},{"location":"05-standards-compliance/nist/#nist-fate-face-analysis-technology-evaluation","title":"NIST FATE (Face Analysis Technology Evaluation)","text":""},{"location":"05-standards-compliance/nist/#what-it-is_1","title":"What It Is","text":"<p>FATE is NIST's broader evaluation framework covering multiple face analysis capabilities beyond just recognition:</p> FATE Component Description Relevance to Liveness Morph Detection Detecting face morphing attacks in ID photos Directly relevant \u2014 morphing is an attack vector Age Estimation Estimating age from face images Indirect \u2014 age affects liveness system performance PAD Presentation attack detection Core relevance Quality Face image quality assessment Impacts liveness system performance Attribute Detection Detecting face attributes (glasses, makeup, etc.) Relevant for edge case handling"},{"location":"05-standards-compliance/nist/#fate-morph-detection","title":"FATE Morph Detection","text":"<p>Particularly relevant for banking because morphed photos are used in document fraud:</p> <pre><code>graph LR\n    A[\"Person A's&lt;br&gt;face photo\"] --&gt; C[\"Morphing&lt;br&gt;Algorithm\"]\n    B[\"Person B's&lt;br&gt;face photo\"] --&gt; C\n    C --&gt; D[\"Morphed face&lt;br&gt;(matches both&lt;br&gt;A and B)\"]\n    D --&gt; E[\"Used on&lt;br&gt;fraudulent ID\"]\n    E --&gt; F[\"Both A and B&lt;br&gt;can pass face&lt;br&gt;matching against&lt;br&gt;this document\"]</code></pre> <p>NIST FATE Morph Detection evaluates:</p> <ul> <li>Differential morph detection: Given a trusted live photo AND the document photo, detect if the document photo is morphed</li> <li>Single-image morph detection: Given only the document photo, detect if it's morphed (harder)</li> <li>Print-scan resilience: Detection after the morphed image has been printed, used in a document, and then scanned/photographed</li> </ul>"},{"location":"05-standards-compliance/nist/#nist-sp-800-63b-digital-identity-guidelines","title":"NIST SP 800-63B: Digital Identity Guidelines","text":"<p>This is NIST's prescriptive standard for digital identity verification, directly applicable to banking.</p>"},{"location":"05-standards-compliance/nist/#identity-assurance-levels-ial","title":"Identity Assurance Levels (IAL)","text":"Level Description Biometric/PAD Requirement Banking Applicability IAL1 Self-asserted identity No biometric required Not suitable for banking IAL2 Remote or in-person proofing with evidence verification Biometric required with PAD for remote proofing Standard banking onboarding IAL3 In-person or supervised remote proofing Biometric required with PAD + in-person or supervised High-value accounts, regulatory-sensitive"},{"location":"05-standards-compliance/nist/#ial2-pad-requirements-most-relevant-for-banking","title":"IAL2 PAD Requirements (Most Relevant for Banking)","text":"<p>Key Requirements</p> <p>NIST SP 800-63B Section 5.2.3 states for IAL2 remote identity proofing:</p> <ul> <li>Liveness detection is MANDATORY for remote biometric verification</li> <li>The system must implement PAD that meets the requirements of ISO/IEC 30107</li> <li>PAD must detect at minimum: printed photos, screen display attacks, and video replay attacks</li> <li>Testing must be performed by an accredited laboratory (iBeta satisfies this)</li> <li>Results must demonstrate resistance to presentation attacks across specified PAI species</li> </ul>"},{"location":"05-standards-compliance/nist/#ial3-additional-requirements","title":"IAL3 Additional Requirements","text":"<ul> <li>Supervised remote or in-person proofing required</li> <li>Biometric comparison with PAD is mandatory</li> <li>Operator must be trained in detecting presentation attacks</li> <li>Physical document inspection may be required</li> <li>Stronger cryptographic binding of biometric to identity</li> </ul>"},{"location":"05-standards-compliance/nist/#how-to-use-nist-in-banking-deployments","title":"How to Use NIST in Banking Deployments","text":""},{"location":"05-standards-compliance/nist/#for-vendor-evaluation","title":"For Vendor Evaluation","text":"Scenario Use NIST How Comparing vendors Request vendors' NIST FRVT scores for 1:1 verification AND PAD (if available) Bias assessment Review NIST FRVT Demographics results for the vendor's algorithm Regulatory documentation Reference NIST SP 800-63B IAL2/IAL3 compliance in your regulatory filings Morph detection needs Check NIST FATE Morph Detection results if document fraud is a concern"},{"location":"05-standards-compliance/nist/#for-regulatory-compliance","title":"For Regulatory Compliance","text":"<pre><code>graph TD\n    A[\"Regulatory Requirement:&lt;br&gt;'Implement biometric&lt;br&gt;verification with anti-spoofing'\"] --&gt; B{\"Which standard&lt;br&gt;to reference?\"}\n    B --&gt; C[\"ISO/IEC 30107-3&lt;br&gt;(Testing methodology)\"]\n    B --&gt; D[\"NIST SP 800-63B&lt;br&gt;(Identity assurance levels)\"]\n    B --&gt; E[\"iBeta Certification&lt;br&gt;(Third-party validation)\"]\n\n    C --&gt; F[\"Your Compliance&lt;br&gt;Documentation\"]\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt; G[\"Submit to Regulator\"]</code></pre>"},{"location":"05-standards-compliance/nist/#for-in-house-development","title":"For In-House Development","text":"<p>If building liveness in-house:</p> <ol> <li>Target NIST SP 800-63B IAL2 as your minimum baseline</li> <li>Submit to NIST FRVT for objective benchmarking (it's free)</li> <li>Use NIST FRVT Demographics testing to validate fairness</li> <li>Obtain iBeta certification for commercial validation</li> <li>Reference all three (NIST guidelines, NIST evaluation, iBeta certification) in regulatory submissions</li> </ol>"},{"location":"05-standards-compliance/nist/#nist-vs-ibeta-when-to-use-which","title":"NIST vs iBeta: When to Use Which","text":"Need Use Pass/fail certification for procurement iBeta Comparative ranking against competitors NIST FRVT Regulatory compliance documentation Both (NIST for framework, iBeta for certification) Bias/fairness assessment NIST FRVT Demographics Document morphing detection evaluation NIST FATE Free evaluation with government authority NIST Fast, definitive result for sales/marketing iBeta <p>Best Practice</p> <p>Use both. iBeta certification for commercial credibility and procurement compliance. NIST FRVT for continuous benchmarking and detailed performance understanding. Reference NIST SP 800-63B for regulatory framework compliance.</p> <p>Next: FIDO Biometric Certification \u2192</p>"},{"location":"05-standards-compliance/regional-regulations/","title":"Regional Regulations","text":"<p>Detailed content for this section covers regional regulations requirements and best practices for banking deployments.</p> <p>This page is part of the comprehensive Face Liveness Verification guide.</p>"},{"location":"06-deployment-operations/device-platform/","title":"Device Platform","text":"<p>Detailed deployment guidance for device platform in banking face liveness systems.</p> <p>This page is part of the Face Liveness Verification deployment guide.</p>"},{"location":"06-deployment-operations/error-handling/","title":"Error Handling","text":"<p>Detailed deployment guidance for error handling in banking face liveness systems.</p> <p>This page is part of the Face Liveness Verification deployment guide.</p>"},{"location":"06-deployment-operations/infrastructure/","title":"Infrastructure","text":"<p>Detailed deployment guidance for infrastructure in banking face liveness systems.</p> <p>This page is part of the Face Liveness Verification deployment guide.</p>"},{"location":"06-deployment-operations/input-quality/","title":"6.1 Input Quality Requirements","text":""},{"location":"06-deployment-operations/input-quality/#overview","title":"Overview","text":"<p>The accuracy of any face liveness system is fundamentally constrained by the quality of the input. A state-of-the-art liveness model will fail if the input images are blurry, poorly lit, too small, or improperly framed. Defining and enforcing strict input quality requirements is one of the most impactful steps for a successful deployment.</p> <p>The #1 Deployment Failure</p> <p>The most common cause of high false rejection rates in production is not model accuracy \u2014 it's poor input quality from uncontrolled capture conditions on diverse user devices.</p>"},{"location":"06-deployment-operations/input-quality/#image-quality-parameters","title":"Image Quality Parameters","text":""},{"location":"06-deployment-operations/input-quality/#resolution-face-size","title":"Resolution &amp; Face Size","text":"<pre><code>graph TD\n    A[\"Captured Frame&lt;br&gt;Resolution\"] --&gt; B[\"Face Detection&lt;br&gt;&amp; Cropping\"]\n    B --&gt; C[\"Face Region&lt;br&gt;(Inter-Ocular Distance)\"]\n    C --&gt; D{\"IOD \u2265 90px?\"}\n    D --&gt;|\"Yes\"| E[\"\u2705 Sufficient&lt;br&gt;for liveness\"]\n    D --&gt;|\"No\"| F[\"\u274c Too small&lt;br&gt;Ask user to&lt;br&gt;move closer\"]\n\n    style E fill:#27ae60,color:#fff\n    style F fill:#e74c3c,color:#fff</code></pre> Parameter Minimum Recommended Ideal Notes Capture resolution 640\u00d7480 (VGA) 1280\u00d7720 (720p) 1920\u00d71080 (1080p) Higher resolution preserves texture detail Inter-ocular distance (IOD) 60 pixels 90-120 pixels 150+ pixels Distance between eye centers in the cropped face Face size in frame 20% of frame width 30-50% of frame width 40-60% of frame width Too small = insufficient detail; too large = face cropping Face region resolution 112\u00d7112 224\u00d7224 256\u00d7256+ After cropping and alignment, fed to model Effective face pixels 12,544 (112\u00b2) 50,176 (224\u00b2) 65,536+ (256\u00b2) Total pixels in face region"},{"location":"06-deployment-operations/input-quality/#sharpness-blur","title":"Sharpness / Blur","text":"Blur Type Cause Impact on Liveness Detection Method Motion blur Camera or subject movement during capture Destroys micro-texture and frequency features Laplacian variance &lt; threshold Focus blur Out-of-focus camera, wrong focal plane Smooths pore detail, removes fine texture Gradient magnitude analysis Compression blur Heavy JPEG/H.264 compression Block artifacts, loss of fine detail DCT coefficient analysis <p>Sharpness thresholds (Laplacian variance):</p> Laplacian Variance Quality Action &lt; 50 Very blurry \u2014 unusable Reject, guide user to stabilize 50-100 Blurry \u2014 degraded accuracy Warn user, attempt processing with lower confidence 100-300 Acceptable Process normally &gt; 300 Sharp \u2014 optimal Process with highest confidence"},{"location":"06-deployment-operations/input-quality/#illumination","title":"Illumination","text":"Parameter Minimum Recommended Measurement Ambient illumination 50 lux 200-500 lux Estimated from face brightness Face brightness (mean pixel value) 60 (0-255 range) 100-180 Mean of Y channel in YCbCr Brightness uniformity Ratio &lt; 3:1 (bright/dark side) Ratio &lt; 2:1 Compare left/right face halves No harsh shadows Partial face shadow &lt; 30% No visible shadows Shadow detection on face mask No extreme backlighting Face not silhouetted Face brighter than background Face vs background brightness ratio <p>Lighting condition impact on liveness accuracy:</p> Condition Impact BPCER Increase Well-lit indoor Baseline (optimal) 0% (reference) Moderate indoor Minimal impact +0.5-1% Dim indoor Noticeable texture detail loss +2-5% Harsh overhead lighting Strong shadows confuse depth estimation +3-8% Direct sunlight Overexposure, specular glare +5-15% Extreme backlight Face underexposed, features lost +10-30% Near darkness (&lt; 10 lux) Camera noise dominates signal +20-50%"},{"location":"06-deployment-operations/input-quality/#face-pose-angles","title":"Face Pose Angles","text":"<pre><code>graph LR\n    subgraph \"Acceptable Pose Range\"\n        A[\"YAW&lt;br&gt;(left/right)&lt;br&gt;\u00b120\u00b0\"]\n        B[\"PITCH&lt;br&gt;(up/down)&lt;br&gt;\u00b115\u00b0\"]\n        C[\"ROLL&lt;br&gt;(tilt)&lt;br&gt;\u00b115\u00b0\"]\n    end</code></pre> Angle Acceptable Range Optimal Range Impact of Exceeding Yaw (left-right rotation) \u00b120\u00b0 \u00b110\u00b0 One side of face occluded; asymmetric texture analysis Pitch (up-down tilt) \u00b115\u00b0 \u00b110\u00b0 Forehead or chin occluded; eye region distorted Roll (head tilt) \u00b115\u00b0 \u00b15\u00b0 Face alignment affected; landmark detection degraded"},{"location":"06-deployment-operations/input-quality/#occlusion-handling","title":"Occlusion Handling","text":"Occlusion Type Acceptable Action Notes No occlusion \u2705 Optimal Process normally \u2014 Prescription glasses \u2705 Acceptable Process with adjusted parameters Reflections may affect eye analysis Sunglasses \u274c Not acceptable Request removal Eyes occluded \u2014 critical for liveness Medical mask (N95/surgical) \u26a0\ufe0f Conditional Reduced accuracy; may need fallback Lower face entirely occluded Religious head covering (hijab) \u2705 Acceptable Process normally Face visible; head covering doesn't affect liveness Full face covering (niqab) \u274c Not processable Fallback to alternative verification Face not visible Hat/cap \u2705 Usually acceptable Process if face not shadowed May shadow upper face Hand on face \u274c Not acceptable Guide user to remove hand Partial face occlusion Hair covering face \u26a0\ufe0f Conditional Guide user to adjust hair Depends on coverage extent"},{"location":"06-deployment-operations/input-quality/#video-quality-parameters-for-active-liveness","title":"Video Quality Parameters (for Active Liveness)","text":"Parameter Minimum Recommended Notes Frame rate 10 FPS 15-30 FPS Below 10 FPS loses motion dynamics for active challenges Duration 2 seconds 3-8 seconds Enough for challenge completion + rPPG analysis Codec H.264 Baseline H.264 Main/High Avoid H.265 \u2014 not universally supported Bitrate 500 Kbps 1-2 Mbps Below 500 Kbps introduces heavy compression artifacts Resolution 480p 720p Higher resolution not needed for video (increases bandwidth) Audio Not required for most 16KHz mono Required only for speech-based challenges Temporal consistency No frame drops &gt; 3 consecutive Smooth, even frame delivery Frame drops break temporal analysis"},{"location":"06-deployment-operations/input-quality/#device-camera-requirements","title":"Device Camera Requirements","text":""},{"location":"06-deployment-operations/input-quality/#minimum-camera-specifications","title":"Minimum Camera Specifications","text":"Specification Minimum Recommended Notes Sensor resolution 2 MP (1920\u00d71080) 5 MP+ Higher megapixels don't always mean better quality Auto-focus Fixed focus acceptable Auto-focus preferred Fixed focus may not focus at face distance Focus distance 30-60 cm 25-80 cm range Must be able to focus at arm's length Aperture f/2.8 or wider f/2.0 or wider Wider aperture = better low-light performance Sensor size 1/5\" 1/3\" or larger Larger sensor = lower noise, better dynamic range Color depth 8-bit 8-bit sufficient 10-bit provides no meaningful liveness improvement NIR sensor Not required Beneficial if available Dramatically improves material classification"},{"location":"06-deployment-operations/input-quality/#platform-specific-considerations","title":"Platform-Specific Considerations","text":"AndroidiOSWeb (Browser) <ul> <li>Camera2 API required (Camera1 API insufficient for frame-level control)</li> <li>Auto-exposure settling time: 300-800ms after camera open (first frames are unusable)</li> <li>Manufacturer-specific image processing: Samsung, Xiaomi, Huawei apply different beautification/HDR</li> <li>Critical: Disable beauty mode / face smoothing \u2014 it destroys texture features</li> <li>Memory management: Some low-end devices (&lt; 3GB RAM) may OOM with heavy model inference</li> <li>CameraX recommended for SDK development (abstracts manufacturer differences)</li> </ul> <ul> <li>AVFoundation provides consistent, high-quality camera access</li> <li>TrueDepth camera (iPhone X+) provides depth data \u2014 use if available</li> <li>Auto-exposure more predictable than Android</li> <li>No beauty mode by default \u2014 iOS camera output is cleaner for liveness</li> <li>CoreML / Metal for on-device inference (excellent performance)</li> <li>Privacy: Camera permission prompt required (iOS 14+)</li> </ul> <ul> <li>getUserMedia API for camera access</li> <li>Highly variable quality across browsers and devices</li> <li>No control over auto-focus, exposure, or white balance in most browsers</li> <li>WebRTC for video streaming (if server-side processing)</li> <li>Canvas API for frame extraction</li> <li>Major limitation: Cannot detect virtual cameras or browser extensions manipulating the feed</li> <li>Recommended: Use native SDK for high-security scenarios; web only for low-risk</li> </ul>"},{"location":"06-deployment-operations/input-quality/#quality-assessment-pipeline","title":"Quality Assessment Pipeline","text":"<pre><code>graph TD\n    A[\"Raw Camera&lt;br&gt;Frame\"] --&gt; B[\"Face Detection&lt;br&gt;(BlazeFace / SCRFD)\"]\n    B --&gt; C{\"Face&lt;br&gt;Detected?\"}\n    C --&gt;|\"No\"| D[\"Guide: 'Position&lt;br&gt;your face in frame'\"]\n    C --&gt;|\"Yes\"| E[\"Landmark&lt;br&gt;Detection\"]\n\n    E --&gt; F[\"Check 1:&lt;br&gt;Face Size&lt;br&gt;(IOD \u2265 90px?)\"]\n    F --&gt;|\"Fail\"| G[\"Guide: 'Move&lt;br&gt;closer/farther'\"]\n    F --&gt;|\"Pass\"| H[\"Check 2:&lt;br&gt;Sharpness&lt;br&gt;(Laplacian &gt; 100?)\"]\n\n    H --&gt;|\"Fail\"| I[\"Guide: 'Hold&lt;br&gt;device steady'\"]\n    H --&gt;|\"Pass\"| J[\"Check 3:&lt;br&gt;Brightness&lt;br&gt;(60 &lt; mean &lt; 200?)\"]\n\n    J --&gt;|\"Fail\"| K[\"Guide: 'Move to&lt;br&gt;better lighting'\"]\n    J --&gt;|\"Pass\"| L[\"Check 4:&lt;br&gt;Pose Angles&lt;br&gt;(Within limits?)\"]\n\n    L --&gt;|\"Fail\"| M[\"Guide: 'Look&lt;br&gt;straight at camera'\"]\n    L --&gt;|\"Pass\"| N[\"Check 5:&lt;br&gt;Occlusion&lt;br&gt;(No sunglasses/hands?)\"]\n\n    N --&gt;|\"Fail\"| O[\"Guide: 'Remove&lt;br&gt;obstruction'\"]\n    N --&gt;|\"Pass\"| P[\"\u2705 Quality Pass&lt;br&gt;\u2192 Liveness Analysis\"]\n\n    style P fill:#27ae60,color:#fff</code></pre>"},{"location":"06-deployment-operations/input-quality/#real-time-quality-feedback-ui","title":"Real-Time Quality Feedback UI","text":"<p>Best Practice: Progressive Quality Guidance</p> <p>Show real-time visual feedback overlaid on the camera preview:</p> <ul> <li>Green face outline = All quality checks passing</li> <li>Yellow face outline = Minor issues (lighting, slight blur)</li> <li>Red face outline = Major issues (face too small, severe blur, occluded)</li> <li>Text prompts = Specific actionable guidance (\"Move closer\", \"Find better lighting\")</li> <li>Progress indicator = Shows which checks have passed</li> </ul> <p>This reduces average capture time from 15 seconds to 5 seconds and increases first-attempt success rate from 60% to 85%.</p>"},{"location":"06-deployment-operations/input-quality/#quality-impact-on-liveness-accuracy","title":"Quality Impact on Liveness Accuracy","text":"Quality Level Description Liveness BPCER Liveness APCER Recommendation Excellent All parameters optimal 0.5-1% &lt; 0.1% Proceed with high confidence Good Minor deviations (slight pose, acceptable blur) 1-3% 0.1-0.5% Proceed normally Acceptable Noticeable issues (dim lighting, moderate blur) 3-8% 0.5-2% Proceed with reduced confidence; consider active challenge Poor Significant issues (very dim, blurry, extreme pose) 8-20% 2-5% Reject and re-capture with guidance Unusable Critical failures (no face, extreme blur, fully occluded) N/A N/A Reject immediately"},{"location":"06-deployment-operations/input-quality/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ol> <li>Input quality is the single biggest factor in production liveness accuracy</li> <li>IOD \u2265 90 pixels is the minimum face size for reliable liveness analysis</li> <li>Disable beauty mode / face smoothing on all Android devices \u2014 it destroys liveness texture features</li> <li>Real-time quality feedback reduces capture time by 3x and increases success rates by 25%</li> <li>Web browser liveness has inherent limitations \u2014 use native SDK for high-security banking</li> <li>Camera warm-up time (300-800ms on Android) means first few frames are unusable \u2014 discard them</li> <li>Define quality gates that reject poor input before it reaches the liveness model</li> </ol> <p>Next: Device &amp; Platform Considerations \u2192</p>"},{"location":"06-deployment-operations/model-updates/","title":"Model Updates","text":"<p>Detailed deployment guidance for model updates in banking face liveness systems.</p> <p>This page is part of the Face Liveness Verification deployment guide.</p>"},{"location":"06-deployment-operations/monitoring/","title":"Monitoring","text":"<p>Detailed deployment guidance for monitoring in banking face liveness systems.</p> <p>This page is part of the Face Liveness Verification deployment guide.</p>"},{"location":"06-deployment-operations/network-bandwidth/","title":"Network Bandwidth","text":"<p>Detailed deployment guidance for network bandwidth in banking face liveness systems.</p> <p>This page is part of the Face Liveness Verification deployment guide.</p>"},{"location":"06-deployment-operations/ux-patterns/","title":"Ux Patterns","text":"<p>Detailed deployment guidance for ux patterns in banking face liveness systems.</p> <p>This page is part of the Face Liveness Verification deployment guide.</p>"},{"location":"07-banking-use-cases/account-opening/","title":"7.1 Account Opening (Digital Onboarding)","text":""},{"location":"07-banking-use-cases/account-opening/#the-primary-use-case","title":"The Primary Use Case","text":"<p>Digital account opening is the highest-volume use case for face liveness verification in banking. It represents 80%+ of all liveness verification transactions.</p>"},{"location":"07-banking-use-cases/account-opening/#complete-flow","title":"Complete Flow","text":"<pre><code>graph TD\n    A[\"Customer downloads&lt;br&gt;banking app\"] --&gt; B[\"Selects 'Open Account'\"]\n    B --&gt; C[\"Enters basic info&lt;br&gt;(name, phone, email)\"]\n    C --&gt; D[\"OTP verification&lt;br&gt;(phone/email)\"]\n    D --&gt; E[\"Document capture&lt;br&gt;(ID front + back)\"]\n    E --&gt; F[\"Document verification&lt;br&gt;&amp; OCR extraction\"]\n    F --&gt; G[\"FACE LIVENESS&lt;br&gt;VERIFICATION\"]\n    G --&gt; H[\"Face matching&lt;br&gt;(live face vs document photo)\"]\n    H --&gt; I[\"PEP/Sanctions&lt;br&gt;screening\"]\n    I --&gt; J{\"Risk Decision\"}\n    J --&gt;|\"Auto-approve\"| K[\"Account activated&lt;br&gt;(2-5 minutes total)\"]\n    J --&gt;|\"Review\"| L[\"Manual review&lt;br&gt;(24-48 hours)\"]\n    J --&gt;|\"Reject\"| M[\"Application declined\"]</code></pre>"},{"location":"07-banking-use-cases/account-opening/#liveness-configuration-for-onboarding","title":"Liveness Configuration for Onboarding","text":"Parameter Recommended Setting Rationale Method Hybrid (passive-first) Balance UX with security Passive threshold (auto-pass) \u2265 0.88 80% of users pass without active challenge Active challenge (if needed) 1-2 challenges (head turn + smile) Minimal friction for uncertain cases Max attempts 3 Prevent brute-force while accommodating genuine failures Session timeout 5 minutes Enough time without leaving session open for exploitation Deepfake detection Enabled Essential for digital channel Device attestation Enabled Detect virtual cameras and rooted devices"},{"location":"07-banking-use-cases/account-opening/#conversion-impact","title":"Conversion Impact","text":"Metric Before Liveness (Branch) After Liveness (Digital) Improvement Completion rate 45% 82% +82% Time to open 3-5 days 4-8 minutes 99% reduction Cost per account $25-50 $0.50-2.00 95% reduction Geographic reach Branch footprint Nationwide Unlimited Fraud rate 0.3% 0.05% 83% reduction <p>Next: Transaction Authentication \u2192</p>"},{"location":"07-banking-use-cases/account-recovery/","title":"Account Recovery","text":"<p>Banking use case details for account recovery with face liveness verification integration.</p> <p>Part of the Face Liveness Verification in eKYC guide.</p>"},{"location":"07-banking-use-cases/agent-kiosk/","title":"Agent Kiosk","text":"<p>Banking use case details for agent kiosk with face liveness verification integration.</p> <p>Part of the Face Liveness Verification in eKYC guide.</p>"},{"location":"07-banking-use-cases/cross-border/","title":"Cross Border","text":"<p>Banking use case details for cross border with face liveness verification integration.</p> <p>Part of the Face Liveness Verification in eKYC guide.</p>"},{"location":"07-banking-use-cases/loan-disbursement/","title":"Loan Disbursement","text":"<p>Banking use case details for loan disbursement with face liveness verification integration.</p> <p>Part of the Face Liveness Verification in eKYC guide.</p>"},{"location":"07-banking-use-cases/transaction-auth/","title":"7.2 Transaction Authentication","text":""},{"location":"07-banking-use-cases/transaction-auth/#step-up-authentication-with-liveness","title":"Step-Up Authentication with Liveness","text":"<p>For high-value or high-risk transactions, liveness verification serves as a step-up authentication factor.</p> <pre><code>graph TD\n    A[\"Customer initiates&lt;br&gt;transaction\"] --&gt; B{\"Transaction&lt;br&gt;risk level?\"}\n    B --&gt;|\"Low (&lt; $1K)\"| C[\"Standard auth&lt;br&gt;(PIN/OTP)\"]\n    B --&gt;|\"Medium ($1K-50K)\"| D[\"Passive liveness&lt;br&gt;(single selfie)\"]\n    B --&gt;|\"High (&gt; $50K)\"| E[\"Active liveness&lt;br&gt;(challenges +&lt;br&gt;deepfake detection)\"]\n    B --&gt;|\"Critical (&gt; $500K)\"| F[\"Full liveness +&lt;br&gt;manual approval\"]</code></pre> Transaction Type Liveness Requirement Threshold Internal transfer &lt; $1,000 None (standard auth) \u2014 External transfer $1K-$50K Passive liveness 0.80 Wire transfer &gt; $50K Active + passive 0.90 Beneficiary change Active liveness 0.85 Large loan disbursement Full hybrid + deepfake 0.92 <p>Next: Video KYC (V-CIP) \u2192</p>"},{"location":"07-banking-use-cases/video-kyc/","title":"7.3 Video KYC (V-CIP)","text":""},{"location":"07-banking-use-cases/video-kyc/#rbi-v-cip-framework","title":"RBI V-CIP Framework","text":"<p>Video-based Customer Identification Process (V-CIP) is an RBI-approved method for remote KYC in India, where a bank official conducts a live video call with the customer.</p>"},{"location":"07-banking-use-cases/video-kyc/#how-liveness-integrates-with-v-cip","title":"How Liveness Integrates with V-CIP","text":"<pre><code>graph TD\n    A[\"Customer connects&lt;br&gt;to video call\"] --&gt; B[\"Automated liveness&lt;br&gt;check runs&lt;br&gt;continuously\"]\n    B --&gt; C[\"Bank official&lt;br&gt;verifies identity&lt;br&gt;via video\"]\n    C --&gt; D[\"Official captures&lt;br&gt;screenshots as&lt;br&gt;evidence\"]\n    D --&gt; E[\"Automated face&lt;br&gt;matching against&lt;br&gt;ID document\"]\n    E --&gt; F[\"V-CIP session&lt;br&gt;recorded and&lt;br&gt;archived\"]</code></pre> V-CIP Requirement Liveness Role Real-time interaction Continuous liveness monitoring throughout call Customer identification Liveness + face matching against ID Geo-tagging Verify customer's claimed location matches device GPS Video recording Ensure recorded video is of live person (not replayed) Official verification Liveness assists official; doesn't replace human judgment <p>Next: Loan Disbursement \u2192</p>"},{"location":"08-quality-testing/datasets/","title":"Datasets","text":"<p>Detailed quality and testing content for datasets.</p> <p>Part of the Face Liveness Verification testing guide.</p>"},{"location":"08-quality-testing/demographic-performance/","title":"Demographic Performance","text":"<p>Detailed quality and testing content for demographic performance.</p> <p>Part of the Face Liveness Verification testing guide.</p>"},{"location":"08-quality-testing/edge-cases/","title":"Edge Cases","text":"<p>Detailed quality and testing content for edge cases.</p> <p>Part of the Face Liveness Verification testing guide.</p>"},{"location":"08-quality-testing/performance-metrics/","title":"8.1 Performance Metrics Deep Dive","text":""},{"location":"08-quality-testing/performance-metrics/#core-metrics","title":"Core Metrics","text":""},{"location":"08-quality-testing/performance-metrics/#the-apcer-bpcer-tradeoff","title":"The APCER-BPCER Tradeoff","text":"<pre><code>graph LR\n    A[\"Tighter Threshold&lt;br&gt;(More Secure)\"] --&gt; B[\"Lower APCER&lt;br&gt;(Fewer attacks pass)\"]\n    A --&gt; C[\"Higher BPCER&lt;br&gt;(More genuine users rejected)\"]\n\n    D[\"Looser Threshold&lt;br&gt;(More Permissive)\"] --&gt; E[\"Higher APCER&lt;br&gt;(More attacks pass)\"]\n    D --&gt; F[\"Lower BPCER&lt;br&gt;(Fewer genuine users rejected)\"]</code></pre>"},{"location":"08-quality-testing/performance-metrics/#metrics-reference","title":"Metrics Reference","text":"Metric Formula Banking Target What It Means APCER Attacks accepted / Total attacks (per species) &lt; 1% (iBeta requires 0%) How many attacks get through BPCER Genuine rejected / Total genuine &lt; 5% How many real users are blocked ACER (APCER + BPCER) / 2 &lt; 3% Overall balanced error EER Point where APCER = BPCER &lt; 2% System's inherent accuracy TDR @ BPCER=1% Attack detection rate at 1% false rejection &gt; 99% Security at practical operating point AUC Area under ROC curve &gt; 0.999 Overall discriminative power"},{"location":"08-quality-testing/performance-metrics/#reporting-best-practices","title":"Reporting Best Practices","text":"<p>Always Report Per-Species APCER</p> <p>Aggregate APCER is misleading. A system might have 0% APCER for prints but 15% for screen replays. Always report APCER broken down by PAI species, as required by ISO 30107-3.</p>"},{"location":"08-quality-testing/performance-metrics/#threshold-tuning","title":"Threshold Tuning","text":"Banking Context Recommended Operating Point Expected BPCER Mass-market onboarding APCER &lt; 2% 2-3% Premium account opening APCER &lt; 0.5% 3-5% High-value transactions APCER &lt; 0.1% 5-8% Regulatory-critical (V-CIP) APCER = 0% (target) 8-12% <p>Next: Testing Methodology \u2192</p>"},{"location":"08-quality-testing/red-team/","title":"Red Team","text":"<p>Detailed quality and testing content for red team.</p> <p>Part of the Face Liveness Verification testing guide.</p>"},{"location":"08-quality-testing/testing-methodology/","title":"8.2 Testing Methodology","text":""},{"location":"08-quality-testing/testing-methodology/#testing-layers","title":"Testing Layers","text":"<pre><code>graph TD\n    A[\"Unit Tests&lt;br&gt;(Component level)\"] --&gt; B[\"Integration Tests&lt;br&gt;(Pipeline level)\"]\n    B --&gt; C[\"PAD Evaluation&lt;br&gt;(ISO 30107-3&lt;br&gt;protocol)\"]\n    C --&gt; D[\"Red Team&lt;br&gt;Exercises\"]\n    D --&gt; E[\"Production&lt;br&gt;Monitoring\"]</code></pre>"},{"location":"08-quality-testing/testing-methodology/#internal-pad-test-protocol","title":"Internal PAD Test Protocol","text":"<p>Before submitting to iBeta, conduct comprehensive internal testing:</p> Phase What How Minimum Scale Phase 1: Lab testing Controlled environment, known attacks Internal team generates attacks 50 subjects \u00d7 10 species = 500 attacks Phase 2: Extended testing Multiple environments, more subjects Recruit diverse test subjects 200 subjects \u00d7 15 species = 3,000 attacks Phase 3: Red team Adversarial testing by security team Team actively tries to bypass system Ongoing, document all findings Phase 4: Field pilot Real-world conditions with real users Limited deployment with monitoring 10,000+ genuine + track attack attempts"},{"location":"08-quality-testing/testing-methodology/#test-matrix-template","title":"Test Matrix Template","text":"PAI Species # Attempts # Accepted APCER Status Print (laser, A4) 150 0 0.00% \u2705 Pass Print (photo lab, glossy) 150 0 0.00% \u2705 Pass Screen (phone, photo) 150 0 0.00% \u2705 Pass Screen (phone, video) 150 1 0.67% \u26a0\ufe0f Review Screen (tablet, video) 150 0 0.00% \u2705 Pass Screen (4K monitor, video) 150 3 2.00% \u274c Fail Paper mask (cutouts) 150 0 0.00% \u2705 Pass Latex mask 150 2 1.33% \u274c Fail Bona fide 300 285 accepted BPCER = 5% \u26a0\ufe0f Borderline <p>Next: Red Team &amp; Penetration Testing \u2192</p>"},{"location":"09-security-privacy/anti-fraud/","title":"9.2 Anti-Fraud Intelligence","text":""},{"location":"09-security-privacy/anti-fraud/#beyond-liveness-fraud-signal-integration","title":"Beyond Liveness: Fraud Signal Integration","text":"<pre><code>graph TD\n    A[\"Liveness Score\"] --&gt; F[\"Fraud Decision Engine\"]\n    B[\"Device Fingerprint\"] --&gt; F\n    C[\"Velocity Checks&lt;br&gt;(attempts per device/IP)\"] --&gt; F\n    D[\"Behavioral Biometrics&lt;br&gt;(typing, swiping patterns)\"] --&gt; F\n    E[\"Network Intelligence&lt;br&gt;(IP reputation, VPN/proxy)\"] --&gt; F\n\n    F --&gt; G{\"Risk Level\"}</code></pre>"},{"location":"09-security-privacy/anti-fraud/#key-anti-fraud-signals","title":"Key Anti-Fraud Signals","text":"Signal What It Detects Implementation Device fingerprinting Same device used for multiple identities Device ID hash (hardware IDs, screen, OS version) Velocity checks Brute-force attempts, fraud rings Max 3 attempts/session, 10/day/device, 50/day/IP Face clustering Same face attempting multiple identities Compare face embeddings across recent applications Geo-velocity Impossible travel (two applications from distant locations within short time) GPS + IP geolocation comparison IP reputation Known VPN, proxy, data center IPs Commercial IP intelligence feeds Behavioral biometrics Bot-like interaction patterns Touch pressure, typing speed, scrolling patterns Dark web monitoring Liveness bypass tools being sold Threat intelligence feeds, underground forum monitoring <p>Next: Privacy &amp; Data Protection \u2192</p>"},{"location":"09-security-privacy/incident-response/","title":"9.4 Incident Response Playbook","text":""},{"location":"09-security-privacy/incident-response/#when-a-liveness-bypass-is-discovered","title":"When a Liveness Bypass Is Discovered","text":"<pre><code>graph TD\n    A[\"\ud83d\udea8 Bypass&lt;br&gt;Detected\"] --&gt; B[\"1. CONTAIN&lt;br&gt;(Disable affected&lt;br&gt;attack vector)\"]\n    B --&gt; C[\"2. ASSESS&lt;br&gt;(How many accounts&lt;br&gt;affected?)\"]\n    C --&gt; D[\"3. NOTIFY&lt;br&gt;(Regulator, CISO,&lt;br&gt;Board)\"]\n    D --&gt; E[\"4. REMEDIATE&lt;br&gt;(Patch model,&lt;br&gt;update thresholds)\"]\n    E --&gt; F[\"5. RE-VERIFY&lt;br&gt;(Review affected&lt;br&gt;accounts)\"]\n    F --&gt; G[\"6. REPORT&lt;br&gt;(Root cause,&lt;br&gt;lessons learned)\"]</code></pre>"},{"location":"09-security-privacy/incident-response/#response-timeline","title":"Response Timeline","text":"Action SLA Responsible Initial detection and triage &lt; 1 hour Security Operations Center Contain (disable vulnerable vector) &lt; 4 hours Engineering + SecOps Impact assessment (how many affected) &lt; 24 hours Fraud + Engineering Regulator notification (if required) &lt; 72 hours (GDPR), varies by jurisdiction Compliance Model patch deployed &lt; 1 week ML Engineering Affected account re-verification &lt; 2 weeks Operations + Fraud Root cause analysis report &lt; 30 days Security + Engineering"},{"location":"09-security-privacy/incident-response/#communication-templates","title":"Communication Templates","text":"<p>Regulator Notification (Template)</p> <p>Subject: Biometric Verification System Security Incident Notification</p> <p>Key information to include: - Date and time of discovery - Nature of the vulnerability - Number of potentially affected accounts - Immediate containment actions taken - Remediation plan and timeline - Customer impact assessment - Ongoing monitoring measures</p> <p>Next: Legal &amp; Contractual Framework \u2192</p>"},{"location":"09-security-privacy/legal-framework/","title":"9.5 Legal &amp; Contractual Framework","text":""},{"location":"09-security-privacy/legal-framework/#liability-framework","title":"Liability Framework","text":"Scenario Typical Liability Contractual Protection Liveness bypass leads to fraud Vendor (if within contracted performance) or Bank (if outside contracted scope) SLA with APCER guarantees and financial penalties for breach False rejection of genuine customer Bank (customer relationship) Insurance + vendor SLA on BPCER Data breach of biometric data Bank (as data controller under GDPR/DPDPA) Data processor agreement with vendor Regulatory non-compliance Bank Vendor provides compliance documentation and audit support"},{"location":"09-security-privacy/legal-framework/#key-contract-clauses-for-liveness-vendors","title":"Key Contract Clauses for Liveness Vendors","text":"Clause What to Include Performance SLA APCER \u2264 X% and BPCER \u2264 Y% at defined operating point Availability SLA 99.9%+ uptime with defined maintenance windows Latency SLA P95 response time &lt; 500ms Certification Maintain iBeta Level 1/2 certification (re-certify after major updates) Indemnification Vendor indemnifies for losses caused by PAD failures within SLA Data processing GDPR/DPDPA-compliant data processor agreement Audit rights Bank can audit vendor's security, data handling, model performance Model updates Minimum quarterly model updates with testing evidence Incident response Defined notification timelines and cooperation obligations Exit clause Data portability and transition support upon contract termination <p>Back to: Security Hardening \u2192</p>"},{"location":"09-security-privacy/privacy/","title":"9.3 Privacy &amp; Data Protection","text":""},{"location":"09-security-privacy/privacy/#biometric-data-lifecycle","title":"Biometric Data Lifecycle","text":"<pre><code>graph LR\n    A[\"Capture\"] --&gt; B[\"Process\"] --&gt; C[\"Store&lt;br&gt;(if required)\"] --&gt; D[\"Retain&lt;br&gt;(per regulation)\"] --&gt; E[\"Delete&lt;br&gt;(mandatory)\"]</code></pre>"},{"location":"09-security-privacy/privacy/#regulatory-requirements","title":"Regulatory Requirements","text":"Regulation Classification Consent Storage Retention Deletion GDPR (EU) Special category (Art. 9) Explicit consent required Encrypted, within EU/adequate country Purpose-limited Right to erasure (Art. 17) DPDPA (India) Sensitive personal data Informed consent Within India (data localization) Per purpose + regulatory need On withdrawal of consent BIPA (Illinois) Biometric identifier Written informed consent N/A (don't sell/lease) Max 3 years or purpose completion Upon achieving purpose CCPA (California) Biometric information Notice at collection Reasonable security 12 months minimum record Right to delete"},{"location":"09-security-privacy/privacy/#privacy-by-design-principles","title":"Privacy-by-Design Principles","text":"Principle Implementation Data minimization Process liveness without storing raw face images when possible Purpose limitation Use biometric data only for identity verification, not marketing Encryption at rest AES-256 encryption for any stored biometric data Encryption in transit TLS 1.3 for all biometric data transmission Access controls Strict RBAC; biometric data accessible only to verification pipeline Audit trails Log all access to biometric data for compliance auditing Retention limits Delete raw images after verification; retain only scores/decisions Consent management Obtain explicit consent before biometric capture; allow withdrawal <p>Next: Incident Response Playbook \u2192</p>"},{"location":"09-security-privacy/security-hardening/","title":"9.1 Security Hardening","text":""},{"location":"09-security-privacy/security-hardening/#defense-layers","title":"Defense Layers","text":"<pre><code>graph TD\n    A[\"Layer 1: Device Integrity&lt;br&gt;(Root/JB detection, attestation)\"] --&gt; B[\"Layer 2: SDK Integrity&lt;br&gt;(Obfuscation, anti-tampering)\"]\n    B --&gt; C[\"Layer 3: Transport Security&lt;br&gt;(TLS pinning, encryption)\"]\n    C --&gt; D[\"Layer 4: Session Security&lt;br&gt;(Nonces, anti-replay)\"]\n    D --&gt; E[\"Layer 5: Model Security&lt;br&gt;(Obfuscation, adversarial defense)\"]\n    E --&gt; F[\"Layer 6: API Security&lt;br&gt;(Rate limiting, authentication)\"]\n    F --&gt; G[\"Layer 7: Monitoring&lt;br&gt;(Anomaly detection, alerting)\"]</code></pre>"},{"location":"09-security-privacy/security-hardening/#implementation-checklist","title":"Implementation Checklist","text":"Control Priority Implementation TLS 1.3 with certificate pinning P0 Pin to leaf or intermediate certificate Device attestation P0 SafetyNet/Play Integrity (Android), DeviceCheck (iOS) Session nonces P0 Cryptographically random, single-use, time-bound Frame encryption P0 AES-256-GCM with session-derived keys Anti-replay tokens P0 One-time tokens bound to session + timestamp Root/jailbreak detection P1 Multiple detection methods (not just one check) Code obfuscation P1 ProGuard/R8 (Android), bitcode (iOS), LLVM obfuscation Anti-debugging P1 Detect Frida, GDB, LLDB, Cycript Model encryption P1 Encrypt model weights at rest, decrypt in memory Rate limiting P1 Per-device, per-IP, per-session limits API authentication P1 OAuth 2.0 / API keys with rotation Score obfuscation P2 Return binary decisions to client, never raw scores Sensor correlation P2 Verify camera motion matches gyroscope data <p>Next: Anti-Fraud Intelligence \u2192</p>"},{"location":"10-business-strategy/build-vs-buy/","title":"10.1 Build vs Buy Analysis","text":""},{"location":"10-business-strategy/build-vs-buy/#decision-framework","title":"Decision Framework","text":"<pre><code>graph TD\n    A{\"Core competency&lt;br&gt;in biometric AI?\"} --&gt;|\"Yes\"| B{\"Budget for&lt;br&gt;5+ person ML team&lt;br&gt;(ongoing)?\"}\n    A --&gt;|\"No\"| C[\"BUY (SDK/API)\"]\n\n    B --&gt;|\"Yes\"| D{\"Regulatory need&lt;br&gt;for full control?\"}\n    B --&gt;|\"No\"| C\n\n    D --&gt;|\"Yes\"| E[\"BUILD in-house\"]\n    D --&gt;|\"No\"| F[\"HYBRID&lt;br&gt;(Buy SDK +&lt;br&gt;customize)\"]</code></pre>"},{"location":"10-business-strategy/build-vs-buy/#comparison","title":"Comparison","text":"Factor Build In-House Buy (SDK/API) Hybrid Time to market 12-24 months 2-4 weeks 2-6 months Initial cost $500K-2M $50K-200K $200K-500K Annual cost $300K-1M (team) $100K-500K (license) $200K-600K Team required 5-10 ML engineers 1-2 integration engineers 2-4 engineers iBeta certification Your responsibility ($30K-80K) Vendor provides Shared Model updates Your responsibility (continuous) Vendor provides Shared Customization Full control Limited Good Data sovereignty Full control Depends on vendor architecture Better control Risk High (technical + regulatory) Low (vendor responsibility) Medium"},{"location":"10-business-strategy/build-vs-buy/#when-to-build","title":"When to Build","text":"<p>Build If:</p> <ul> <li>You have 100M+ annual verifications (cost efficiency at scale)</li> <li>Biometric AI is a core business differentiator</li> <li>Regulatory requirements demand full data and model control</li> <li>You have established ML infrastructure and team</li> </ul>"},{"location":"10-business-strategy/build-vs-buy/#when-to-buy","title":"When to Buy","text":"<p>Buy If:</p> <ul> <li>You need to launch within 3 months</li> <li>Annual verification volume &lt; 10M</li> <li>You don't have ML engineering capacity</li> <li>You want proven iBeta-certified solution</li> </ul> <p>Next: Vendor Evaluation Framework \u2192</p>"},{"location":"10-business-strategy/case-studies/","title":"10.5 Case Studies","text":""},{"location":"10-business-strategy/case-studies/#case-1-large-indian-private-bank-digital-onboarding-transformation","title":"Case 1: Large Indian Private Bank \u2014 Digital Onboarding Transformation","text":"Aspect Before After KYC method Branch-based with Aadhaar biometric Digital eKYC with face liveness Onboarding time 3-5 days 8 minutes average Monthly new accounts 45,000 180,000 Cost per account \u20b9350 ($4.20) \u20b925 ($0.30) Fraud rate 0.4% 0.03% Liveness approach N/A Hybrid (passive + active) Key challenge High device diversity (budget Android phones) Tuned for low-end devices, progressive quality guidance"},{"location":"10-business-strategy/case-studies/#case-2-european-neobank-deepfake-attack-incident","title":"Case 2: European Neobank \u2014 Deepfake Attack Incident","text":"Timeline Event Month 1 Neobank launches with passive-only liveness (vendor A) Month 4 Fraud team detects 23 accounts opened with AI-generated faces (StyleGAN) Month 5 Investigation reveals additional 150+ synthetic identity accounts Month 6 Switched to vendor B with deepfake detection; added active challenges for high-risk Month 8 Synthetic identity fraud dropped to zero; overall fraud rate down 94% Lesson Passive-only liveness without deepfake detection is insufficient for digital banking"},{"location":"10-business-strategy/case-studies/#case-3-southeast-asian-digital-bank-accessibility-driven-design","title":"Case 3: Southeast Asian Digital Bank \u2014 Accessibility-Driven Design","text":"Challenge Solution Result Diverse user base including elderly with limited tech literacy Passive-first approach with simple guidance overlay 89% first-attempt completion rate Multiple languages (Thai, Malay, Bahasa, English) Localized challenge prompts and guidance 12% improvement in completion for non-English speakers Hot climate causing face moisture/reflection issues Adaptive specular highlight handling 40% reduction in outdoor false rejections Low-end devices (&lt; $100 phones) Lightweight on-device model + server-side verification Works on devices with 2GB RAM <p>Next: Future Trends \u2192</p>"},{"location":"10-business-strategy/cost-roi/","title":"10.3 Cost Analysis &amp; ROI","text":""},{"location":"10-business-strategy/cost-roi/#cost-components","title":"Cost Components","text":"Component One-Time Annual Recurring SDK/API license $0-50K (setup) $100K-500K (volume-dependent) Integration development $50K-200K $20K-50K (maintenance) Infrastructure (GPU) $0 (if cloud API) $50K-200K (if self-hosted) iBeta certification (if in-house) $30K-80K $15K-30K (re-certification) Security testing $20K-50K $10K-30K Compliance &amp; legal $10K-30K $5K-15K Total (Buy) $80K-300K $150K-600K Total (Build) $500K-2M $300K-1M"},{"location":"10-business-strategy/cost-roi/#roi-model","title":"ROI Model","text":"<pre><code>Annual Fraud Prevention Savings:\n  Digital onboarding attempts/year:     500,000\n  Attempted fraud rate:                 0.5% (2,500 attacks)\n  Average fraud loss per success:       $5,000\n\n  Without liveness (80% attack success): 2,000 \u00d7 $5,000 = $10,000,000\n  With liveness (0.5% success):          12 \u00d7 $5,000  = $60,000\n\n  Annual fraud savings:                  $9,940,000\n\nOperational Savings:\n  Branch KYC cost/verification:          $30\n  Digital KYC cost/verification:         $1.50\n  Annual verifications:                  500,000\n  Annual operational savings:            $14,250,000\n\nTotal Annual Benefit:                    $24,190,000\nTotal Annual Cost (Buy model):           $300,000\nROI:                                     80x\nPayback Period:                          &lt; 2 months\n</code></pre> <p>Next: Implementation Roadmap \u2192</p>"},{"location":"10-business-strategy/future-trends/","title":"10.6 Future Trends","text":""},{"location":"10-business-strategy/future-trends/#emerging-technologies-2025-2030","title":"Emerging Technologies (2025-2030)","text":"Trend Timeline Impact Readiness Multimodal biometrics (face + voice + behavioral) Now - 2026 Higher security through signal diversity Production-ready On-device large models (efficient transformers on mobile) 2025-2027 Privacy-preserving, lower latency Early adoption Federated learning for liveness models 2026-2028 Privacy-preserving model training across institutions Research stage Post-quantum cryptography for biometric data 2027-2030 Future-proof encryption of biometric templates Standards emerging Neural avatars / 3D Gaussians as attack vectors Now - 2026 Real-time photorealistic face rendering threatens current detection Active threat AI-generated video (Sora, Runway) Now - 2026 Full-scene video generation may bypass environmental checks Growing threat Continuous authentication (always-on liveness during session) 2025-2027 Protects against session hijacking post-verification Early production Decentralized identity (self-sovereign with biometrics) 2027-2030 User-controlled biometric credentials stored on blockchain/device Conceptual Regulation convergence (global PAD standards) 2026-2028 Unified requirements across jurisdictions In progress"},{"location":"10-business-strategy/future-trends/#the-path-forward","title":"The Path Forward","text":"<p>Strategic Recommendations</p> <ol> <li>Invest in deepfake detection \u2014 this is the fastest-growing threat vector</li> <li>Plan for continuous model updates \u2014 treat liveness as a living system, not a one-time deployment</li> <li>Build multi-modal \u2014 don't rely on face liveness alone; add voice, behavioral, device signals</li> <li>Monitor the research landscape \u2014 academic conferences (CVPR, ICCV, ECCV) publish new attacks and defenses quarterly</li> <li>Engage with regulators proactively \u2014 shape emerging standards rather than reacting to them</li> <li>Join industry consortiums \u2014 FS-ISAC, FIDO Alliance, Biometrics Institute for threat intelligence sharing</li> </ol>"},{"location":"10-business-strategy/implementation-roadmap/","title":"10.4 Implementation Roadmap","text":""},{"location":"10-business-strategy/implementation-roadmap/#phased-deployment-plan","title":"Phased Deployment Plan","text":"<pre><code>gantt\n    title Face Liveness Implementation Roadmap\n    dateFormat  YYYY-MM\n\n    section Phase 1: Foundation\n    Vendor evaluation &amp; selection    :2026-01, 2M\n    Contract &amp; procurement          :2026-02, 1M\n\n    section Phase 2: Integration\n    SDK integration (dev)           :2026-03, 2M\n    Backend infrastructure          :2026-03, 2M\n    Security testing                :2026-05, 1M\n\n    section Phase 3: Pilot\n    Internal pilot (employees)      :2026-06, 1M\n    Limited user pilot (5%)         :2026-07, 1M\n    Performance monitoring          :2026-07, 1M\n\n    section Phase 4: Rollout\n    Gradual rollout (25%\u219250%\u2192100%)  :2026-08, 2M\n    Full production                 :2026-10, 1M\n\n    section Phase 5: Optimization\n    Threshold tuning                :2026-10, 2M\n    Model updates                   :2026-11, 1M\n    Regulatory audit                :2026-12, 1M</code></pre>"},{"location":"10-business-strategy/implementation-roadmap/#go-live-checklist","title":"Go-Live Checklist","text":"<ul> <li> iBeta Level 1 certification obtained (or vendor-provided)</li> <li> Internal security testing passed (all PAI species)</li> <li> Red team exercise completed</li> <li> BPCER &lt; 5% confirmed on production device mix</li> <li> Deepfake detection enabled and tested</li> <li> Device attestation enabled</li> <li> Audit logging verified and compliant</li> <li> Fallback flows tested (what happens when liveness fails)</li> <li> Manual review queue configured and staffed</li> <li> Monitoring dashboards live</li> <li> Regulatory documentation prepared</li> <li> Customer communication prepared</li> <li> Support team trained on liveness failure handling</li> <li> Data privacy impact assessment completed</li> <li> Incident response playbook documented</li> </ul> <p>Next: Case Studies \u2192</p>"},{"location":"10-business-strategy/vendor-evaluation/","title":"10.2 Vendor Evaluation Framework","text":""},{"location":"10-business-strategy/vendor-evaluation/#evaluation-criteria-matrix","title":"Evaluation Criteria Matrix","text":"Category Criteria Weight Scoring (1-5) Security iBeta Level 1 certification 15% 5=L2 certified, 4=L1, 3=Testing, 1=None Security Deepfake detection capability 10% 5=Real-time + offline, 3=Basic, 1=None Security Injection attack defense 8% 5=Comprehensive, 3=Basic, 1=None Performance APCER (overall) 10% 5=&lt;0.1%, 3=&lt;1%, 1=&gt;5% Performance BPCER 8% 5=&lt;1%, 3=&lt;3%, 1=&gt;5% Performance Latency (P95) 5% 5=&lt;300ms, 3=&lt;1s, 1=&gt;3s UX Passive liveness support 7% 5=Full, 3=Partial, 1=Active only UX Drop-off rate 5% 5=&lt;5%, 3=&lt;10%, 1=&gt;15% Compliance GDPR/DPDPA compliant 7% 5=Fully, 3=Partially, 1=No Compliance Data residency options 5% 5=Any region, 3=Major regions, 1=US only Technical SDK size (mobile) 3% 5=&lt;5MB, 3=&lt;10MB, 1=&gt;20MB Technical Platform coverage 5% 5=iOS+Android+Web, 3=Mobile only, 1=Single Technical On-device inference option 3% 5=Full, 3=Partial, 1=None Business Pricing model 4% 5=Per-transaction, 3=Tiered, 1=Fixed high Business SLA guarantees 5% 5=Strong with penalties, 3=Basic, 1=None"},{"location":"10-business-strategy/vendor-evaluation/#poc-testing-protocol","title":"PoC Testing Protocol","text":"Phase Duration Activities Phase 1: Technical Integration 1-2 weeks SDK integration, API connectivity, basic functionality Phase 2: Security Testing 2-3 weeks Internal attack testing (10+ PAI species), injection testing Phase 3: UX Testing 1-2 weeks User testing with 50+ subjects, drop-off measurement Phase 4: Scale Testing 1 week Load testing, latency measurement, failure mode testing Phase 5: Evaluation 1 week Score against criteria matrix, vendor comparison <p>Next: Cost Analysis &amp; ROI \u2192</p>"},{"location":"appendices/go-live-checklist/","title":"Appendix E: Production Go-Live Checklist","text":""},{"location":"appendices/go-live-checklist/#security-readiness","title":"Security Readiness","text":"<ul> <li> iBeta Level 1 certification verified (minimum)</li> <li> iBeta Level 2 certification verified (recommended for high-value)</li> <li> Internal PAD testing passed across all target PAI species</li> <li> Deepfake detection module enabled and tested</li> <li> Virtual camera / injection attack defense verified</li> <li> Device attestation (SafetyNet/Play Integrity/DeviceCheck) enabled</li> <li> Red team exercise completed \u2014 all findings addressed</li> <li> Adversarial testing performed \u2014 model robust</li> <li> Penetration test on API endpoints completed</li> </ul>"},{"location":"appendices/go-live-checklist/#performance-readiness","title":"Performance Readiness","text":"<ul> <li> APCER &lt; 1% confirmed across all PAI species</li> <li> BPCER &lt; 5% confirmed on target device mix</li> <li> End-to-end latency P95 &lt; 2 seconds</li> <li> Drop-off rate &lt; 10% in pilot testing</li> <li> Load testing passed at 2x expected peak volume</li> <li> Failover and disaster recovery tested</li> </ul>"},{"location":"appendices/go-live-checklist/#compliance-readiness","title":"Compliance Readiness","text":"<ul> <li> Data Privacy Impact Assessment (DPIA) completed</li> <li> Biometric data consent flow implemented and tested</li> <li> Data residency requirements verified</li> <li> Audit logging verified \u2014 captures all required fields</li> <li> Data retention policies configured per regulation</li> <li> Regulatory documentation prepared (ISO 30107 compliance evidence)</li> </ul>"},{"location":"appendices/go-live-checklist/#operational-readiness","title":"Operational Readiness","text":"<ul> <li> Monitoring dashboards live (APCER, BPCER, latency, volume)</li> <li> Alerting configured for anomaly detection</li> <li> Manual review queue configured and staffed</li> <li> Fallback flows tested (liveness failure \u2192 alternative verification)</li> <li> Incident response playbook documented and rehearsed</li> <li> Support team trained on liveness-related customer issues</li> <li> Customer-facing FAQ and help documentation published</li> </ul>"},{"location":"appendices/go-live-checklist/#integration-readiness","title":"Integration Readiness","text":"<ul> <li> Face matching pipeline verified end-to-end with liveness</li> <li> Session integrity verified (same face used for liveness + matching)</li> <li> Anti-replay protection verified</li> <li> Frame encryption verified</li> <li> Model versioning and rollback capability tested</li> <li> A/B testing infrastructure ready for threshold tuning</li> </ul>"},{"location":"appendices/go-live-checklist/#business-readiness","title":"Business Readiness","text":"<ul> <li> Vendor SLA signed with performance guarantees</li> <li> Escalation paths documented (vendor, internal, regulatory)</li> <li> Customer communication prepared (privacy notice, FAQ)</li> <li> Training completed for all stakeholders</li> <li> Success metrics defined and tracking enabled</li> <li> Post-launch review scheduled (30/60/90 days)</li> </ul>"},{"location":"appendices/ibeta-pai-species/","title":"Appendix B: iBeta PAI Species Reference","text":""},{"location":"appendices/ibeta-pai-species/#level-1-species","title":"Level 1 Species","text":"ID Category Species Specifications P.1 Print Color laser, A4 600 DPI, standard paper P.2 Print Color inkjet, A4 High-quality, photo paper P.3 Print Photo lab, 4\u00d76 glossy Professional lab print P.4 Print Photo lab, 4\u00d76 matte Professional lab print P.5 Print Photo lab, 8\u00d710 Larger format P.6 Print Life-size poster Full face size P.7 Print Curved photo Bent on cylindrical form S.1 Screen Phone, photo 5-7\", max brightness S.2 Screen Phone, video Pre-recorded, natural motion S.3 Screen Tablet, photo 9-12\" S.4 Screen Tablet, video HD video S.5 Screen Monitor, photo 13\"+ HD/FHD S.6 Screen Monitor, video HD video playback"},{"location":"appendices/ibeta-pai-species/#level-2-additional-species","title":"Level 2 Additional Species","text":"ID Category Species Specifications M.1 Mask Paper mask, eye cutouts Worn by attacker M.2 Mask Paper mask, full Complete face coverage M.3 Mask Latex, commercial Off-the-shelf realistic M.4 Mask Resin/plaster cast Custom rigid mask M.5 Mask Silicone, custom Hand-painted, realistic skin M.6 Mannequin With photo Photo on mannequin head M.7 Mannequin With makeup Makeup applied M.8 3D Print From face scan FDM/SLA, painted"},{"location":"appendices/regulatory-links/","title":"Appendix C: Regulatory Reference Links","text":""},{"location":"appendices/regulatory-links/#india","title":"India","text":"<ul> <li>RBI Master Direction on KYC: RBI Website</li> <li>V-CIP Guidelines (January 2020)</li> <li>DPDPA 2023: MeitY</li> <li>UIDAI Aadhaar Authentication Guidelines</li> </ul>"},{"location":"appendices/regulatory-links/#europe","title":"Europe","text":"<ul> <li>EBA Guidelines on Remote Onboarding (EBA/GL/2022/15)</li> <li>eIDAS 2.0 Regulation</li> <li>GDPR: EUR-Lex</li> <li>ETSI TS 119 461 (Video Identification)</li> <li>BaFin Video Identification Circular</li> </ul>"},{"location":"appendices/regulatory-links/#united-states","title":"United States","text":"<ul> <li>NIST SP 800-63B: NIST</li> <li>BSA/AML (FinCEN): FinCEN</li> <li>Illinois BIPA: 740 ILCS 14</li> </ul>"},{"location":"appendices/regulatory-links/#international-standards","title":"International Standards","text":"<ul> <li>ISO/IEC 30107 (Parts 1-4): ISO</li> <li>ISO/IEC 19795-1: Biometric Performance Testing</li> <li>FIDO Alliance Biometric Certification: FIDO</li> </ul>"},{"location":"appendices/regulatory-links/#testing-labs","title":"Testing Labs","text":"<ul> <li>iBeta Quality Assurance: iBeta</li> <li>NIST FRVT: NIST FRVT</li> </ul>"},{"location":"appendices/research-papers/","title":"Appendix D: Research Papers &amp; Recommended Reading","text":""},{"location":"appendices/research-papers/#face-anti-spoofing-core","title":"Face Anti-Spoofing (Core)","text":"<ul> <li>FaceForensics++ (R\u00f6ssler et al., ICCV 2019) \u2014 Benchmark dataset for face manipulation detection</li> <li>CDCN (Yu et al., CVPR 2020) \u2014 Central Difference Convolutional Network for face anti-spoofing</li> <li>NAS-FAS (Yu et al., TPAMI 2020) \u2014 Neural architecture search for face anti-spoofing</li> <li>SSDG (Jia et al., CVPR 2020) \u2014 Single Side Domain Generalization</li> <li>SSAN (Wang et al., CVPR 2022) \u2014 Shuffled Style Assembly Network</li> </ul>"},{"location":"appendices/research-papers/#domain-generalization","title":"Domain Generalization","text":"<ul> <li>MADDG (Shao et al., CVPR 2019) \u2014 Multi-Adversarial Discriminative Deep Domain Generalization</li> <li>DRDG (Liu et al., AAAI 2021) \u2014 Dual Reweighting Domain Generalization</li> <li>AMEL (Zhou et al., AAAI 2022) \u2014 Adaptive Meta-learning</li> </ul>"},{"location":"appendices/research-papers/#deepfake-detection","title":"Deepfake Detection","text":"<ul> <li>Thinking in Frequency (Qian et al., ECCV 2020)</li> <li>Multi-Attentional Deepfake Detection (Zhao et al., CVPR 2021)</li> <li>DeepfakeBench (Yan et al., NeurIPS 2023)</li> <li>Implicit Identity Leakage (Dong et al., CVPR 2023)</li> </ul>"},{"location":"appendices/research-papers/#datasets","title":"Datasets","text":"<ul> <li>OULU-NPU (Boulkenafet et al., 2017) \u2014 4 protocols, 4 conditions</li> <li>CASIA-FASD (Zhang et al., 2012) \u2014 Classic benchmark</li> <li>SiW (Liu et al., CVPR 2018) \u2014 Spoof in the Wild</li> <li>CelebA-Spoof (Zhang et al., ECCV 2020) \u2014 Large-scale with rich annotations</li> <li>WMCA (George et al., TIFS 2020) \u2014 Wide Multi-Channel Attack database</li> <li>SiW-Mv2 (Guo et al., 2022) \u2014 Extended spoof in the wild</li> </ul>"},{"location":"appendices/rfp-template/","title":"Appendix A: Sample RFP Questions for Liveness Vendor Selection","text":""},{"location":"appendices/rfp-template/#security-certification","title":"Security &amp; Certification","text":"<ol> <li>Do you hold iBeta Level 1 and/or Level 2 certification? Provide certificate and report.</li> <li>What is your APCER per PAI species (print, screen photo, screen video, mask)?</li> <li>What is your BPCER at the operating threshold recommended for banking?</li> <li>Do you have deepfake detection capability? Against which deepfake types?</li> <li>How do you detect virtual camera injection attacks?</li> <li>What device integrity checks do you perform (root, jailbreak, emulator)?</li> <li>Have you submitted to NIST FRVT PAD? Provide results.</li> <li>How do you handle adversarial machine learning attacks?</li> </ol>"},{"location":"appendices/rfp-template/#technical","title":"Technical","text":"<ol> <li>What is your SDK size (Android AAR, iOS framework)?</li> <li>What is your end-to-end latency (P50, P95, P99)?</li> <li>Do you support passive liveness, active liveness, or both?</li> <li>What is your minimum device/camera requirement?</li> <li>Do you support web browser-based liveness?</li> <li>What is your on-device vs server-side processing split?</li> <li>How do you handle poor lighting, blur, and partial occlusion?</li> </ol>"},{"location":"appendices/rfp-template/#privacy-compliance","title":"Privacy &amp; Compliance","text":"<ol> <li>Where is biometric data processed and stored?</li> <li>Do you support data residency in India / EU / specific regions?</li> <li>Are you GDPR Article 9 and DPDPA compliant for biometric data?</li> <li>What is your data retention policy for biometric samples?</li> <li>Provide your Data Processing Agreement (DPA) template.</li> </ol>"},{"location":"appendices/rfp-template/#performance-reliability","title":"Performance &amp; Reliability","text":"<ol> <li>What is your uptime SLA?</li> <li>What is your maximum concurrent verification capacity?</li> <li>How do you handle failover and disaster recovery?</li> <li>What is your average drop-off rate for genuine users?</li> </ol>"},{"location":"appendices/rfp-template/#business","title":"Business","text":"<ol> <li>What is your pricing model (per-transaction, tiered, flat)?</li> <li>What support tiers do you offer?</li> <li>What is your model update frequency?</li> <li>What are your SLA penalty terms?</li> <li>Provide 3 banking client references.</li> <li>What is your contract termination and data portability process?</li> </ol>"}]}